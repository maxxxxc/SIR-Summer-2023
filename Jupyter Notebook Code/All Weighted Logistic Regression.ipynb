{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b0ddd4b",
   "metadata": {},
   "source": [
    "# Magic Gamma Telescope\n",
    "\n",
    "Dataset is from https://archive.ics.uci.edu/dataset/159/magic+gamma+telescope\n",
    "\n",
    "n = 19020\n",
    "\n",
    "10 features\n",
    "\n",
    "- X1.  fLength:  continuous  # major axis of ellipse [mm]\n",
    "- X2.  fWidth:   continuous  # minor axis of ellipse [mm] \n",
    "- X3.  fSize:    continuous  # 10-log of sum of content of all pixels [in #phot]\n",
    "- X4.  fConc:    continuous  # ratio of sum of two highest pixels over fSize  [ratio]\n",
    "- X5.  fConc1:   continuous  # ratio of highest pixel over fSize  [ratio]\n",
    "- X6.  fAsym:    continuous  # distance from highest pixel to center, projected onto major axis [mm]\n",
    "- X7.  fM3Long:  continuous  # 3rd root of third moment along major axis  [mm] \n",
    "- X8.  fM3Trans: continuous  # 3rd root of third moment along minor axis  [mm]\n",
    "- X9.  fAlpha:   continuous  # angle of major axis with vector to origin [deg]\n",
    "- X10.  fDist:    continuous  # distance from origin to center of ellipse [mm]\n",
    "- Y.  class:    g,h         # gamma (signal), hadron (background)\n",
    "  - g = gamma (signal):     12332\n",
    "  - h = hadron (background): 6688"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "961a7d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b861a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/maxxxxc/SIR-Summer-2023/main/dataset/magic04.data\"\n",
    "column_names = [\"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X7\", \"X8\", \"X9\", \"X10\", \"Y\"]\n",
    "df = pd.read_csv(url, header=None, names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4330eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ff409132",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"Y\", axis=1)\n",
    "y = df[\"Y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "28cccefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12332\n",
       "0     6688\n",
       "Name: Y, dtype: int64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.replace({'g' : 1, 'h' : 0})\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a20454",
   "metadata": {},
   "source": [
    "# Divide and Conquer\n",
    "\n",
    "Divide the training data into 11 batches, train a logistic model on each of the batch, and then combine the 11 prediction results. Consider the following two ensemble methods:\n",
    "- majority voting\n",
    "- average (or sum) of the logit output and then make decision based on its sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "806458ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_process(X, y):\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "    \n",
    "    # Define the number of batches\n",
    "    num_batches = 11\n",
    "    \n",
    "    # Randomly shuffle the data indices\n",
    "    #indices = np.random.permutation(len(X))\n",
    "    \n",
    "    #change 7/12\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    \n",
    "    # Calculate the batch size\n",
    "    batch_size = len(X_train) // num_batches\n",
    "    \n",
    "    # Make predictions on the test set using majority voting\n",
    "    preds_voting = np.zeros(len(y_test))\n",
    "    # Make predictions on the test set using average of logit\n",
    "    preds_logit = np.zeros(len(y_test))\n",
    "    #Make predictions on the test set using average of probs\n",
    "    preds_prob = np.zeros(len(y_test))\n",
    "    \n",
    "    preds_voting_weighted = np.zeros(len(y_test))\n",
    "    preds_logit_weighted = np.zeros(len(y_test))\n",
    "    preds_prob_weighted = np.zeros(len(y_test))\n",
    "    \n",
    "    total_cverr = 0\n",
    "    \n",
    "    # Split the training data into batches, fit a logistic regression model on each batch\n",
    "    for i in range(num_batches):\n",
    "        # Calculate the starting and ending indices for the current batch\n",
    "        start_index = i * batch_size\n",
    "        end_index = (i + 1) * batch_size\n",
    "        \n",
    "        # Create a logistic regression model\n",
    "        model = LogisticRegression(max_iter=500)\n",
    "        \n",
    "        # Select the current batch for training\n",
    "        #X_batch = X_train[start_index:end_index]\n",
    "        #y_batch = y_train[start_index:end_index]\n",
    "        \n",
    "        #change 7/12\n",
    "        X_batch = X_train.iloc[indices[start_index:end_index]]\n",
    "        y_batch = y_train.iloc[indices[start_index:end_index]]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_batch_scaled = scaler.fit_transform(X_batch)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Fit the model on the current batch\n",
    "        model.fit(X_batch_scaled, y_batch)\n",
    "        current_cverr = cross_val_score(model, X_batch_scaled, y_batch, cv = 5, scoring = 'accuracy').mean()\n",
    "        total_cverr += current_cverr\n",
    "               \n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        # Accumulate the predictions using majority voting\n",
    "        preds_voting += (y_pred == 1)\n",
    "        preds_voting_weighted += (y_pred == 1) * current_cverr\n",
    "    \n",
    "        # Accumulate the predictions using majority voting\n",
    "        y_pred = model.decision_function(X_test_scaled)\n",
    "        preds_logit += y_pred\n",
    "        preds_logit_weighted += y_pred * current_cverr\n",
    "        \n",
    "        #Accumulate the probs\n",
    "        y_pred = model.predict_proba(X_test_scaled)\n",
    "        preds_prob += y_pred[:,1]\n",
    "        preds_prob_weighted += y_pred[:,1] * current_cverr\n",
    "    \n",
    "    accuracy = np.zeros(7)\n",
    "    auc_accuracy = np.zeros(7)\n",
    "    \n",
    "    preds_voting_weighted = preds_voting_weighted / total_cverr * num_batches\n",
    "    preds_logit_weighted = preds_logit_weighted / total_cverr * num_batches\n",
    "    preds_prob_weighted = preds_prob_weighted / total_cverr * num_batches\n",
    "    \n",
    "    # Majority voting (selecting the most frequent prediction for each sample)\n",
    "    final_predictions = np.where(preds_voting > num_batches / 2, 1, 0)\n",
    "    accuracy[0] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[0] = roc_auc_score(y_test, preds_voting)\n",
    "    \n",
    "    final_predictions = np.where(preds_voting_weighted > num_batches / 2, 1, 0)\n",
    "    accuracy[1] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[1] = roc_auc_score(y_test, preds_voting_weighted)\n",
    "    \n",
    "    # Average of logit\n",
    "    final_predictions = np.where(preds_logit > 0, 1, 0)\n",
    "    accuracy[2] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[2] = roc_auc_score(y_test, preds_logit)\n",
    "    \n",
    "    final_predictions = np.where(preds_logit_weighted > 0, 1, 0)\n",
    "    accuracy[3] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[3] = roc_auc_score(y_test, preds_logit_weighted)\n",
    "    \n",
    "    #Average of probs\n",
    "    final_predictions = np.where(preds_prob / num_batches > 0.5, 1, 0)\n",
    "    accuracy[4] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[4] = roc_auc_score(y_test, preds_prob)\n",
    "    \n",
    "    final_predictions = np.where(preds_prob_weighted / num_batches > 0.5, 1, 0)\n",
    "    accuracy[5] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[5] = roc_auc_score(y_test, preds_prob_weighted)\n",
    "    \n",
    "    # Train a model on all 11 batches of training data\n",
    "    model = LogisticRegression(max_iter=500)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy[6] = accuracy_score(y_test, y_pred)\n",
    "    y_pred = model.decision_function(X_test_scaled)\n",
    "    auc_accuracy[6] = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, auc_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273e621",
   "metadata": {},
   "source": [
    "Try the divide and conquer approaches 10 times, reporting the following error matrix. \n",
    "- 10-by-4\n",
    "- col_1: majority voting\n",
    "- col_2: average the logit\n",
    "- col_3: average probabilities\n",
    "- col_4: using the model trained on all the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "67dade4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies: [[0.79022082 0.79022082 0.78890641 0.78890641 0.78890641 0.78903785\n",
      "  0.78890641]\n",
      " [0.78509464 0.78509464 0.78548896 0.78548896 0.78548896 0.78548896\n",
      "  0.78588328]\n",
      " [0.7898265  0.7898265  0.78956362 0.78943218 0.78956362 0.78943218\n",
      "  0.78930074]\n",
      " [0.79100946 0.79100946 0.79035226 0.79035226 0.79035226 0.7904837\n",
      "  0.79035226]\n",
      " [0.79915878 0.79915878 0.79994742 0.79994742 0.79994742 0.79994742\n",
      "  0.80007886]\n",
      " [0.78128286 0.78128286 0.78325447 0.78325447 0.78325447 0.78325447\n",
      "  0.78299159]\n",
      " [0.79100946 0.79100946 0.79100946 0.79087802 0.79087802 0.79087802\n",
      "  0.79166667]\n",
      " [0.78680336 0.78680336 0.78732913 0.78732913 0.78719769 0.78719769\n",
      "  0.78732913]\n",
      " [0.79258675 0.79258675 0.79035226 0.79061514 0.7904837  0.7904837\n",
      "  0.79061514]\n",
      " [0.79311251 0.79311251 0.79324395 0.79324395 0.79324395 0.79324395\n",
      "  0.79271819]\n",
      " [0.78995794 0.78995794 0.78969506 0.78969506 0.78969506 0.78969506\n",
      "  0.78969506]\n",
      " [0.79390116 0.79390116 0.79376972 0.79376972 0.79363828 0.79376972\n",
      "  0.79337539]\n",
      " [0.79337539 0.79337539 0.79416404 0.7940326  0.79416404 0.7940326\n",
      "  0.79390116]\n",
      " [0.78575184 0.78575184 0.78588328 0.78588328 0.78588328 0.78588328\n",
      "  0.78548896]\n",
      " [0.78798633 0.78798633 0.78759201 0.78772345 0.78759201 0.78772345\n",
      "  0.78759201]\n",
      " [0.79140379 0.79140379 0.79232387 0.79245531 0.79232387 0.79232387\n",
      "  0.79206099]\n",
      " [0.79547844 0.79547844 0.79652997 0.79666141 0.79626709 0.79652997\n",
      "  0.79600421]\n",
      " [0.7982387  0.7982387  0.79731861 0.79731861 0.79731861 0.79731861\n",
      "  0.79784437]\n",
      " [0.78877497 0.78877497 0.78969506 0.78969506 0.78969506 0.78969506\n",
      "  0.78930074]\n",
      " [0.78299159 0.78299159 0.78286015 0.78286015 0.78286015 0.78286015\n",
      "  0.78312303]\n",
      " [0.78995794 0.78995794 0.79232387 0.79192955 0.79232387 0.79206099\n",
      "  0.79140379]\n",
      " [0.78706625 0.78706625 0.7862776  0.78614616 0.78640904 0.7862776\n",
      "  0.78614616]\n",
      " [0.79166667 0.79166667 0.79127234 0.79127234 0.79127234 0.79127234\n",
      "  0.79179811]\n",
      " [0.79271819 0.79271819 0.79245531 0.79219243 0.79219243 0.79206099\n",
      "  0.79311251]\n",
      " [0.79363828 0.79363828 0.79153523 0.79153523 0.79140379 0.79153523\n",
      "  0.79166667]\n",
      " [0.7891693  0.7891693  0.79035226 0.79022082 0.79035226 0.79022082\n",
      "  0.79022082]\n",
      " [0.7904837  0.7904837  0.79087802 0.79061514 0.79087802 0.79061514\n",
      "  0.79140379]\n",
      " [0.79455836 0.79455836 0.79232387 0.79219243 0.79245531 0.79219243\n",
      "  0.79311251]\n",
      " [0.79416404 0.79416404 0.79442692 0.79455836 0.79442692 0.79442692\n",
      "  0.79482124]\n",
      " [0.79442692 0.79442692 0.795347   0.79521556 0.795347   0.79521556\n",
      "  0.79521556]\n",
      " [0.79416404 0.79416404 0.79482124 0.79455836 0.79482124 0.79455836\n",
      "  0.79482124]\n",
      " [0.78614616 0.78614616 0.78522608 0.78483176 0.78509464 0.78509464\n",
      "  0.78509464]\n",
      " [0.79666141 0.79666141 0.79574132 0.79600421 0.79587277 0.79574132\n",
      "  0.79639853]\n",
      " [0.78930074 0.78930074 0.78903785 0.7891693  0.78890641 0.78903785\n",
      "  0.78969506]\n",
      " [0.79876446 0.79876446 0.79942166 0.7995531  0.79929022 0.7995531\n",
      "  0.80047319]\n",
      " [0.78417455 0.78417455 0.78456887 0.78443743 0.78456887 0.78443743\n",
      "  0.78456887]\n",
      " [0.7904837  0.7904837  0.79008938 0.78995794 0.78995794 0.78995794\n",
      "  0.79035226]\n",
      " [0.7946898  0.7946898  0.79245531 0.79258675 0.79245531 0.79245531\n",
      "  0.79245531]\n",
      " [0.7849632  0.7849632  0.78535752 0.78548896 0.78522608 0.78535752\n",
      "  0.78509464]\n",
      " [0.79166667 0.79166667 0.79061514 0.79061514 0.79061514 0.79074658\n",
      "  0.79100946]\n",
      " [0.78930074 0.78930074 0.78864353 0.78851209 0.78890641 0.78864353\n",
      "  0.7904837 ]\n",
      " [0.78864353 0.78864353 0.78930074 0.78943218 0.78930074 0.78930074\n",
      "  0.78930074]\n",
      " [0.79061514 0.79061514 0.79074658 0.79100946 0.79061514 0.79074658\n",
      "  0.79035226]\n",
      " [0.79087802 0.79087802 0.79179811 0.79166667 0.79166667 0.79166667\n",
      "  0.79271819]\n",
      " [0.79206099 0.79206099 0.79153523 0.79153523 0.79153523 0.79166667\n",
      "  0.79206099]\n",
      " [0.78824921 0.78824921 0.78864353 0.78851209 0.78864353 0.78864353\n",
      "  0.78864353]\n",
      " [0.79140379 0.79140379 0.79100946 0.7911409  0.7911409  0.79100946\n",
      "  0.7904837 ]\n",
      " [0.79600421 0.79600421 0.79442692 0.79416404 0.79455836 0.79442692\n",
      "  0.79455836]\n",
      " [0.7911409  0.7911409  0.79127234 0.79127234 0.79140379 0.79140379\n",
      "  0.79179811]\n",
      " [0.79206099 0.79206099 0.79219243 0.79219243 0.79219243 0.79206099\n",
      "  0.79140379]]\n",
      "AUC: [[0.76979465 0.76973873 0.83747954 0.83746256 0.83744868 0.83743716\n",
      "  0.83746931]\n",
      " [0.76857458 0.76859037 0.83519018 0.83519572 0.83511557 0.8351229\n",
      "  0.83498564]\n",
      " [0.77488535 0.77488952 0.84169796 0.84169789 0.84173917 0.84173802\n",
      "  0.84195017]\n",
      " [0.77351864 0.77357363 0.84000626 0.8399903  0.84000482 0.83999567\n",
      "  0.83976867]\n",
      " [0.7803712  0.78046507 0.83866912 0.83864741 0.83899676 0.83898613\n",
      "  0.83870791]\n",
      " [0.77072887 0.77081053 0.83997803 0.83996636 0.84020009 0.84018909\n",
      "  0.83985563]\n",
      " [0.77115333 0.7711668  0.83734832 0.83732168 0.83759659 0.83757999\n",
      "  0.83731527]\n",
      " [0.77646128 0.77659401 0.83907072 0.83904422 0.83947768 0.83945339\n",
      "  0.83912469]\n",
      " [0.78097751 0.78106228 0.83832614 0.8383184  0.83853225 0.83852648\n",
      "  0.83837752]\n",
      " [0.77712826 0.7771539  0.84015452 0.84015841 0.84012956 0.84013246\n",
      "  0.84012842]\n",
      " [0.77404135 0.77405832 0.83785287 0.83784916 0.8380945  0.83809245\n",
      "  0.83801282]\n",
      " [0.78420762 0.78434189 0.8425888  0.84257406 0.84280442 0.84279655\n",
      "  0.84231028]\n",
      " [0.78087683 0.78087232 0.84002164 0.84002111 0.84009676 0.84009054\n",
      "  0.84027707]\n",
      " [0.77563119 0.77560091 0.83986358 0.83986742 0.83982281 0.83983394\n",
      "  0.83976842]\n",
      " [0.76986945 0.76988574 0.83582288 0.83582127 0.83568257 0.83568356\n",
      "  0.83581386]\n",
      " [0.78063376 0.78065518 0.83907427 0.83907526 0.83943419 0.83942925\n",
      "  0.83895554]\n",
      " [0.77987811 0.77985161 0.84455384 0.84456157 0.84473219 0.84474271\n",
      "  0.84437868]\n",
      " [0.78702309 0.78697554 0.84244035 0.84243397 0.84253265 0.84252969\n",
      "  0.84237031]\n",
      " [0.77133396 0.77136758 0.83023753 0.8302229  0.83053712 0.83052932\n",
      "  0.82986744]\n",
      " [0.77695576 0.77698424 0.8328511  0.83282927 0.83310483 0.83308757\n",
      "  0.83288592]\n",
      " [0.77393219 0.77399906 0.83958485 0.83955667 0.8396748  0.83964693\n",
      "  0.83955042]\n",
      " [0.77139458 0.77132471 0.82655769 0.82655655 0.82659343 0.82659191\n",
      "  0.82656507]\n",
      " [0.77304741 0.77313152 0.83457917 0.83456185 0.83463579 0.83461795\n",
      "  0.83473654]\n",
      " [0.77325226 0.77329645 0.83688698 0.83689555 0.83696694 0.83697672\n",
      "  0.83695245]\n",
      " [0.77512279 0.77533499 0.83123621 0.83122546 0.83153935 0.8315315\n",
      "  0.83124109]\n",
      " [0.77393839 0.77394528 0.83899819 0.83900652 0.83900129 0.83900501\n",
      "  0.83895665]\n",
      " [0.77984655 0.77981089 0.84462608 0.84462222 0.84479108 0.8447907\n",
      "  0.84458649]\n",
      " [0.77677228 0.77676197 0.83990564 0.83989747 0.83994732 0.8399432\n",
      "  0.83961349]\n",
      " [0.77845306 0.77852065 0.84068956 0.84068381 0.84104642 0.84102849\n",
      "  0.84055006]\n",
      " [0.77833624 0.77843734 0.84281214 0.84281252 0.84296113 0.84296463\n",
      "  0.84256949]\n",
      " [0.78283293 0.78289939 0.83877517 0.83878329 0.83915701 0.83916058\n",
      "  0.8387398 ]\n",
      " [0.77946868 0.77971492 0.83537806 0.8353534  0.83566665 0.8356541\n",
      "  0.83561394]\n",
      " [0.78658986 0.78661344 0.84340999 0.84340147 0.84363288 0.84362801\n",
      "  0.8434179 ]\n",
      " [0.7757735  0.77573969 0.8411517  0.84115852 0.8413386  0.84134004\n",
      "  0.84113236]\n",
      " [0.7778891  0.77780465 0.84641579 0.84638632 0.84653581 0.84650875\n",
      "  0.84641542]\n",
      " [0.7670704  0.767091   0.83710655 0.83711867 0.83708652 0.83709902\n",
      "  0.8370241 ]\n",
      " [0.77425535 0.77432393 0.83924529 0.83923519 0.83920352 0.83920101\n",
      "  0.83934524]\n",
      " [0.77929101 0.77934542 0.84402633 0.84403079 0.84401134 0.84402451\n",
      "  0.84403246]\n",
      " [0.77129281 0.77121198 0.83719788 0.8372102  0.83706628 0.837078\n",
      "  0.83700693]\n",
      " [0.76956513 0.76963828 0.83614312 0.83614312 0.83625285 0.83625308\n",
      "  0.83626011]\n",
      " [0.77325595 0.77328937 0.84154387 0.84150459 0.84163925 0.84159991\n",
      "  0.84167687]\n",
      " [0.76975559 0.76979229 0.83808614 0.83808817 0.83802672 0.83803002\n",
      "  0.83841186]\n",
      " [0.78081984 0.78091914 0.84191968 0.84189335 0.84220179 0.84217765\n",
      "  0.84187079]\n",
      " [0.77860627 0.77866981 0.83897218 0.83895176 0.83904797 0.83902399\n",
      "  0.83908231]\n",
      " [0.77590568 0.77590908 0.83936964 0.83936851 0.83942786 0.83943058\n",
      "  0.839309  ]\n",
      " [0.77487871 0.77488734 0.83550026 0.83551112 0.83552522 0.83554219\n",
      "  0.83533714]\n",
      " [0.7704283  0.77044721 0.83730364 0.83728789 0.83737197 0.83735895\n",
      "  0.83706603]\n",
      " [0.77574913 0.77579799 0.8391397  0.83912335 0.83922697 0.83920222\n",
      "  0.83945026]\n",
      " [0.77467575 0.77469324 0.83575424 0.83575615 0.83592469 0.83592713\n",
      "  0.83574591]\n",
      " [0.77550544 0.77547323 0.84008571 0.84008055 0.84041825 0.84041195\n",
      "  0.83983097]]\n"
     ]
    }
   ],
   "source": [
    "# Number of times to repeat the process\n",
    "num_repeats = 50\n",
    "\n",
    "# Initialize an empty matrix (10-by-4) to store accuracies\n",
    "accuracies = np.zeros((num_repeats, 7))\n",
    "auc_accuracies = np.zeros((num_repeats, 7))\n",
    "\n",
    "seed = 42\n",
    "# Repeat the process and store accuracies\n",
    "for i in range(num_repeats):\n",
    "    np.random.seed(seed)\n",
    "    accuracies[i], auc_accuracies[i] = iterate_process(X, y)\n",
    "    seed += 2\n",
    "    \n",
    "\n",
    "# Print the accuracies\n",
    "print(\"Accuracies:\", accuracies)\n",
    "print(\"AUC:\", auc_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "df9593cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.79094374, 0.79094374, 0.79086751, 0.79084122, 0.79085174,\n",
       "       0.79083333, 0.79097792])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accuracies, axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0b76f33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00387429, 0.00387429, 0.00374648, 0.00376297, 0.00374123,\n",
       "       0.00374397, 0.00383604])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "eb08c798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.775835  , 0.77586925, 0.83871258, 0.83870528, 0.83884006,\n",
       "       0.83883451, 0.83868829])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(auc_accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "dd556ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00452981, 0.00454062, 0.00367318, 0.00367262, 0.00367974,\n",
       "       0.0036787 , 0.00367083])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(auc_accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810a4909",
   "metadata": {},
   "source": [
    "# Wireless Indoor Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a931601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1000\n",
       "1    1000\n",
       "Name: 7, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/maxxxxc/SIR-Summer-2023/main/dataset/wifi_localization.txt\"\n",
    "df = pd.read_csv(url, sep = '\\t', header = None)\n",
    "\n",
    "X = df.drop(7, axis=1)\n",
    "y = df[7]\n",
    "\n",
    "y = y.replace({2 : 0, 1 : 0, 3: 1, 4 : 1})\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d383e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies: [[0.9325  0.9325  0.9325  0.9325  0.9325  0.9325  0.93375]\n",
      " [0.91    0.91    0.9125  0.9125  0.9125  0.9125  0.90625]\n",
      " [0.9325  0.9325  0.93125 0.93    0.93    0.92875 0.9325 ]\n",
      " [0.9225  0.9225  0.92125 0.92125 0.92125 0.92125 0.9175 ]\n",
      " [0.92375 0.92375 0.92375 0.92375 0.92375 0.92375 0.92   ]\n",
      " [0.92125 0.92125 0.92125 0.92    0.92125 0.92    0.915  ]\n",
      " [0.92    0.92    0.92375 0.9225  0.9225  0.9225  0.9225 ]\n",
      " [0.925   0.925   0.92625 0.92625 0.92625 0.92625 0.9225 ]\n",
      " [0.94125 0.94125 0.94    0.94    0.94    0.94    0.94   ]\n",
      " [0.9275  0.9275  0.92625 0.92625 0.92625 0.92625 0.92125]\n",
      " [0.92625 0.92625 0.9225  0.9225  0.92125 0.9225  0.9225 ]\n",
      " [0.9225  0.9225  0.92    0.92    0.92    0.92    0.9175 ]\n",
      " [0.9225  0.9225  0.9275  0.9275  0.9275  0.9275  0.9275 ]\n",
      " [0.935   0.935   0.935   0.935   0.935   0.935   0.93   ]\n",
      " [0.92875 0.92875 0.9275  0.9275  0.9275  0.9275  0.9275 ]\n",
      " [0.9075  0.9075  0.91    0.91    0.91    0.91    0.91375]\n",
      " [0.92375 0.92375 0.9275  0.92625 0.9275  0.92625 0.92125]\n",
      " [0.9175  0.9175  0.9175  0.9175  0.9175  0.9175  0.91625]\n",
      " [0.93375 0.93375 0.93    0.93125 0.93    0.93125 0.92625]\n",
      " [0.92625 0.92625 0.93125 0.93125 0.93125 0.93125 0.92875]\n",
      " [0.92875 0.92875 0.92625 0.92625 0.92625 0.92625 0.93125]\n",
      " [0.9275  0.9275  0.92875 0.92875 0.93    0.92875 0.93   ]\n",
      " [0.91625 0.91625 0.92    0.92    0.92    0.92    0.92   ]\n",
      " [0.93    0.93    0.92875 0.9275  0.93    0.92875 0.93   ]\n",
      " [0.9225  0.9225  0.9175  0.9175  0.9175  0.91875 0.92125]\n",
      " [0.93    0.93    0.92875 0.93    0.92875 0.92875 0.92875]\n",
      " [0.9275  0.9275  0.92625 0.92625 0.92625 0.92625 0.93   ]\n",
      " [0.93125 0.93125 0.93125 0.93125 0.93125 0.93125 0.92875]\n",
      " [0.92375 0.92375 0.925   0.925   0.925   0.925   0.9225 ]\n",
      " [0.93    0.93    0.93125 0.93125 0.93125 0.93125 0.92375]\n",
      " [0.915   0.915   0.92    0.92    0.92    0.91875 0.9175 ]\n",
      " [0.91625 0.91625 0.91875 0.91875 0.91875 0.91875 0.9175 ]\n",
      " [0.91875 0.91875 0.92    0.91875 0.92    0.91875 0.92125]\n",
      " [0.91875 0.91875 0.91875 0.91875 0.91875 0.91875 0.91625]\n",
      " [0.91125 0.91125 0.9175  0.91625 0.91625 0.91625 0.915  ]\n",
      " [0.925   0.925   0.92    0.92    0.92    0.92    0.92   ]\n",
      " [0.90875 0.90875 0.9075  0.9075  0.9075  0.9075  0.91125]\n",
      " [0.93    0.93    0.92625 0.92625 0.92625 0.92625 0.9225 ]\n",
      " [0.925   0.925   0.92875 0.9275  0.92875 0.92875 0.93125]\n",
      " [0.935   0.935   0.93125 0.93125 0.93125 0.9325  0.92375]\n",
      " [0.93    0.93    0.92875 0.92875 0.92875 0.92875 0.92625]\n",
      " [0.9375  0.9375  0.9325  0.9325  0.93375 0.93375 0.9325 ]\n",
      " [0.925   0.925   0.93125 0.93125 0.93125 0.93125 0.93   ]\n",
      " [0.92625 0.92625 0.92875 0.92875 0.9275  0.92875 0.92625]\n",
      " [0.92875 0.92875 0.92875 0.92875 0.9275  0.9275  0.9275 ]\n",
      " [0.9225  0.9225  0.92125 0.92125 0.92125 0.92125 0.92125]\n",
      " [0.91    0.91    0.91625 0.91625 0.91625 0.91625 0.91375]\n",
      " [0.92    0.92    0.91125 0.9125  0.91375 0.91375 0.9125 ]\n",
      " [0.91125 0.91125 0.92    0.91875 0.91875 0.9175  0.9125 ]\n",
      " [0.93375 0.93375 0.9275  0.9275  0.9275  0.9275  0.9275 ]]\n",
      "AUC: [[0.968817   0.96911389 0.98189898 0.98193023 0.98186148 0.98187398\n",
      "  0.98293029]\n",
      " [0.95887795 0.9588967  0.9760025  0.97598375 0.97659634 0.97656509\n",
      "  0.97783404]\n",
      " [0.96438742 0.96409639 0.98262557 0.98264434 0.98270693 0.98270067\n",
      "  0.98329526]\n",
      " [0.95985276 0.95973684 0.97841479 0.97837719 0.9787406  0.97865288\n",
      "  0.97909774]\n",
      " [0.96383234 0.96420424 0.97861755 0.9786238  0.97890506 0.97886131\n",
      "  0.97980511]\n",
      " [0.97158732 0.97170607 0.98238739 0.98238114 0.98248114 0.98251239\n",
      "  0.98209989]\n",
      " [0.95705872 0.95702746 0.97866847 0.97861845 0.97866847 0.9785997\n",
      "  0.97875599]\n",
      " [0.96905941 0.96892189 0.98224822 0.98224197 0.9824795  0.982467\n",
      "  0.98232323]\n",
      " [0.97044895 0.97085612 0.9842457  0.98428955 0.98441484 0.98447748\n",
      "  0.98446495]\n",
      " [0.9675985  0.96743902 0.98300188 0.98302064 0.98310194 0.98309568\n",
      "  0.98327079]\n",
      " [0.97075312 0.97061875 0.982      0.98205625 0.98195625 0.98195625\n",
      "  0.98183125]\n",
      " [0.96578487 0.96555347 0.98075672 0.98071295 0.98117573 0.98115697\n",
      "  0.98010632]\n",
      " [0.96574152 0.96587592 0.9807845  0.98077825 0.98089702 0.98089076\n",
      "  0.98057822]\n",
      " [0.96853407 0.96874042 0.98487359 0.98484233 0.98503617 0.98513623\n",
      "  0.98436083]\n",
      " [0.9698692  0.96971601 0.98151135 0.98148634 0.98188025 0.98184899\n",
      "  0.98218037]\n",
      " [0.95342187 0.95354688 0.9748125  0.9748     0.97501875 0.9750625\n",
      "  0.9757625 ]\n",
      " [0.96637681 0.96632055 0.9802018  0.98016429 0.98023305 0.9802393\n",
      "  0.98121452]\n",
      " [0.95774198 0.95750732 0.97600811 0.97599559 0.97607694 0.97607069\n",
      "  0.97545118]\n",
      " [0.96097247 0.96087245 0.97898277 0.97895777 0.97925783 0.97917656\n",
      "  0.97968293]\n",
      " [0.97053473 0.97062538 0.98271971 0.98271346 0.98355746 0.98358872\n",
      "  0.98276972]\n",
      " [0.96370241 0.96378063 0.97943731 0.97943105 0.97962504 0.97965633\n",
      "  0.97966884]\n",
      " [0.96150529 0.96164904 0.97878697 0.97874947 0.97898697 0.97894322\n",
      "  0.97995575]\n",
      " [0.96196731 0.96201429 0.97943485 0.97940353 0.97960398 0.97960398\n",
      "  0.9801427 ]\n",
      " [0.96632453 0.96613376 0.98105442 0.9810294  0.98111697 0.98112948\n",
      "  0.98223657]\n",
      " [0.9652543  0.9653793  0.97934259 0.97932384 0.9794676  0.9794676\n",
      "  0.97922383]\n",
      " [0.96745105 0.96748235 0.98173327 0.98167067 0.98199619 0.98197741\n",
      "  0.98258464]\n",
      " [0.96675542 0.96684917 0.98291832 0.98288082 0.98299332 0.98299332\n",
      "  0.98291832]\n",
      " [0.96618534 0.9662416  0.98161588 0.98159712 0.98177215 0.98170339\n",
      "  0.98197218]\n",
      " [0.960967   0.96078229 0.97729023 0.97723388 0.97732154 0.97725266\n",
      "  0.97767217]\n",
      " [0.97340339 0.97313458 0.98263984 0.9826461  0.98293991 0.9828899\n",
      "  0.98338376]\n",
      " [0.95996211 0.96011217 0.97674695 0.9767532  0.97699705 0.97701581\n",
      "  0.97689701]\n",
      " [0.96004101 0.96004726 0.97741367 0.97739491 0.97720737 0.97720112\n",
      "  0.97787627]\n",
      " [0.96089896 0.96074888 0.97746359 0.9774886  0.9774886  0.97749486\n",
      "  0.97750736]\n",
      " [0.96269992 0.96284065 0.98072292 0.98069165 0.98044146 0.98044146\n",
      "  0.97946572]\n",
      " [0.95541612 0.95540361 0.97825543 0.97822415 0.97823041 0.97823041\n",
      "  0.97859323]\n",
      " [0.95833021 0.95857396 0.97976824 0.97978074 0.97978699 0.97973074\n",
      "  0.979862  ]\n",
      " [0.9624152  0.96230558 0.97951628 0.97958519 0.97972926 0.97972926\n",
      "  0.97945991]\n",
      " [0.95803501 0.95795371 0.98061519 0.98061519 0.98072774 0.98075276\n",
      "  0.98016496]\n",
      " [0.95684951 0.9569464  0.98022191 0.9801844  0.98025316 0.98024691\n",
      "  0.98071574]\n",
      " [0.9655386  0.96534465 0.98149921 0.98147418 0.98146793 0.98148044\n",
      "  0.98131777]\n",
      " [0.97297542 0.97312231 0.98269202 0.98271077 0.98289829 0.98292329\n",
      "  0.98292954]\n",
      " [0.97047487 0.97056888 0.98400592 0.98401845 0.98420647 0.98417513\n",
      "  0.98446343]\n",
      " [0.96963134 0.96950945 0.98062931 0.98063556 0.98081683 0.98080433\n",
      "  0.98157941]\n",
      " [0.96904204 0.96915768 0.98347867 0.98347867 0.98372246 0.9837037\n",
      "  0.98315987]\n",
      " [0.96546217 0.96531216 0.98260451 0.98263576 0.98274827 0.98270452\n",
      "  0.98261701]\n",
      " [0.96966487 0.96931173 0.97953074 0.97951199 0.98025576 0.98024951\n",
      "  0.98086202]\n",
      " [0.94960302 0.94972815 0.97707579 0.97705076 0.97758257 0.97757006\n",
      "  0.97769519]\n",
      " [0.96089296 0.9607367  0.97711021 0.97705396 0.97747275 0.97739774\n",
      "  0.97791654]\n",
      " [0.95294368 0.95290929 0.97742222 0.97740971 0.97719713 0.97719088\n",
      "  0.97671569]\n",
      " [0.95971891 0.95971891 0.98337616 0.98340117 0.98320111 0.98321361\n",
      "  0.98363874]]\n"
     ]
    }
   ],
   "source": [
    "# Number of times to repeat the process\n",
    "num_repeats = 50\n",
    "\n",
    "# Initialize an empty matrix (10-by-4) to store accuracies\n",
    "accuracies = np.zeros((num_repeats, 7))\n",
    "auc_accuracies = np.zeros((num_repeats, 7))\n",
    "\n",
    "seed = 42\n",
    "# Repeat the process and store accuracies\n",
    "for i in range(num_repeats):\n",
    "    np.random.seed(seed)\n",
    "    accuracies[i], auc_accuracies[i] = iterate_process(X, y)\n",
    "    seed += 2\n",
    "    \n",
    "\n",
    "# Print the accuracies\n",
    "print(\"Accuracies:\", accuracies)\n",
    "print(\"AUC:\", auc_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34e502b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.924325, 0.924325, 0.924525, 0.924375, 0.924475, 0.9244  ,\n",
       "       0.9231  ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d9d82f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00771852, 0.00771852, 0.00666094, 0.00668136, 0.00665249,\n",
       "       0.00669533, 0.00688858])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29295cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96390726, 0.9639035 , 0.98030329, 0.98029366, 0.98046602,\n",
       "       0.98045614, 0.98062023])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(auc_accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "980a4563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00551189, 0.00550877, 0.002402  , 0.00241128, 0.00240214,\n",
       "       0.00241261, 0.00233349])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(auc_accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec614f",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
