{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b0ddd4b",
   "metadata": {},
   "source": [
    "# Epileptic Seizure Recognition\n",
    "\n",
    "Dataset is from https://archive.ics.uci.edu/dataset/388/epileptic+seizure+recognition\n",
    "\n",
    "n = 11500\n",
    "\n",
    "178 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "961a7d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b861a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/maxxxxc/SIR-Summer-2023/main/dataset/Epileptic%20Seizure%20Recognition.csv\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9658e176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11500, 180)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c99a4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed   X1   X2   X3   X4   X5   X6   X7   X8   X9  ...  X170  X171  \\\n",
      "0  X21.V1.791  135  190  229  223  192  125   55   -9  -33  ...   -17   -15   \n",
      "1  X15.V1.924  386  382  356  331  320  315  307  272  244  ...   164   150   \n",
      "2     X8.V1.1  -32  -39  -47  -37  -32  -36  -57  -73  -85  ...    57    64   \n",
      "3   X16.V1.60 -105 -101  -96  -92  -89  -95 -102 -100  -87  ...   -82   -81   \n",
      "4   X20.V1.54   -9  -65  -98 -102  -78  -48  -16    0  -21  ...     4     2   \n",
      "\n",
      "   X172  X173  X174  X175  X176  X177  X178  y  \n",
      "0   -31   -77  -103  -127  -116   -83   -51  4  \n",
      "1   146   152   157   156   154   143   129  1  \n",
      "2    48    19   -12   -30   -35   -35   -36  5  \n",
      "3   -80   -77   -85   -77   -72   -69   -65  5  \n",
      "4   -12   -32   -41   -65   -83   -89   -73  5  \n",
      "\n",
      "[5 rows x 180 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4330eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff409132",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"y\", axis=1)\n",
    "y = df[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8321cca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X169</th>\n",
       "      <th>X170</th>\n",
       "      <th>X171</th>\n",
       "      <th>X172</th>\n",
       "      <th>X173</th>\n",
       "      <th>X174</th>\n",
       "      <th>X175</th>\n",
       "      <th>X176</th>\n",
       "      <th>X177</th>\n",
       "      <th>X178</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X21.V1.791</td>\n",
       "      <td>135</td>\n",
       "      <td>190</td>\n",
       "      <td>229</td>\n",
       "      <td>223</td>\n",
       "      <td>192</td>\n",
       "      <td>125</td>\n",
       "      <td>55</td>\n",
       "      <td>-9</td>\n",
       "      <td>-33</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>-17</td>\n",
       "      <td>-15</td>\n",
       "      <td>-31</td>\n",
       "      <td>-77</td>\n",
       "      <td>-103</td>\n",
       "      <td>-127</td>\n",
       "      <td>-116</td>\n",
       "      <td>-83</td>\n",
       "      <td>-51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X15.V1.924</td>\n",
       "      <td>386</td>\n",
       "      <td>382</td>\n",
       "      <td>356</td>\n",
       "      <td>331</td>\n",
       "      <td>320</td>\n",
       "      <td>315</td>\n",
       "      <td>307</td>\n",
       "      <td>272</td>\n",
       "      <td>244</td>\n",
       "      <td>...</td>\n",
       "      <td>168</td>\n",
       "      <td>164</td>\n",
       "      <td>150</td>\n",
       "      <td>146</td>\n",
       "      <td>152</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>154</td>\n",
       "      <td>143</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X8.V1.1</td>\n",
       "      <td>-32</td>\n",
       "      <td>-39</td>\n",
       "      <td>-47</td>\n",
       "      <td>-37</td>\n",
       "      <td>-32</td>\n",
       "      <td>-36</td>\n",
       "      <td>-57</td>\n",
       "      <td>-73</td>\n",
       "      <td>-85</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>57</td>\n",
       "      <td>64</td>\n",
       "      <td>48</td>\n",
       "      <td>19</td>\n",
       "      <td>-12</td>\n",
       "      <td>-30</td>\n",
       "      <td>-35</td>\n",
       "      <td>-35</td>\n",
       "      <td>-36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>X16.V1.60</td>\n",
       "      <td>-105</td>\n",
       "      <td>-101</td>\n",
       "      <td>-96</td>\n",
       "      <td>-92</td>\n",
       "      <td>-89</td>\n",
       "      <td>-95</td>\n",
       "      <td>-102</td>\n",
       "      <td>-100</td>\n",
       "      <td>-87</td>\n",
       "      <td>...</td>\n",
       "      <td>-80</td>\n",
       "      <td>-82</td>\n",
       "      <td>-81</td>\n",
       "      <td>-80</td>\n",
       "      <td>-77</td>\n",
       "      <td>-85</td>\n",
       "      <td>-77</td>\n",
       "      <td>-72</td>\n",
       "      <td>-69</td>\n",
       "      <td>-65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X20.V1.54</td>\n",
       "      <td>-9</td>\n",
       "      <td>-65</td>\n",
       "      <td>-98</td>\n",
       "      <td>-102</td>\n",
       "      <td>-78</td>\n",
       "      <td>-48</td>\n",
       "      <td>-16</td>\n",
       "      <td>0</td>\n",
       "      <td>-21</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-12</td>\n",
       "      <td>-32</td>\n",
       "      <td>-41</td>\n",
       "      <td>-65</td>\n",
       "      <td>-83</td>\n",
       "      <td>-89</td>\n",
       "      <td>-73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 179 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed   X1   X2   X3   X4   X5   X6   X7   X8   X9  ...  X169  X170  \\\n",
       "0  X21.V1.791  135  190  229  223  192  125   55   -9  -33  ...     8   -17   \n",
       "1  X15.V1.924  386  382  356  331  320  315  307  272  244  ...   168   164   \n",
       "2     X8.V1.1  -32  -39  -47  -37  -32  -36  -57  -73  -85  ...    29    57   \n",
       "3   X16.V1.60 -105 -101  -96  -92  -89  -95 -102 -100  -87  ...   -80   -82   \n",
       "4   X20.V1.54   -9  -65  -98 -102  -78  -48  -16    0  -21  ...    10     4   \n",
       "\n",
       "   X171  X172  X173  X174  X175  X176  X177  X178  \n",
       "0   -15   -31   -77  -103  -127  -116   -83   -51  \n",
       "1   150   146   152   157   156   154   143   129  \n",
       "2    64    48    19   -12   -30   -35   -35   -36  \n",
       "3   -81   -80   -77   -85   -77   -72   -69   -65  \n",
       "4     2   -12   -32   -41   -65   -83   -89   -73  \n",
       "\n",
       "[5 rows x 179 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aecae1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(\"Unnamed\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9f31060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X169</th>\n",
       "      <th>X170</th>\n",
       "      <th>X171</th>\n",
       "      <th>X172</th>\n",
       "      <th>X173</th>\n",
       "      <th>X174</th>\n",
       "      <th>X175</th>\n",
       "      <th>X176</th>\n",
       "      <th>X177</th>\n",
       "      <th>X178</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>135</td>\n",
       "      <td>190</td>\n",
       "      <td>229</td>\n",
       "      <td>223</td>\n",
       "      <td>192</td>\n",
       "      <td>125</td>\n",
       "      <td>55</td>\n",
       "      <td>-9</td>\n",
       "      <td>-33</td>\n",
       "      <td>-38</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>-17</td>\n",
       "      <td>-15</td>\n",
       "      <td>-31</td>\n",
       "      <td>-77</td>\n",
       "      <td>-103</td>\n",
       "      <td>-127</td>\n",
       "      <td>-116</td>\n",
       "      <td>-83</td>\n",
       "      <td>-51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>386</td>\n",
       "      <td>382</td>\n",
       "      <td>356</td>\n",
       "      <td>331</td>\n",
       "      <td>320</td>\n",
       "      <td>315</td>\n",
       "      <td>307</td>\n",
       "      <td>272</td>\n",
       "      <td>244</td>\n",
       "      <td>232</td>\n",
       "      <td>...</td>\n",
       "      <td>168</td>\n",
       "      <td>164</td>\n",
       "      <td>150</td>\n",
       "      <td>146</td>\n",
       "      <td>152</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>154</td>\n",
       "      <td>143</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-32</td>\n",
       "      <td>-39</td>\n",
       "      <td>-47</td>\n",
       "      <td>-37</td>\n",
       "      <td>-32</td>\n",
       "      <td>-36</td>\n",
       "      <td>-57</td>\n",
       "      <td>-73</td>\n",
       "      <td>-85</td>\n",
       "      <td>-94</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>57</td>\n",
       "      <td>64</td>\n",
       "      <td>48</td>\n",
       "      <td>19</td>\n",
       "      <td>-12</td>\n",
       "      <td>-30</td>\n",
       "      <td>-35</td>\n",
       "      <td>-35</td>\n",
       "      <td>-36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-105</td>\n",
       "      <td>-101</td>\n",
       "      <td>-96</td>\n",
       "      <td>-92</td>\n",
       "      <td>-89</td>\n",
       "      <td>-95</td>\n",
       "      <td>-102</td>\n",
       "      <td>-100</td>\n",
       "      <td>-87</td>\n",
       "      <td>-79</td>\n",
       "      <td>...</td>\n",
       "      <td>-80</td>\n",
       "      <td>-82</td>\n",
       "      <td>-81</td>\n",
       "      <td>-80</td>\n",
       "      <td>-77</td>\n",
       "      <td>-85</td>\n",
       "      <td>-77</td>\n",
       "      <td>-72</td>\n",
       "      <td>-69</td>\n",
       "      <td>-65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9</td>\n",
       "      <td>-65</td>\n",
       "      <td>-98</td>\n",
       "      <td>-102</td>\n",
       "      <td>-78</td>\n",
       "      <td>-48</td>\n",
       "      <td>-16</td>\n",
       "      <td>0</td>\n",
       "      <td>-21</td>\n",
       "      <td>-59</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-12</td>\n",
       "      <td>-32</td>\n",
       "      <td>-41</td>\n",
       "      <td>-65</td>\n",
       "      <td>-83</td>\n",
       "      <td>-89</td>\n",
       "      <td>-73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 178 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    X1   X2   X3   X4   X5   X6   X7   X8   X9  X10  ...  X169  X170  X171  \\\n",
       "0  135  190  229  223  192  125   55   -9  -33  -38  ...     8   -17   -15   \n",
       "1  386  382  356  331  320  315  307  272  244  232  ...   168   164   150   \n",
       "2  -32  -39  -47  -37  -32  -36  -57  -73  -85  -94  ...    29    57    64   \n",
       "3 -105 -101  -96  -92  -89  -95 -102 -100  -87  -79  ...   -80   -82   -81   \n",
       "4   -9  -65  -98 -102  -78  -48  -16    0  -21  -59  ...    10     4     2   \n",
       "\n",
       "   X172  X173  X174  X175  X176  X177  X178  \n",
       "0   -31   -77  -103  -127  -116   -83   -51  \n",
       "1   146   152   157   156   154   143   129  \n",
       "2    48    19   -12   -30   -35   -35   -36  \n",
       "3   -80   -77   -85   -77   -72   -69   -65  \n",
       "4   -12   -32   -41   -65   -83   -89   -73  \n",
       "\n",
       "[5 rows x 178 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c6d4ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    2300\n",
       "1    2300\n",
       "5    2300\n",
       "2    2300\n",
       "3    2300\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "848faca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9200\n",
       "1    2300\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.replace({4 : 0, 3 : 0, 2 : 0, 5 : 0})\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201d8a20",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b2ba396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8423188405797102\n",
      "Test Accuracy: 0.8208695652173913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\panda\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "#model = LogisticRegression()\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a20454",
   "metadata": {},
   "source": [
    "# Divide and Conquer\n",
    "\n",
    "Divide the training data into 11 batches, train a logistic model on each of the batch, and then combine the 11 prediction results. Consider the following two ensemble methods:\n",
    "- majority voting\n",
    "- average (or sum) of the logit output and then make decision based on its sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "806458ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_process(X, y):\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "    \n",
    "    # Define the number of batches\n",
    "    num_batches = 11\n",
    "    \n",
    "    # Randomly shuffle the data indices\n",
    "    #indices = np.random.permutation(len(X))\n",
    "    \n",
    "    #change 7/12\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    \n",
    "    # Calculate the batch size\n",
    "    batch_size = len(X_train) // num_batches\n",
    "    \n",
    "    # Make predictions on the test set using majority voting\n",
    "    preds_voting = np.zeros(len(y_test))\n",
    "    # Make predictions on the test set using average of logit\n",
    "    preds_logit = np.zeros(len(y_test))\n",
    "    #Make predictions on the test set using average of probs\n",
    "    preds_prob = np.zeros(len(y_test))\n",
    "    \n",
    "    preds_voting_weighted = np.zeros(len(y_test))\n",
    "    preds_logit_weighted = np.zeros(len(y_test))\n",
    "    preds_prob_weighted = np.zeros(len(y_test))\n",
    "    \n",
    "    total_cverr = 0\n",
    "    \n",
    "    # Split the training data into batches, fit a logistic regression model on each batch\n",
    "    for i in range(num_batches):\n",
    "        # Calculate the starting and ending indices for the current batch\n",
    "        start_index = i * batch_size\n",
    "        end_index = (i + 1) * batch_size\n",
    "        \n",
    "        # Create a logistic regression model\n",
    "        model = LogisticRegression(max_iter=500)\n",
    "        \n",
    "        # Select the current batch for training\n",
    "        #X_batch = X_train[start_index:end_index]\n",
    "        #y_batch = y_train[start_index:end_index]\n",
    "        \n",
    "        #change 7/12\n",
    "        X_batch = X_train.iloc[indices[start_index:end_index]]\n",
    "        y_batch = y_train.iloc[indices[start_index:end_index]]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_batch_scaled = scaler.fit_transform(X_batch)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Fit the model on the current batch\n",
    "        model.fit(X_batch_scaled, y_batch)\n",
    "        current_cverr = cross_val_score(model, X_batch_scaled, y_batch, cv = 5, scoring = 'accuracy').mean()\n",
    "        total_cverr += current_cverr\n",
    "               \n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        # Accumulate the predictions using majority voting\n",
    "        preds_voting += (y_pred == 1)\n",
    "        preds_voting_weighted += (y_pred == 1) * current_cverr\n",
    "    \n",
    "        # Accumulate the predictions using majority voting\n",
    "        y_pred = model.decision_function(X_test_scaled)\n",
    "        preds_logit += y_pred\n",
    "        preds_logit_weighted += y_pred * current_cverr\n",
    "        \n",
    "        #Accumulate the probs\n",
    "        y_pred = model.predict_proba(X_test_scaled)\n",
    "        preds_prob += y_pred[:,1]\n",
    "        preds_prob_weighted += y_pred[:,1] * current_cverr\n",
    "    \n",
    "    accuracy = np.zeros(7)\n",
    "    auc_accuracy = np.zeros(7)\n",
    "    \n",
    "    preds_voting_weighted = preds_voting_weighted / total_cverr * num_batches\n",
    "    preds_logit_weighted = preds_logit_weighted / total_cverr * num_batches\n",
    "    preds_prob_weighted = preds_prob_weighted / total_cverr * num_batches\n",
    "    \n",
    "    # Majority voting (selecting the most frequent prediction for each sample)\n",
    "    final_predictions = np.where(preds_voting > num_batches / 2, 1, 0)\n",
    "    accuracy[0] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[0] = roc_auc_score(y_test, preds_voting)\n",
    "    \n",
    "    final_predictions = np.where(preds_voting_weighted > num_batches / 2, 1, 0)\n",
    "    accuracy[1] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[1] = roc_auc_score(y_test, preds_voting_weighted)\n",
    "    \n",
    "    # Average of logit\n",
    "    final_predictions = np.where(preds_logit > 0, 1, 0)\n",
    "    accuracy[2] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[2] = roc_auc_score(y_test, preds_logit)\n",
    "    \n",
    "    final_predictions = np.where(preds_logit_weighted > 0, 1, 0)\n",
    "    accuracy[3] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[3] = roc_auc_score(y_test, preds_logit_weighted)\n",
    "    \n",
    "    #Average of probs\n",
    "    final_predictions = np.where(preds_prob / num_batches > 0.5, 1, 0)\n",
    "    accuracy[4] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[4] = roc_auc_score(y_test, preds_prob)\n",
    "    \n",
    "    final_predictions = np.where(preds_prob_weighted / num_batches > 0.5, 1, 0)\n",
    "    accuracy[5] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[5] = roc_auc_score(y_test, preds_prob_weighted)\n",
    "    \n",
    "    # Train a model on all 11 batches of training data\n",
    "    model = LogisticRegression(max_iter=500)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy[6] = accuracy_score(y_test, y_pred)\n",
    "    y_pred = model.decision_function(X_test_scaled)\n",
    "    auc_accuracy[6] = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, auc_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273e621",
   "metadata": {},
   "source": [
    "Try the divide and conquer approaches 10 times, reporting the following error matrix. \n",
    "- 10-by-4\n",
    "- col_1: majority voting\n",
    "- col_2: average the logit\n",
    "- col_3: average probabilities\n",
    "- col_4: using the model trained on all the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67dade4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies: [[0.82173913 0.82173913 0.81391304 0.81413043 0.81847826 0.81804348\n",
      "  0.815     ]\n",
      " [0.8226087  0.8226087  0.81695652 0.81695652 0.82065217 0.82043478\n",
      "  0.8176087 ]\n",
      " [0.83086957 0.83086957 0.82891304 0.82978261 0.83065217 0.83043478\n",
      "  0.82543478]\n",
      " [0.83217391 0.83217391 0.82391304 0.82391304 0.82847826 0.82847826\n",
      "  0.82630435]\n",
      " [0.82826087 0.82826087 0.82478261 0.82456522 0.82478261 0.825\n",
      "  0.825     ]\n",
      " [0.82282609 0.82282609 0.81717391 0.8173913  0.81847826 0.81891304\n",
      "  0.81086957]\n",
      " [0.82217391 0.82217391 0.8173913  0.8176087  0.82108696 0.82065217\n",
      "  0.8226087 ]\n",
      " [0.82086957 0.82086957 0.81695652 0.8173913  0.81826087 0.81847826\n",
      "  0.81478261]\n",
      " [0.81782609 0.81782609 0.81586957 0.81586957 0.81652174 0.81652174\n",
      "  0.81782609]\n",
      " [0.82326087 0.82326087 0.82065217 0.82086957 0.8223913  0.82282609\n",
      "  0.82586957]\n",
      " [0.82543478 0.82543478 0.81891304 0.81913043 0.82217391 0.8223913\n",
      "  0.81956522]\n",
      " [0.82108696 0.82108696 0.81782609 0.81804348 0.81934783 0.81978261\n",
      "  0.81673913]\n",
      " [0.81847826 0.81847826 0.81195652 0.8123913  0.81543478 0.81543478\n",
      "  0.81369565]\n",
      " [0.82304348 0.82304348 0.81826087 0.81826087 0.82304348 0.8226087\n",
      "  0.81608696]\n",
      " [0.835      0.835      0.82869565 0.82891304 0.83152174 0.83152174\n",
      "  0.8273913 ]\n",
      " [0.82891304 0.82891304 0.82347826 0.82347826 0.82413043 0.82391304\n",
      "  0.82195652]\n",
      " [0.82326087 0.82326087 0.81586957 0.81608696 0.82130435 0.82152174\n",
      "  0.81869565]\n",
      " [0.83086957 0.83086957 0.82326087 0.82282609 0.8273913  0.82782609\n",
      "  0.81869565]\n",
      " [0.82347826 0.82347826 0.82       0.82       0.8226087  0.8223913\n",
      "  0.81956522]\n",
      " [0.82108696 0.82108696 0.81456522 0.81478261 0.81913043 0.81891304\n",
      "  0.81934783]\n",
      " [0.8176087  0.8176087  0.81478261 0.81543478 0.81652174 0.81652174\n",
      "  0.81347826]\n",
      " [0.82130435 0.82130435 0.81130435 0.81130435 0.81782609 0.8176087\n",
      "  0.81521739]\n",
      " [0.82826087 0.82826087 0.82195652 0.82195652 0.8273913  0.8273913\n",
      "  0.82652174]\n",
      " [0.82804348 0.82804348 0.8173913  0.8176087  0.82347826 0.82391304\n",
      "  0.82152174]\n",
      " [0.82434783 0.82434783 0.81869565 0.81869565 0.82065217 0.82065217\n",
      "  0.8223913 ]\n",
      " [0.82173913 0.82173913 0.81956522 0.81978261 0.81956522 0.82\n",
      "  0.82021739]\n",
      " [0.82434783 0.82434783 0.82021739 0.82       0.82217391 0.82217391\n",
      "  0.82021739]\n",
      " [0.82282609 0.82282609 0.81543478 0.81586957 0.82021739 0.82043478\n",
      "  0.81586957]\n",
      " [0.82695652 0.82695652 0.82217391 0.82173913 0.82521739 0.82521739\n",
      "  0.82021739]\n",
      " [0.82130435 0.82130435 0.8176087  0.81782609 0.81891304 0.81934783\n",
      "  0.81673913]\n",
      " [0.83152174 0.83152174 0.82695652 0.82695652 0.82869565 0.82847826\n",
      "  0.82695652]\n",
      " [0.82347826 0.82347826 0.81521739 0.81565217 0.82108696 0.82065217\n",
      "  0.81913043]\n",
      " [0.82673913 0.82673913 0.82391304 0.82391304 0.825      0.82521739\n",
      "  0.81826087]\n",
      " [0.82413043 0.82413043 0.82217391 0.82217391 0.81978261 0.82021739\n",
      "  0.81956522]\n",
      " [0.83       0.83       0.82413043 0.82434783 0.82804348 0.82804348\n",
      "  0.825     ]\n",
      " [0.81934783 0.81934783 0.81347826 0.81347826 0.81673913 0.81673913\n",
      "  0.81326087]\n",
      " [0.82086957 0.82086957 0.81347826 0.81326087 0.81608696 0.81608696\n",
      "  0.81804348]\n",
      " [0.82478261 0.82478261 0.81782609 0.81782609 0.8226087  0.8226087\n",
      "  0.82065217]\n",
      " [0.8223913  0.8223913  0.81608696 0.81608696 0.81869565 0.81913043\n",
      "  0.82152174]\n",
      " [0.82043478 0.82043478 0.8173913  0.81782609 0.8176087  0.81804348\n",
      "  0.8123913 ]\n",
      " [0.82173913 0.82173913 0.81434783 0.81413043 0.81934783 0.81913043\n",
      "  0.81608696]\n",
      " [0.82673913 0.82673913 0.81956522 0.82043478 0.82391304 0.82369565\n",
      "  0.81434783]\n",
      " [0.82434783 0.82434783 0.81847826 0.81847826 0.82108696 0.82086957\n",
      "  0.82      ]\n",
      " [0.82021739 0.82021739 0.81369565 0.81413043 0.81521739 0.815\n",
      "  0.81608696]\n",
      " [0.82195652 0.82195652 0.81891304 0.81891304 0.82021739 0.82021739\n",
      "  0.81565217]\n",
      " [0.82478261 0.82478261 0.8173913  0.81695652 0.82195652 0.82195652\n",
      "  0.81652174]\n",
      " [0.8326087  0.8326087  0.82608696 0.82565217 0.83108696 0.83043478\n",
      "  0.82804348]\n",
      " [0.83086957 0.83086957 0.82695652 0.8273913  0.82934783 0.83\n",
      "  0.82521739]\n",
      " [0.82347826 0.82347826 0.81673913 0.81652174 0.82152174 0.82130435\n",
      "  0.81521739]\n",
      " [0.82934783 0.82934783 0.82130435 0.82152174 0.82543478 0.82565217\n",
      "  0.82173913]]\n",
      "AUC: [[0.93521656 0.9362483  0.52222844 0.52255021 0.92881133 0.92910537\n",
      "  0.53929874]\n",
      " [0.93748125 0.93778834 0.53620794 0.53574305 0.92422418 0.92439983\n",
      "  0.52957795]\n",
      " [0.93395314 0.93451992 0.52433541 0.52529316 0.9178886  0.91825271\n",
      "  0.53814919]\n",
      " [0.93066841 0.92932929 0.52922804 0.52912675 0.92635096 0.92634285\n",
      "  0.527685  ]\n",
      " [0.92771231 0.92802057 0.53165654 0.53198037 0.90153974 0.90180243\n",
      "  0.52094595]\n",
      " [0.93598612 0.93596739 0.52848393 0.52842575 0.92130052 0.92115636\n",
      "  0.53089613]\n",
      " [0.94110295 0.94242112 0.52459501 0.5253728  0.93548843 0.9359402\n",
      "  0.54319311]\n",
      " [0.93413364 0.93508525 0.51218379 0.51227861 0.92481249 0.92497697\n",
      "  0.5211767 ]\n",
      " [0.93295797 0.93250399 0.51060507 0.5103656  0.92014094 0.92032171\n",
      "  0.53097297]\n",
      " [0.93015494 0.92995405 0.53219901 0.53172659 0.92113067 0.92100042\n",
      "  0.51968499]\n",
      " [0.92746394 0.92641237 0.50363615 0.5036622  0.91123206 0.91067641\n",
      "  0.52887358]\n",
      " [0.92812853 0.92782552 0.51185104 0.51231342 0.91514353 0.91513596\n",
      "  0.50980834]\n",
      " [0.94034425 0.94047962 0.52582827 0.52602363 0.93035055 0.93054505\n",
      "  0.51872272]\n",
      " [0.93315506 0.93325042 0.52073402 0.52049013 0.92008213 0.92011867\n",
      "  0.52640654]\n",
      " [0.93603919 0.93689772 0.53987437 0.54020291 0.92259865 0.92271069\n",
      "  0.52784983]\n",
      " [0.94359184 0.94452283 0.52647548 0.5271204  0.93835054 0.93885378\n",
      "  0.53844382]\n",
      " [0.94497886 0.94515444 0.52587043 0.52554858 0.93276989 0.93298219\n",
      "  0.52623966]\n",
      " [0.92417773 0.92606194 0.49591076 0.49605696 0.88986142 0.89034514\n",
      "  0.5105599 ]\n",
      " [0.94697694 0.94749099 0.53284141 0.53233219 0.93275908 0.93294717\n",
      "  0.547008  ]\n",
      " [0.94617606 0.94670183 0.51891623 0.51838885 0.93938062 0.93945885\n",
      "  0.52436202]\n",
      " [0.93401692 0.93457846 0.50929825 0.50869887 0.93259807 0.93278162\n",
      "  0.5079197 ]\n",
      " [0.93546531 0.93579137 0.50808298 0.50804231 0.92887973 0.92897166\n",
      "  0.51598052]\n",
      " [0.94083824 0.94156341 0.52409933 0.52382094 0.93539148 0.93542265\n",
      "  0.5342309 ]\n",
      " [0.94295266 0.94175837 0.53190515 0.53236111 0.94682655 0.94656361\n",
      "  0.54098945]\n",
      " [0.9339554  0.93392649 0.52232603 0.52204497 0.91631802 0.91637942\n",
      "  0.52511401]\n",
      " [0.93592054 0.93558941 0.51689592 0.51616316 0.92454022 0.92444398\n",
      "  0.53419201]\n",
      " [0.92990577 0.92967676 0.52462717 0.52450754 0.92440737 0.92456014\n",
      "  0.52572618]\n",
      " [0.93546471 0.93475629 0.5052324  0.50533276 0.92674615 0.92653104\n",
      "  0.50662839]\n",
      " [0.92848505 0.92871812 0.51020531 0.51044663 0.89791009 0.89809689\n",
      "  0.52822338]\n",
      " [0.9385444  0.93912221 0.50759387 0.50784309 0.93975852 0.93993682\n",
      "  0.52153993]\n",
      " [0.9369836  0.93728468 0.53721058 0.53717305 0.91159309 0.91174711\n",
      "  0.52767633]\n",
      " [0.93570259 0.93633398 0.51336117 0.51225894 0.9135352  0.91410419\n",
      "  0.52890451]\n",
      " [0.93770806 0.93725038 0.52859906 0.52840883 0.92368101 0.92364639\n",
      "  0.53119841]\n",
      " [0.93175102 0.93060238 0.51018182 0.51042335 0.91620347 0.91630043\n",
      "  0.5216916 ]\n",
      " [0.94053787 0.94068787 0.51923145 0.52013617 0.92601624 0.92597532\n",
      "  0.52179635]\n",
      " [0.94220306 0.94312707 0.49644101 0.49666644 0.93964965 0.93991564\n",
      "  0.52151415]\n",
      " [0.93999253 0.94042723 0.52375146 0.52284971 0.92803369 0.92835185\n",
      "  0.53265412]\n",
      " [0.93318881 0.93352115 0.50582876 0.50578335 0.92191273 0.92194723\n",
      "  0.51527757]\n",
      " [0.93138317 0.93067226 0.51098371 0.51030599 0.93490232 0.93497314\n",
      "  0.54555993]\n",
      " [0.92758059 0.92808267 0.51813761 0.51849692 0.91597612 0.91600534\n",
      "  0.51774736]\n",
      " [0.92849247 0.92822547 0.52573385 0.52567919 0.92572292 0.92594272\n",
      "  0.51433909]\n",
      " [0.9379982  0.93829017 0.54074323 0.54051564 0.92546477 0.92537263\n",
      "  0.54067957]\n",
      " [0.92489307 0.92605576 0.5295659  0.52898553 0.91788959 0.9181765\n",
      "  0.52353942]\n",
      " [0.94058715 0.94062172 0.50239681 0.5024729  0.92782242 0.9280356\n",
      "  0.49425216]\n",
      " [0.93581181 0.93500449 0.5194044  0.51884469 0.932079   0.93189311\n",
      "  0.53167599]\n",
      " [0.93120006 0.93088695 0.50167175 0.50161501 0.92081233 0.92074162\n",
      "  0.52201943]\n",
      " [0.93752297 0.9381509  0.53976036 0.53945495 0.93172102 0.93173754\n",
      "  0.52605826]\n",
      " [0.93310811 0.93333048 0.52554024 0.52526697 0.92114655 0.92173544\n",
      "  0.51476216]\n",
      " [0.94643913 0.9458834  0.51214459 0.51128596 0.92981684 0.92978791\n",
      "  0.52550739]\n",
      " [0.92927889 0.9296056  0.51522197 0.51562718 0.91649911 0.9166266\n",
      "  0.54950022]]\n"
     ]
    }
   ],
   "source": [
    "# Number of times to repeat the process\n",
    "num_repeats = 50\n",
    "\n",
    "# Initialize an empty matrix (10-by-4) to store accuracies\n",
    "accuracies = np.zeros((num_repeats, 7))\n",
    "auc_accuracies = np.zeros((num_repeats, 7))\n",
    "\n",
    "seed = 42\n",
    "# Repeat the process and store accuracies\n",
    "for i in range(num_repeats):\n",
    "    np.random.seed(seed)\n",
    "    accuracies[i], auc_accuracies[i] = iterate_process(X, y)\n",
    "    seed += 2\n",
    "    \n",
    "\n",
    "# Print the accuracies\n",
    "print(\"Accuracies:\", accuracies)\n",
    "print(\"AUC:\", auc_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dba311e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.82459565, 0.82459565, 0.81905217, 0.81916522, 0.82202609,\n",
       "       0.82205652, 0.81938261])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "292f8134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00413112, 0.00413112, 0.00430364, 0.00428903, 0.00421637,\n",
       "       0.00420285, 0.00432043])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "834e6406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93536624, 0.93552323, 0.51979675, 0.51973029, 0.92416141,\n",
       "       0.92427554, 0.52613507])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(auc_accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e0b1456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00564122, 0.00573139, 0.01137982, 0.0113639 , 0.0105878 ,\n",
       "       0.01056563, 0.01088222])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(auc_accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7247db1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
