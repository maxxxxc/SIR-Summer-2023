{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b0ddd4b",
   "metadata": {},
   "source": [
    "# Turkiye Student Evaluation\n",
    "\n",
    "Dataset is from https://archive.ics.uci.edu/dataset/262/turkiye+student+evaluation\n",
    "\n",
    "n = 5820\n",
    "\n",
    "28 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "961a7d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b861a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/maxxxxc/SIR-Summer-2023/main/dataset/turkiye-student-evaluation_generic.csv\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9658e176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5820, 33)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c99a4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   instr  class  nb.repeat  attendance  difficulty  Q1  Q2  Q3  Q4  Q5  ...  \\\n",
      "0      1      2          1           0           4   3   3   3   3   3  ...   \n",
      "1      1      2          1           1           3   3   3   3   3   3  ...   \n",
      "2      1      2          1           2           4   5   5   5   5   5  ...   \n",
      "3      1      2          1           1           3   3   3   3   3   3  ...   \n",
      "4      1      2          1           0           1   1   1   1   1   1  ...   \n",
      "\n",
      "   Q19  Q20  Q21  Q22  Q23  Q24  Q25  Q26  Q27  Q28  \n",
      "0    3    3    3    3    3    3    3    3    3    3  \n",
      "1    3    3    3    3    3    3    3    3    3    3  \n",
      "2    5    5    5    5    5    5    5    5    5    5  \n",
      "3    3    3    3    3    3    3    3    3    3    3  \n",
      "4    1    1    1    1    1    1    1    1    1    1  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4330eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c46f419",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"difficulty\", axis=1)\n",
    "y = df[\"difficulty\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a773d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(\"instr\", axis=1)\n",
    "X = X.drop(\"class\", axis=1)\n",
    "X = X.drop(\"nb.repeat\", axis=1)\n",
    "X = X.drop(\"attendance\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b12ec0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5</th>\n",
       "      <th>Q6</th>\n",
       "      <th>Q7</th>\n",
       "      <th>Q8</th>\n",
       "      <th>Q9</th>\n",
       "      <th>Q10</th>\n",
       "      <th>...</th>\n",
       "      <th>Q19</th>\n",
       "      <th>Q20</th>\n",
       "      <th>Q21</th>\n",
       "      <th>Q22</th>\n",
       "      <th>Q23</th>\n",
       "      <th>Q24</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q26</th>\n",
       "      <th>Q27</th>\n",
       "      <th>Q28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Q1  Q2  Q3  Q4  Q5  Q6  Q7  Q8  Q9  Q10  ...  Q19  Q20  Q21  Q22  Q23  Q24  \\\n",
       "0   3   3   3   3   3   3   3   3   3    3  ...    3    3    3    3    3    3   \n",
       "1   3   3   3   3   3   3   3   3   3    3  ...    3    3    3    3    3    3   \n",
       "2   5   5   5   5   5   5   5   5   5    5  ...    5    5    5    5    5    5   \n",
       "3   3   3   3   3   3   3   3   3   3    3  ...    3    3    3    3    3    3   \n",
       "4   1   1   1   1   1   1   1   1   1    1  ...    1    1    1    1    1    1   \n",
       "\n",
       "   Q25  Q26  Q27  Q28  \n",
       "0    3    3    3    3  \n",
       "1    3    3    3    3  \n",
       "2    5    5    5    5  \n",
       "3    3    3    3    3  \n",
       "4    1    1    1    1  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c6d4ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    1774\n",
       "1    1620\n",
       "4    1225\n",
       "5     652\n",
       "2     549\n",
       "Name: difficulty, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "848faca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3651\n",
       "0    2169\n",
       "Name: difficulty, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.replace({2 : 0, 1 : 0, 3: 1, 4 : 1, 5 : 1})\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201d8a20",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b2ba396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6466208476517755\n",
      "Test Accuracy: 0.6559278350515464\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "#model = LogisticRegression()\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a20454",
   "metadata": {},
   "source": [
    "# Divide and Conquer\n",
    "\n",
    "Divide the training data into 11 batches, train a logistic model on each of the batch, and then combine the 11 prediction results. Consider the following two ensemble methods:\n",
    "- majority voting\n",
    "- average (or sum) of the logit output and then make decision based on its sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "806458ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_process(X, y):\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "    \n",
    "    # Define the number of batches\n",
    "    num_batches = 11\n",
    "    \n",
    "    # Randomly shuffle the data indices\n",
    "    #indices = np.random.permutation(len(X))\n",
    "    \n",
    "    #change 7/12\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    \n",
    "    # Calculate the batch size\n",
    "    batch_size = len(X_train) // num_batches\n",
    "    \n",
    "    # Make predictions on the test set using majority voting\n",
    "    preds_voting = np.zeros(len(y_test))\n",
    "    # Make predictions on the test set using average of logit\n",
    "    preds_logit = np.zeros(len(y_test))\n",
    "    #Make predictions on the test set using average of probs\n",
    "    preds_prob = np.zeros(len(y_test))\n",
    "    \n",
    "    preds_voting_weighted = np.zeros(len(y_test))\n",
    "    preds_logit_weighted = np.zeros(len(y_test))\n",
    "    preds_prob_weighted = np.zeros(len(y_test))\n",
    "    \n",
    "    total_cverr = 0\n",
    "    \n",
    "    # Split the training data into batches, fit a logistic regression model on each batch\n",
    "    for i in range(num_batches):\n",
    "        # Calculate the starting and ending indices for the current batch\n",
    "        start_index = i * batch_size\n",
    "        end_index = (i + 1) * batch_size\n",
    "        \n",
    "        # Create a logistic regression model\n",
    "        model = LogisticRegression(max_iter=500)\n",
    "        \n",
    "        # Select the current batch for training\n",
    "        #X_batch = X_train[start_index:end_index]\n",
    "        #y_batch = y_train[start_index:end_index]\n",
    "        \n",
    "        #change 7/12\n",
    "        X_batch = X_train.iloc[indices[start_index:end_index]]\n",
    "        y_batch = y_train.iloc[indices[start_index:end_index]]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_batch_scaled = scaler.fit_transform(X_batch)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Fit the model on the current batch\n",
    "        model.fit(X_batch_scaled, y_batch)\n",
    "        current_cverr = cross_val_score(model, X_batch_scaled, y_batch, cv = 5, scoring = 'accuracy').mean()\n",
    "        total_cverr += current_cverr\n",
    "               \n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        # Accumulate the predictions using majority voting\n",
    "        preds_voting += (y_pred == 1)\n",
    "        preds_voting_weighted += (y_pred == 1) * current_cverr\n",
    "    \n",
    "        # Accumulate the predictions using majority voting\n",
    "        y_pred = model.decision_function(X_test_scaled)\n",
    "        preds_logit += y_pred\n",
    "        preds_logit_weighted += y_pred * current_cverr\n",
    "        \n",
    "        #Accumulate the probs\n",
    "        y_pred = model.predict_proba(X_test_scaled)\n",
    "        preds_prob += y_pred[:,1]\n",
    "        preds_prob_weighted += y_pred[:,1] * current_cverr\n",
    "    \n",
    "    accuracy = np.zeros(7)\n",
    "    auc_accuracy = np.zeros(7)\n",
    "    \n",
    "    preds_voting_weighted = preds_voting_weighted / total_cverr * num_batches\n",
    "    preds_logit_weighted = preds_logit_weighted / total_cverr * num_batches\n",
    "    preds_prob_weighted = preds_prob_weighted / total_cverr * num_batches\n",
    "    \n",
    "    # Majority voting (selecting the most frequent prediction for each sample)\n",
    "    final_predictions = np.where(preds_voting > num_batches / 2, 1, 0)\n",
    "    accuracy[0] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[0] = roc_auc_score(y_test, preds_voting)\n",
    "    \n",
    "    final_predictions = np.where(preds_voting_weighted > num_batches / 2, 1, 0)\n",
    "    accuracy[1] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[1] = roc_auc_score(y_test, preds_voting_weighted)\n",
    "    \n",
    "    # Average of logit\n",
    "    final_predictions = np.where(preds_logit > 0, 1, 0)\n",
    "    accuracy[2] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[2] = roc_auc_score(y_test, preds_logit)\n",
    "    \n",
    "    final_predictions = np.where(preds_logit_weighted > 0, 1, 0)\n",
    "    accuracy[3] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[3] = roc_auc_score(y_test, preds_logit_weighted)\n",
    "    \n",
    "    #Average of probs\n",
    "    final_predictions = np.where(preds_prob / num_batches > 0.5, 1, 0)\n",
    "    accuracy[4] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[4] = roc_auc_score(y_test, preds_prob)\n",
    "    \n",
    "    final_predictions = np.where(preds_prob_weighted / num_batches > 0.5, 1, 0)\n",
    "    accuracy[5] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[5] = roc_auc_score(y_test, preds_prob_weighted)\n",
    "    \n",
    "    # Train a model on all 11 batches of training data\n",
    "    model = LogisticRegression(max_iter=500)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy[6] = accuracy_score(y_test, y_pred)\n",
    "    y_pred = model.decision_function(X_test_scaled)\n",
    "    auc_accuracy[6] = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, auc_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273e621",
   "metadata": {},
   "source": [
    "Try the divide and conquer approaches 10 times, reporting the following error matrix. \n",
    "- 10-by-4\n",
    "- col_1: majority voting\n",
    "- col_2: average the logit\n",
    "- col_3: average probabilities\n",
    "- col_4: using the model trained on all the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67dade4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies: [[0.65120275 0.65120275 0.65936426 0.65893471 0.66065292 0.66022337\n",
      "  0.65635739]\n",
      " [0.63530928 0.63530928 0.64261168 0.64261168 0.64390034 0.64347079\n",
      "  0.64046392]\n",
      " [0.64948454 0.64948454 0.65120275 0.6507732  0.64905498 0.64991409\n",
      "  0.6516323 ]\n",
      " [0.64905498 0.64905498 0.65506873 0.65549828 0.6580756  0.6580756\n",
      "  0.65292096]\n",
      " [0.64948454 0.64948454 0.65034364 0.65120275 0.64905498 0.6507732\n",
      "  0.645189  ]\n",
      " [0.64733677 0.64733677 0.64089347 0.64261168 0.64175258 0.64218213\n",
      "  0.64690722]\n",
      " [0.64862543 0.64862543 0.6507732  0.6507732  0.65120275 0.65249141\n",
      "  0.64905498]\n",
      " [0.64304124 0.64304124 0.64604811 0.64647766 0.64561856 0.645189\n",
      "  0.64261168]\n",
      " [0.63487973 0.63487973 0.63530928 0.63573883 0.63402062 0.63359107\n",
      "  0.63745704]\n",
      " [0.63402062 0.63402062 0.63487973 0.63573883 0.63487973 0.63487973\n",
      "  0.63831615]\n",
      " [0.6516323  0.6516323  0.6507732  0.64948454 0.64991409 0.64905498\n",
      "  0.64733677]\n",
      " [0.66237113 0.66237113 0.66022337 0.66065292 0.66108247 0.66151203\n",
      "  0.66022337]\n",
      " [0.63702749 0.63702749 0.64089347 0.64046392 0.64003436 0.63960481\n",
      "  0.64347079]\n",
      " [0.63702749 0.63702749 0.63745704 0.6387457  0.63831615 0.63917526\n",
      "  0.63616838]\n",
      " [0.64003436 0.64003436 0.64647766 0.64647766 0.64733677 0.64733677\n",
      "  0.64304124]\n",
      " [0.64733677 0.64733677 0.64690722 0.64647766 0.64862543 0.64862543\n",
      "  0.64905498]\n",
      " [0.62757732 0.62757732 0.62886598 0.62800687 0.62886598 0.62886598\n",
      "  0.62542955]\n",
      " [0.64475945 0.64475945 0.64132302 0.64089347 0.64046392 0.64046392\n",
      "  0.64304124]\n",
      " [0.63616838 0.63616838 0.64089347 0.64089347 0.64046392 0.64132302\n",
      "  0.63616838]\n",
      " [0.63659794 0.63659794 0.64046392 0.63960481 0.64089347 0.64046392\n",
      "  0.6378866 ]\n",
      " [0.64819588 0.64819588 0.64390034 0.645189   0.64561856 0.64690722\n",
      "  0.64604811]\n",
      " [0.64862543 0.64862543 0.64733677 0.64776632 0.64819588 0.64690722\n",
      "  0.64991409]\n",
      " [0.64390034 0.64390034 0.64218213 0.64218213 0.6443299  0.6443299\n",
      "  0.64647766]\n",
      " [0.61984536 0.61984536 0.62414089 0.62285223 0.62285223 0.62328179\n",
      "  0.62199313]\n",
      " [0.63917526 0.63917526 0.6387457  0.63831615 0.63960481 0.6387457\n",
      "  0.63487973]\n",
      " [0.63659794 0.63659794 0.63659794 0.63702749 0.63659794 0.63659794\n",
      "  0.63445017]\n",
      " [0.64475945 0.64475945 0.6516323  0.65249141 0.64905498 0.64991409\n",
      "  0.6507732 ]\n",
      " [0.64132302 0.64132302 0.64561856 0.64347079 0.64304124 0.64261168\n",
      "  0.645189  ]\n",
      " [0.65034364 0.65034364 0.64862543 0.64948454 0.64819588 0.64948454\n",
      "  0.6516323 ]\n",
      " [0.63487973 0.63487973 0.63530928 0.63573883 0.63445017 0.63445017\n",
      "  0.6378866 ]\n",
      " [0.65292096 0.65292096 0.65335052 0.65378007 0.6516323  0.65120275\n",
      "  0.65249141]\n",
      " [0.63616838 0.63616838 0.64089347 0.64046392 0.6387457  0.64003436\n",
      "  0.64046392]\n",
      " [0.6314433  0.6314433  0.63573883 0.63530928 0.63573883 0.63445017\n",
      "  0.63058419]\n",
      " [0.63917526 0.63917526 0.63745704 0.63745704 0.63659794 0.6378866\n",
      "  0.63273196]\n",
      " [0.64304124 0.64304124 0.63917526 0.63917526 0.64175258 0.64304124\n",
      "  0.64003436]\n",
      " [0.64733677 0.64733677 0.64819588 0.64862543 0.64905498 0.64948454\n",
      "  0.64604811]\n",
      " [0.65292096 0.65292096 0.65420962 0.65463918 0.65463918 0.65463918\n",
      "  0.65592784]\n",
      " [0.65206186 0.65206186 0.64862543 0.64819588 0.64862543 0.64776632\n",
      "  0.64690722]\n",
      " [0.63960481 0.63960481 0.64475945 0.64304124 0.64390034 0.64218213\n",
      "  0.6443299 ]\n",
      " [0.64046392 0.64046392 0.64089347 0.64175258 0.64304124 0.64390034\n",
      "  0.63487973]\n",
      " [0.65120275 0.65120275 0.65420962 0.65335052 0.65420962 0.65335052\n",
      "  0.65335052]\n",
      " [0.66280069 0.66280069 0.65979381 0.66065292 0.65936426 0.65979381\n",
      "  0.66065292]\n",
      " [0.63960481 0.63960481 0.63960481 0.64003436 0.63831615 0.63917526\n",
      "  0.6387457 ]\n",
      " [0.63101375 0.63101375 0.63230241 0.63273196 0.63616838 0.63659794\n",
      "  0.63402062]\n",
      " [0.63702749 0.63702749 0.63659794 0.6378866  0.6378866  0.6387457\n",
      "  0.63917526]\n",
      " [0.63573883 0.63573883 0.63530928 0.63445017 0.63487973 0.63445017\n",
      "  0.63359107]\n",
      " [0.6516323  0.6516323  0.65678694 0.65678694 0.65635739 0.65635739\n",
      "  0.65206186]\n",
      " [0.63187285 0.63187285 0.63101375 0.62929553 0.63101375 0.63015464\n",
      "  0.63187285]\n",
      " [0.64347079 0.64347079 0.64218213 0.64261168 0.64347079 0.64304124\n",
      "  0.63702749]\n",
      " [0.66022337 0.66022337 0.65506873 0.65420962 0.65678694 0.65592784\n",
      "  0.65678694]]\n",
      "AUC: [[0.56528832 0.56601776 0.60011576 0.60087145 0.59798519 0.59868555\n",
      "  0.59658043]\n",
      " [0.59636497 0.59675537 0.6097246  0.60996602 0.61143131 0.61144497\n",
      "  0.61053545]\n",
      " [0.5715356  0.56908886 0.62425316 0.62330912 0.61901822 0.61773663\n",
      "  0.62359531]\n",
      " [0.56022566 0.56122343 0.61897198 0.6186508  0.61600693 0.61613508\n",
      "  0.61454668]\n",
      " [0.57509394 0.57563088 0.60695459 0.60700003 0.60638051 0.60607667\n",
      "  0.60504151]\n",
      " [0.55494942 0.55547325 0.60579494 0.60528035 0.60381249 0.60447291\n",
      "  0.60199032]\n",
      " [0.575928   0.57741683 0.6007164  0.60056925 0.59919295 0.59839581\n",
      "  0.59683616]\n",
      " [0.56529647 0.56496654 0.6233968  0.62295508 0.62233846 0.62178728\n",
      "  0.61892386]\n",
      " [0.57458141 0.57231645 0.61368468 0.61372066 0.61439741 0.61434852\n",
      "  0.61330405]\n",
      " [0.56208503 0.56248047 0.61122387 0.61123992 0.60923061 0.60862688\n",
      "  0.61056258]\n",
      " [0.56863298 0.56755063 0.60599461 0.60604948 0.60534173 0.60565436\n",
      "  0.60869102]\n",
      " [0.56256959 0.56451093 0.60786889 0.60676742 0.60804277 0.60778834\n",
      "  0.60606873]\n",
      " [0.58893519 0.59056049 0.60008655 0.6008222  0.60124792 0.60202879\n",
      "  0.60090797]\n",
      " [0.57748623 0.57719205 0.59419462 0.59341526 0.59095706 0.59130795\n",
      "  0.59171595]\n",
      " [0.57779928 0.57866421 0.61302944 0.61300558 0.61224076 0.61279365\n",
      "  0.60507461]\n",
      " [0.58049847 0.58172086 0.61195204 0.61208298 0.61170466 0.611698\n",
      "  0.61612259]\n",
      " [0.56347584 0.56309779 0.58921527 0.58892958 0.58866111 0.58912095\n",
      "  0.58052249]\n",
      " [0.56257272 0.56221361 0.59014625 0.58966168 0.58439028 0.5845816\n",
      "  0.58409037]\n",
      " [0.56553746 0.56525246 0.59971721 0.5989774  0.59806109 0.59769787\n",
      "  0.5970697 ]\n",
      " [0.56035514 0.56084482 0.60243502 0.60188826 0.60109548 0.59961621\n",
      "  0.59540668]\n",
      " [0.55901388 0.55916237 0.59751779 0.5976576  0.59153561 0.59170384\n",
      "  0.5990635 ]\n",
      " [0.57366996 0.57332213 0.60359059 0.60264372 0.60226869 0.60163534\n",
      "  0.60417621]\n",
      " [0.5697972  0.56903154 0.60294414 0.60252629 0.60330454 0.60333169\n",
      "  0.59987244]\n",
      " [0.54253316 0.54432534 0.59594759 0.59566794 0.58966277 0.58960932\n",
      "  0.59293145]\n",
      " [0.56104869 0.55999362 0.5986354  0.599433   0.59412423 0.59366037\n",
      "  0.5910962 ]\n",
      " [0.5600366  0.55986567 0.58854319 0.58856632 0.58839303 0.58903245\n",
      "  0.585833  ]\n",
      " [0.56597254 0.56633164 0.60462161 0.6038264  0.6022332  0.60094643\n",
      "  0.60319678]\n",
      " [0.57489315 0.57335338 0.5969909  0.59615999 0.5957372  0.595085\n",
      "  0.5908623 ]\n",
      " [0.56570391 0.56815545 0.62059268 0.62100694 0.61832511 0.61801847\n",
      "  0.61885333]\n",
      " [0.55581663 0.55393578 0.59687096 0.59659623 0.58948844 0.58979095\n",
      "  0.59222127]\n",
      " [0.56596124 0.5665227  0.60637925 0.60667896 0.60729117 0.60617545\n",
      "  0.60363631]\n",
      " [0.56814275 0.56884893 0.60532798 0.60522397 0.60447049 0.60397233\n",
      "  0.60665118]\n",
      " [0.56365651 0.56341754 0.60193142 0.60148101 0.59638925 0.59579182\n",
      "  0.59616231]\n",
      " [0.56886031 0.56910389 0.5935841  0.59281785 0.59213592 0.59273471\n",
      "  0.59468683]\n",
      " [0.56810288 0.56862194 0.61417688 0.61433331 0.6080865  0.60904858\n",
      "  0.60911522]\n",
      " [0.5721686  0.57267255 0.61251604 0.61322872 0.60968849 0.61001398\n",
      "  0.60985871]\n",
      " [0.57829091 0.57928595 0.61225183 0.61113448 0.60968926 0.60977408\n",
      "  0.60660825]\n",
      " [0.5743246  0.57335869 0.60450411 0.60413109 0.60363728 0.60351373\n",
      "  0.6010893 ]\n",
      " [0.56689249 0.56624359 0.59850145 0.59786641 0.59463659 0.59363714\n",
      "  0.59164179]\n",
      " [0.55839002 0.56010165 0.60390906 0.60240687 0.60015398 0.60091175\n",
      "  0.59104476]\n",
      " [0.56839328 0.56923331 0.600286   0.60034108 0.59750957 0.59791611\n",
      "  0.59768669]\n",
      " [0.57185749 0.57220683 0.61365574 0.61309264 0.61179556 0.61229851\n",
      "  0.60819954]\n",
      " [0.54823027 0.54850703 0.59328469 0.59294237 0.5885311  0.58834424\n",
      "  0.59329529]\n",
      " [0.566432   0.5668301  0.59667922 0.59517432 0.59722578 0.59694723\n",
      "  0.59631276]\n",
      " [0.56721794 0.56614437 0.60757602 0.60776631 0.60480246 0.60431148\n",
      "  0.60292   ]\n",
      " [0.5652498  0.56655668 0.61221927 0.61306179 0.61133718 0.61126507\n",
      "  0.6132977 ]\n",
      " [0.57776468 0.57709158 0.62019505 0.61996884 0.6182671  0.61836201\n",
      "  0.61528124]\n",
      " [0.54683032 0.54741023 0.59514733 0.59466934 0.58898269 0.58909321\n",
      "  0.59383481]\n",
      " [0.55624722 0.55702529 0.58732861 0.5878363  0.58679463 0.58725653\n",
      "  0.58427247]\n",
      " [0.57833515 0.57606467 0.59852931 0.59841248 0.59908298 0.59895329\n",
      "  0.59427222]]\n"
     ]
    }
   ],
   "source": [
    "# Number of times to repeat the process\n",
    "num_repeats = 50\n",
    "\n",
    "# Initialize an empty matrix (10-by-4) to store accuracies\n",
    "accuracies = np.zeros((num_repeats, 7))\n",
    "auc_accuracies = np.zeros((num_repeats, 7))\n",
    "\n",
    "seed = 42\n",
    "# Repeat the process and store accuracies\n",
    "for i in range(num_repeats):\n",
    "    np.random.seed(seed)\n",
    "    accuracies[i], auc_accuracies[i] = iterate_process(X, y)\n",
    "    seed += 2\n",
    "    \n",
    "\n",
    "# Print the accuracies\n",
    "print(\"Accuracies:\", accuracies)\n",
    "print(\"AUC:\", auc_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dba311e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64300687, 0.64300687, 0.64402062, 0.64402062, 0.64416667,\n",
       "       0.64425258, 0.6432732 ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1be4cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0087261 , 0.0087261 , 0.00829162, 0.00842277, 0.00840252,\n",
       "       0.00841692, 0.00864426])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69201422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.56738092, 0.56751404, 0.6042743 , 0.6040363 , 0.60214235,\n",
       "       0.60206266, 0.60131121])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(auc_accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a96ed199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00972318, 0.00964044, 0.00903386, 0.00909138, 0.00954365,\n",
       "       0.00945556, 0.00982095])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(auc_accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c260e8ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
