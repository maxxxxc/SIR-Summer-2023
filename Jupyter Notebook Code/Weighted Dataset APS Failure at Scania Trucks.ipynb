{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b0ddd4b",
   "metadata": {},
   "source": [
    "# APS Failure at Scania Trucks\n",
    "\n",
    "Dataset is from https://archive.ics.uci.edu/dataset/421/aps+failure+at+scania+trucks\n",
    "\n",
    "n = 60000\n",
    "\n",
    "171 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "961a7d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b861a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/maxxxxc/SIR-Summer-2023/main/dataset/aps_failure_test_set.csv\"\n",
    "#df = pd.read_csv(url, na_values = 'na')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6462e0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training dataset from CSV\n",
    "train_df = pd.read_csv(url, na_values = 'na')\n",
    "\n",
    "# Read the test dataset from CSV\n",
    "test_df = pd.read_csv(url, na_values = 'na')\n",
    "\n",
    "# Add a 'Label' column to the test dataset and fill it with NaN values\n",
    "#test_df['Label'] = float('nan')\n",
    "\n",
    "# Concatenate the training and test datasets\n",
    "df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# Save the combined dataset to a new CSV file\n",
    "df.to_csv('combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9658e176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000, 171)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c99a4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  class  aa_000  ab_000  ac_000  ad_000  ae_000  af_000  ag_000  ag_001  \\\n",
      "0   neg      60     0.0    20.0    12.0     0.0     0.0     0.0     0.0   \n",
      "1   neg      82     0.0    68.0    40.0     0.0     0.0     0.0     0.0   \n",
      "2   neg   66002     2.0   212.0   112.0     0.0     0.0     0.0     0.0   \n",
      "3   neg   59816     NaN  1010.0   936.0     0.0     0.0     0.0     0.0   \n",
      "4   neg    1814     NaN   156.0   140.0     0.0     0.0     0.0     0.0   \n",
      "\n",
      "   ag_002  ...    ee_002    ee_003    ee_004    ee_005     ee_006    ee_007  \\\n",
      "0     0.0  ...    1098.0     138.0     412.0     654.0       78.0      88.0   \n",
      "1     0.0  ...    1068.0     276.0    1620.0     116.0       86.0     462.0   \n",
      "2     0.0  ...  495076.0  380368.0  440134.0  269556.0  1315022.0  153680.0   \n",
      "3     0.0  ...  540820.0  243270.0  483302.0  485332.0   431376.0  210074.0   \n",
      "4     0.0  ...    7646.0    4144.0   18466.0   49782.0     3176.0     482.0   \n",
      "\n",
      "     ee_008  ee_009  ef_000  eg_000  \n",
      "0       0.0     0.0     0.0     0.0  \n",
      "1       0.0     0.0     0.0     0.0  \n",
      "2     516.0     0.0     0.0     0.0  \n",
      "3  281662.0  3232.0     0.0     0.0  \n",
      "4      76.0     0.0     0.0     0.0  \n",
      "\n",
      "[5 rows x 171 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d55d1141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "br_000    26258\n",
      "bq_000    25962\n",
      "bp_000    25442\n",
      "bo_000    24752\n",
      "ab_000    24726\n",
      "          ...  \n",
      "cj_000      172\n",
      "ci_000      172\n",
      "bt_000       56\n",
      "aa_000        0\n",
      "class         0\n",
      "Length: 171, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values_count = df.isna().sum()\n",
    "\n",
    "print(missing_values_count.sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4330eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff409132",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"class\", axis=1)\n",
    "y = df[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45f98305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing for na values\n",
    "threshold = len(X) * 0.5\n",
    "X = X.dropna(thresh = threshold, axis = 1)\n",
    "\n",
    "X = X.fillna(X.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b12ec0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa_000</th>\n",
       "      <th>ac_000</th>\n",
       "      <th>ad_000</th>\n",
       "      <th>ae_000</th>\n",
       "      <th>af_000</th>\n",
       "      <th>ag_000</th>\n",
       "      <th>ag_001</th>\n",
       "      <th>ag_002</th>\n",
       "      <th>ag_003</th>\n",
       "      <th>ag_004</th>\n",
       "      <th>...</th>\n",
       "      <th>ee_002</th>\n",
       "      <th>ee_003</th>\n",
       "      <th>ee_004</th>\n",
       "      <th>ee_005</th>\n",
       "      <th>ee_006</th>\n",
       "      <th>ee_007</th>\n",
       "      <th>ee_008</th>\n",
       "      <th>ee_009</th>\n",
       "      <th>ef_000</th>\n",
       "      <th>eg_000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2682.0</td>\n",
       "      <td>4736.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1098.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>412.0</td>\n",
       "      <td>654.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>68.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>748.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1068.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66002</td>\n",
       "      <td>212.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199486.0</td>\n",
       "      <td>1358536.0</td>\n",
       "      <td>...</td>\n",
       "      <td>495076.0</td>\n",
       "      <td>380368.0</td>\n",
       "      <td>440134.0</td>\n",
       "      <td>269556.0</td>\n",
       "      <td>1315022.0</td>\n",
       "      <td>153680.0</td>\n",
       "      <td>516.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59816</td>\n",
       "      <td>1010.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123922.0</td>\n",
       "      <td>...</td>\n",
       "      <td>540820.0</td>\n",
       "      <td>243270.0</td>\n",
       "      <td>483302.0</td>\n",
       "      <td>485332.0</td>\n",
       "      <td>431376.0</td>\n",
       "      <td>210074.0</td>\n",
       "      <td>281662.0</td>\n",
       "      <td>3232.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1814</td>\n",
       "      <td>156.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7646.0</td>\n",
       "      <td>4144.0</td>\n",
       "      <td>18466.0</td>\n",
       "      <td>49782.0</td>\n",
       "      <td>3176.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 162 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa_000  ac_000  ad_000  ae_000  af_000  ag_000  ag_001  ag_002    ag_003  \\\n",
       "0      60    20.0    12.0     0.0     0.0     0.0     0.0     0.0    2682.0   \n",
       "1      82    68.0    40.0     0.0     0.0     0.0     0.0     0.0       0.0   \n",
       "2   66002   212.0   112.0     0.0     0.0     0.0     0.0     0.0  199486.0   \n",
       "3   59816  1010.0   936.0     0.0     0.0     0.0     0.0     0.0       0.0   \n",
       "4    1814   156.0   140.0     0.0     0.0     0.0     0.0     0.0       0.0   \n",
       "\n",
       "      ag_004  ...    ee_002    ee_003    ee_004    ee_005     ee_006  \\\n",
       "0     4736.0  ...    1098.0     138.0     412.0     654.0       78.0   \n",
       "1      748.0  ...    1068.0     276.0    1620.0     116.0       86.0   \n",
       "2  1358536.0  ...  495076.0  380368.0  440134.0  269556.0  1315022.0   \n",
       "3   123922.0  ...  540820.0  243270.0  483302.0  485332.0   431376.0   \n",
       "4       72.0  ...    7646.0    4144.0   18466.0   49782.0     3176.0   \n",
       "\n",
       "     ee_007    ee_008  ee_009  ef_000  eg_000  \n",
       "0      88.0       0.0     0.0     0.0     0.0  \n",
       "1     462.0       0.0     0.0     0.0     0.0  \n",
       "2  153680.0     516.0     0.0     0.0     0.0  \n",
       "3  210074.0  281662.0  3232.0     0.0     0.0  \n",
       "4     482.0      76.0     0.0     0.0     0.0  \n",
       "\n",
       "[5 rows x 162 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c6d4ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neg    31250\n",
       "pos      750\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c770dce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000, 162)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "848faca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    31250\n",
       "1      750\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.replace({\"neg\" : 0, \"pos\" : 1})\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201d8a20",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b2ba396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9905729166666667\n",
      "Test Accuracy: 0.987734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\panda\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "#model = LogisticRegression()\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a20454",
   "metadata": {},
   "source": [
    "# Divide and Conquer\n",
    "\n",
    "Divide the training data into 11 batches, train a logistic model on each of the batch, and then combine the 11 prediction results. Consider the following two ensemble methods:\n",
    "- majority voting\n",
    "- average (or sum) of the logit output and then make decision based on its sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "806458ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_process(X, y):\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "    \n",
    "    # Define the number of batches\n",
    "    num_batches = 11\n",
    "    \n",
    "    # Randomly shuffle the data indices\n",
    "    #indices = np.random.permutation(len(X))\n",
    "    \n",
    "    #change 7/12\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    \n",
    "    # Calculate the batch size\n",
    "    batch_size = len(X_train) // num_batches\n",
    "    \n",
    "    # Make predictions on the test set using majority voting\n",
    "    preds_voting = np.zeros(len(y_test))\n",
    "    # Make predictions on the test set using average of logit\n",
    "    preds_logit = np.zeros(len(y_test))\n",
    "    #Make predictions on the test set using average of probs\n",
    "    preds_prob = np.zeros(len(y_test))\n",
    "    \n",
    "    preds_voting_weighted = np.zeros(len(y_test))\n",
    "    preds_logit_weighted = np.zeros(len(y_test))\n",
    "    preds_prob_weighted = np.zeros(len(y_test))\n",
    "    \n",
    "    total_cverr = 0\n",
    "    \n",
    "    # Split the training data into batches, fit a logistic regression model on each batch\n",
    "    for i in range(num_batches):\n",
    "        # Calculate the starting and ending indices for the current batch\n",
    "        start_index = i * batch_size\n",
    "        end_index = (i + 1) * batch_size\n",
    "        \n",
    "        # Create a logistic regression model\n",
    "        model = LogisticRegression(max_iter=500)\n",
    "        \n",
    "        # Select the current batch for training\n",
    "        #X_batch = X_train[start_index:end_index]\n",
    "        #y_batch = y_train[start_index:end_index]\n",
    "        \n",
    "        #change 7/12\n",
    "        X_batch = X_train.iloc[indices[start_index:end_index]]\n",
    "        y_batch = y_train.iloc[indices[start_index:end_index]]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_batch_scaled = scaler.fit_transform(X_batch)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Fit the model on the current batch\n",
    "        model.fit(X_batch_scaled, y_batch)\n",
    "        current_cverr = cross_val_score(model, X_batch_scaled, y_batch, cv = 5, scoring = 'accuracy').mean()\n",
    "        total_cverr += current_cverr\n",
    "               \n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        # Accumulate the predictions using majority voting\n",
    "        preds_voting += (y_pred == 1)\n",
    "        preds_voting_weighted += (y_pred == 1) * current_cverr\n",
    "    \n",
    "        # Accumulate the predictions using majority voting\n",
    "        y_pred = model.decision_function(X_test_scaled)\n",
    "        preds_logit += y_pred\n",
    "        preds_logit_weighted += y_pred * current_cverr\n",
    "        \n",
    "        #Accumulate the probs\n",
    "        y_pred = model.predict_proba(X_test_scaled)\n",
    "        preds_prob += y_pred[:,1]\n",
    "        preds_prob_weighted += y_pred[:,1] * current_cverr\n",
    "    \n",
    "    accuracy = np.zeros(7)\n",
    "    auc_accuracy = np.zeros(7)\n",
    "    \n",
    "    preds_voting_weighted = preds_voting_weighted / total_cverr * num_batches\n",
    "    preds_logit_weighted = preds_logit_weighted / total_cverr * num_batches\n",
    "    preds_prob_weighted = preds_prob_weighted / total_cverr * num_batches\n",
    "    \n",
    "    # Majority voting (selecting the most frequent prediction for each sample)\n",
    "    final_predictions = np.where(preds_voting > num_batches / 2, 1, 0)\n",
    "    accuracy[0] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[0] = roc_auc_score(y_test, preds_voting)\n",
    "    \n",
    "    final_predictions = np.where(preds_voting_weighted > num_batches / 2, 1, 0)\n",
    "    accuracy[1] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[1] = roc_auc_score(y_test, preds_voting_weighted)\n",
    "    \n",
    "    # Average of logit\n",
    "    final_predictions = np.where(preds_logit > 0, 1, 0)\n",
    "    accuracy[2] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[2] = roc_auc_score(y_test, preds_logit)\n",
    "    \n",
    "    final_predictions = np.where(preds_logit_weighted > 0, 1, 0)\n",
    "    accuracy[3] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[3] = roc_auc_score(y_test, preds_logit_weighted)\n",
    "    \n",
    "    #Average of probs\n",
    "    final_predictions = np.where(preds_prob / num_batches > 0.5, 1, 0)\n",
    "    accuracy[4] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[4] = roc_auc_score(y_test, preds_prob)\n",
    "    \n",
    "    final_predictions = np.where(preds_prob_weighted / num_batches > 0.5, 1, 0)\n",
    "    accuracy[5] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[5] = roc_auc_score(y_test, preds_prob_weighted)\n",
    "    \n",
    "    # Train a model on all 11 batches of training data\n",
    "    model = LogisticRegression(max_iter=500)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy[6] = accuracy_score(y_test, y_pred)\n",
    "    y_pred = model.decision_function(X_test_scaled)\n",
    "    auc_accuracy[6] = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, auc_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273e621",
   "metadata": {},
   "source": [
    "Try the divide and conquer approaches 10 times, reporting the following error matrix. \n",
    "- 10-by-4\n",
    "- col_1: majority voting\n",
    "- col_2: average the logit\n",
    "- col_3: average probabilities\n",
    "- col_4: using the model trained on all the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67dade4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies: [[0.9884375  0.9884375  0.98554688 0.98554688 0.988125   0.988125\n",
      "  0.990625  ]\n",
      " [0.98882812 0.98882812 0.98710937 0.98710937 0.98914062 0.98914062\n",
      "  0.99070313]\n",
      " [0.98898438 0.98898438 0.98726563 0.98726563 0.98882812 0.98882812\n",
      "  0.99046875]\n",
      " [0.989375   0.989375   0.98820312 0.98820312 0.98945313 0.98945313\n",
      "  0.9909375 ]\n",
      " [0.98773438 0.98773438 0.98515625 0.98515625 0.98835938 0.98835938\n",
      "  0.98960938]\n",
      " [0.986875   0.986875   0.9859375  0.9859375  0.98734375 0.98734375\n",
      "  0.98882812]\n",
      " [0.98929687 0.98929687 0.98664063 0.98648437 0.98914062 0.98914062\n",
      "  0.99195312]\n",
      " [0.98796875 0.98796875 0.98671875 0.98664063 0.98820312 0.98820312\n",
      "  0.99171875]\n",
      " [0.9875     0.9875     0.98546875 0.98546875 0.98757812 0.98757812\n",
      "  0.99054688]\n",
      " [0.98898438 0.98898438 0.98765625 0.98765625 0.98921875 0.98921875\n",
      "  0.99140625]\n",
      " [0.98929687 0.98929687 0.9865625  0.9865625  0.98859375 0.98859375\n",
      "  0.99007813]\n",
      " [0.98914062 0.98914062 0.98796875 0.98796875 0.98875    0.98875\n",
      "  0.99179688]\n",
      " [0.98875    0.98875    0.98890625 0.98890625 0.9890625  0.9890625\n",
      "  0.99078125]\n",
      " [0.989375   0.989375   0.98734375 0.98734375 0.9896875  0.9896875\n",
      "  0.99070313]\n",
      " [0.9878125  0.9878125  0.98710937 0.98710937 0.9878125  0.9878125\n",
      "  0.98984375]\n",
      " [0.98960938 0.98960938 0.98796875 0.98796875 0.98945313 0.98945313\n",
      "  0.99195312]\n",
      " [0.988125   0.988125   0.9871875  0.98710937 0.98789063 0.98789063\n",
      "  0.98984375]\n",
      " [0.98890625 0.98890625 0.98859375 0.98859375 0.98851563 0.98851563\n",
      "  0.99140625]\n",
      " [0.98851563 0.98851563 0.98734375 0.98734375 0.98828125 0.98828125\n",
      "  0.98992187]\n",
      " [0.98710937 0.98710937 0.98460938 0.98460938 0.98734375 0.98734375\n",
      "  0.98945313]\n",
      " [0.98828125 0.98828125 0.9846875  0.9846875  0.98820312 0.98828125\n",
      "  0.99101562]\n",
      " [0.98921875 0.98921875 0.9859375  0.9859375  0.98914062 0.98914062\n",
      "  0.9903125 ]\n",
      " [0.98765625 0.98765625 0.9871875  0.9871875  0.98773438 0.98773438\n",
      "  0.98945313]\n",
      " [0.98710937 0.98710937 0.98617188 0.98617188 0.98742188 0.98742188\n",
      "  0.99039062]\n",
      " [0.98921875 0.98921875 0.98875    0.98875    0.98976562 0.98976562\n",
      "  0.9896875 ]\n",
      " [0.98679688 0.98679688 0.98578125 0.9859375  0.98632812 0.98632812\n",
      "  0.98820312]\n",
      " [0.98890625 0.98890625 0.98640625 0.98640625 0.98851563 0.98851563\n",
      "  0.9903125 ]\n",
      " [0.98828125 0.98828125 0.98734375 0.98734375 0.98828125 0.98828125\n",
      "  0.98984375]\n",
      " [0.9871875  0.9871875  0.98617188 0.98617188 0.98726563 0.98726563\n",
      "  0.99117188]\n",
      " [0.99       0.99       0.98695312 0.98695312 0.99039062 0.99039062\n",
      "  0.99054688]\n",
      " [0.98945313 0.98945313 0.9884375  0.98835938 0.98992187 0.98992187\n",
      "  0.99117188]\n",
      " [0.98859375 0.98859375 0.98671875 0.98671875 0.98851563 0.9884375\n",
      "  0.98976562]\n",
      " [0.98890625 0.98890625 0.98804687 0.98804687 0.98921875 0.98921875\n",
      "  0.9896875 ]\n",
      " [0.9884375  0.9884375  0.98554688 0.98554688 0.98820312 0.98820312\n",
      "  0.99015625]\n",
      " [0.98671875 0.98671875 0.98398438 0.98398438 0.98695312 0.98695312\n",
      "  0.99007813]\n",
      " [0.98765625 0.98765625 0.98648437 0.98648437 0.98757812 0.98757812\n",
      "  0.99046875]\n",
      " [0.98789063 0.98789063 0.98632812 0.98632812 0.9878125  0.9878125\n",
      "  0.99015625]\n",
      " [0.98710937 0.98710937 0.98523437 0.98523437 0.98664063 0.98664063\n",
      "  0.988125  ]\n",
      " [0.98882812 0.98882812 0.9878125  0.9878125  0.98898438 0.98898438\n",
      "  0.9903125 ]\n",
      " [0.98914062 0.98914062 0.98828125 0.98828125 0.989375   0.989375\n",
      "  0.99125   ]\n",
      " [0.98765625 0.98765625 0.986875   0.986875   0.9878125  0.9878125\n",
      "  0.99039062]\n",
      " [0.98835938 0.98835938 0.98625    0.98625    0.98835938 0.98835938\n",
      "  0.99148437]\n",
      " [0.98757812 0.98757812 0.986875   0.98695312 0.98828125 0.98828125\n",
      "  0.9903125 ]\n",
      " [0.98992187 0.98992187 0.98734375 0.98734375 0.98976562 0.98976562\n",
      "  0.99085937]\n",
      " [0.9884375  0.9884375  0.98726563 0.98734375 0.98835938 0.98835938\n",
      "  0.99      ]\n",
      " [0.98867187 0.98867187 0.98757812 0.9875     0.98859375 0.98867187\n",
      "  0.99195312]\n",
      " [0.98835938 0.98835938 0.98734375 0.98734375 0.9890625  0.9890625\n",
      "  0.99085937]\n",
      " [0.98835938 0.98835938 0.98671875 0.98671875 0.988125   0.988125\n",
      "  0.99023438]\n",
      " [0.9903125  0.9903125  0.98914062 0.98914062 0.99023438 0.99023438\n",
      "  0.9909375 ]\n",
      " [0.98859375 0.98859375 0.98710937 0.98710937 0.98828125 0.98828125\n",
      "  0.990625  ]]\n",
      "AUC: [[0.95560721 0.95548561 0.92539735 0.92534552 0.99225001 0.99224948\n",
      "  0.96670006]\n",
      " [0.98014559 0.98024055 0.95365135 0.95364316 0.99455295 0.99455322\n",
      "  0.981416  ]\n",
      " [0.96041194 0.9604758  0.93915225 0.93919361 0.99168276 0.99168079\n",
      "  0.95136282]\n",
      " [0.98177853 0.98162503 0.93285023 0.93282617 0.99360397 0.99360065\n",
      "  0.98712939]\n",
      " [0.96940087 0.96925227 0.92119927 0.92120684 0.99193582 0.99193447\n",
      "  0.95314094]\n",
      " [0.97159847 0.97144404 0.94052636 0.9405239  0.99036179 0.9903554\n",
      "  0.96130012]\n",
      " [0.98305292 0.98283467 0.9639267  0.96392619 0.99540557 0.99540711\n",
      "  0.97745975]\n",
      " [0.98436338 0.98421597 0.95657468 0.9565639  0.99264813 0.99264868\n",
      "  0.98277779]\n",
      " [0.97818452 0.9781643  0.95701228 0.95695149 0.99325596 0.9932542\n",
      "  0.97672701]\n",
      " [0.97692477 0.97680704 0.93651269 0.93647901 0.98754551 0.98753702\n",
      "  0.97535413]\n",
      " [0.97987497 0.97982016 0.93840045 0.93844886 0.99391449 0.9939134\n",
      "  0.95787546]\n",
      " [0.98502136 0.98490604 0.92897857 0.92901103 0.99395084 0.99395197\n",
      "  0.95661251]\n",
      " [0.97629215 0.97643506 0.96434874 0.96441161 0.99372507 0.99372591\n",
      "  0.97204837]\n",
      " [0.97953968 0.97943886 0.94821763 0.94830911 0.9930505  0.9930483\n",
      "  0.97632055]\n",
      " [0.97979657 0.97974977 0.97149501 0.9715122  0.99367508 0.99367535\n",
      "  0.96775567]\n",
      " [0.97481248 0.97493306 0.92573317 0.92570524 0.99371487 0.99372013\n",
      "  0.98134763]\n",
      " [0.97500832 0.97496792 0.97835495 0.97835309 0.99308578 0.99309293\n",
      "  0.96424249]\n",
      " [0.98010752 0.98014762 0.96979949 0.96975466 0.99485497 0.99485358\n",
      "  0.98382282]\n",
      " [0.97590722 0.9759879  0.95994313 0.95995739 0.99425869 0.99425922\n",
      "  0.96091101]\n",
      " [0.96219965 0.96209883 0.93028219 0.93027403 0.99117318 0.99118085\n",
      "  0.95988891]\n",
      " [0.97203083 0.97225441 0.97103332 0.97108413 0.99369988 0.99370326\n",
      "  0.94941325]\n",
      " [0.97762717 0.9776926  0.97433482 0.9743236  0.99437995 0.99438148\n",
      "  0.95820108]\n",
      " [0.9737809  0.97379331 0.92583619 0.92568751 0.99349917 0.9935026\n",
      "  0.96630294]\n",
      " [0.96085525 0.96076783 0.97748914 0.97749809 0.99297284 0.99297095\n",
      "  0.96903666]\n",
      " [0.96349867 0.96346492 0.96841941 0.96835734 0.99137592 0.99137809\n",
      "  0.96262241]\n",
      " [0.96423232 0.96416882 0.956172   0.95625591 0.99033396 0.9903337\n",
      "  0.95726859]\n",
      " [0.97201049 0.97204442 0.94843716 0.94854461 0.991396   0.99139385\n",
      "  0.95582744]\n",
      " [0.97115628 0.97133048 0.96838193 0.96840208 0.99288153 0.99288747\n",
      "  0.97632945]\n",
      " [0.98386624 0.983804   0.93952744 0.93957651 0.99132185 0.99132297\n",
      "  0.9722643 ]\n",
      " [0.98303698 0.9830077  0.96995717 0.96993414 0.99420973 0.99420584\n",
      "  0.98053591]\n",
      " [0.97718265 0.97703546 0.9516818  0.95160806 0.99431508 0.99431798\n",
      "  0.94686612]\n",
      " [0.97277382 0.9728231  0.96450872 0.96451992 0.99381645 0.9938152\n",
      "  0.96402762]\n",
      " [0.97017011 0.97019802 0.94479153 0.94492665 0.99386436 0.99386407\n",
      "  0.96839002]\n",
      " [0.95956698 0.95961629 0.90550968 0.90584951 0.99210383 0.99210328\n",
      "  0.96152269]\n",
      " [0.96077799 0.96068328 0.89881746 0.89881077 0.99035315 0.99034753\n",
      "  0.9756477 ]\n",
      " [0.97482427 0.97490952 0.94903712 0.94904603 0.99347128 0.99347614\n",
      "  0.98641813]\n",
      " [0.98210764 0.98195535 0.94892578 0.94891505 0.99367411 0.99367186\n",
      "  0.96821351]\n",
      " [0.97659246 0.97660022 0.97950179 0.97928228 0.99262388 0.9926234\n",
      "  0.97810691]\n",
      " [0.97299749 0.97295617 0.93596141 0.93596038 0.99168625 0.99169762\n",
      "  0.94901676]\n",
      " [0.96891549 0.96887667 0.95321416 0.95322771 0.98782659 0.98783111\n",
      "  0.96668813]\n",
      " [0.97073781 0.9707503  0.93871439 0.93850237 0.99062539 0.99063233\n",
      "  0.97878093]\n",
      " [0.98182    0.9817732  0.93278533 0.932776   0.99429093 0.9942912\n",
      "  0.98004933]\n",
      " [0.98037289 0.98042627 0.92434074 0.92435558 0.99342014 0.9934159\n",
      "  0.96590554]\n",
      " [0.98372285 0.98367068 0.91664558 0.91669525 0.99557342 0.99556509\n",
      "  0.94652328]\n",
      " [0.97481475 0.97482711 0.97421396 0.97422658 0.99413735 0.99413477\n",
      "  0.96941514]\n",
      " [0.96461324 0.96464082 0.86778029 0.86778557 0.98806233 0.98806468\n",
      "  0.98646414]\n",
      " [0.9672072  0.96727013 0.97002169 0.97003521 0.99337987 0.99337801\n",
      "  0.94870093]\n",
      " [0.97093464 0.97099052 0.92032641 0.9204023  0.99388466 0.99388934\n",
      "  0.95386216]\n",
      " [0.97933907 0.97945095 0.94651207 0.94647926 0.9949162  0.99492004\n",
      "  0.95824008]\n",
      " [0.97228681 0.97231666 0.94254304 0.9425956  0.99255161 0.99255504\n",
      "  0.97914192]]\n"
     ]
    }
   ],
   "source": [
    "# Number of times to repeat the process\n",
    "num_repeats = 50\n",
    "\n",
    "# Initialize an empty matrix (10-by-4) to store accuracies\n",
    "accuracies = np.zeros((num_repeats, 7))\n",
    "auc_accuracies = np.zeros((num_repeats, 7))\n",
    "\n",
    "seed = 42\n",
    "# Repeat the process and store accuracies\n",
    "for i in range(num_repeats):\n",
    "    np.random.seed(seed)\n",
    "    accuracies[i], auc_accuracies[i] = iterate_process(X, y)\n",
    "    seed += 2\n",
    "    \n",
    "\n",
    "# Print the accuracies\n",
    "print(\"Accuracies:\", accuracies)\n",
    "print(\"AUC:\", auc_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7715f708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98844531, 0.98844531, 0.98688125, 0.98687812, 0.98847812,\n",
       "       0.98847969, 0.99044687])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dba311e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00086873, 0.00086873, 0.00113212, 0.00112771, 0.00089326,\n",
       "       0.00089312, 0.00084621])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05480309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97387763, 0.97386259, 0.94615556, 0.94616122, 0.99278587,\n",
       "       0.99278635, 0.96747953])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(auc_accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3dab21e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00741473, 0.00740439, 0.02263111, 0.02261487, 0.00180167,\n",
       "       0.00180167, 0.01149073])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(auc_accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf2fba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
