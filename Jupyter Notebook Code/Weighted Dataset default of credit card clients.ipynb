{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b0ddd4b",
   "metadata": {},
   "source": [
    "# default of credit card clients\n",
    "\n",
    "Dataset is from https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients\n",
    "\n",
    "n = 30000\n",
    "\n",
    "23 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "961a7d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b861a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/maxxxxc/SIR-Summer-2023/main/dataset/default%20of%20credit%20card%20clients.csv\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9658e176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 24)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c99a4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       X1  X2  X3  X4  X5  X6  X7  X8  X9  X10  ...    X15    X16    X17  \\\n",
      "0   20000   2   2   1  24   2   2  -1  -1   -2  ...      0      0      0   \n",
      "1  120000   2   2   2  26  -1   2   0   0    0  ...   3272   3455   3261   \n",
      "2   90000   2   2   2  34   0   0   0   0    0  ...  14331  14948  15549   \n",
      "3   50000   2   2   1  37   0   0   0   0    0  ...  28314  28959  29547   \n",
      "4   50000   1   2   1  57  -1   0  -1   0    0  ...  20940  19146  19131   \n",
      "\n",
      "    X18    X19    X20   X21   X22   X23  Y  \n",
      "0     0    689      0     0     0     0  1  \n",
      "1     0   1000   1000  1000     0  2000  1  \n",
      "2  1518   1500   1000  1000  1000  5000  0  \n",
      "3  2000   2019   1200  1100  1069  1000  0  \n",
      "4  2000  36681  10000  9000   689   679  0  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4330eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95e39709",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"Y\", axis=1)\n",
    "y = df[\"Y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b12ec0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X14</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>X20</th>\n",
       "      <th>X21</th>\n",
       "      <th>X22</th>\n",
       "      <th>X23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2682</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13559</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>49291</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>35835</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       X1  X2  X3  X4  X5  X6  X7  X8  X9  X10  ...    X14    X15    X16  \\\n",
       "0   20000   2   2   1  24   2   2  -1  -1   -2  ...    689      0      0   \n",
       "1  120000   2   2   2  26  -1   2   0   0    0  ...   2682   3272   3455   \n",
       "2   90000   2   2   2  34   0   0   0   0    0  ...  13559  14331  14948   \n",
       "3   50000   2   2   1  37   0   0   0   0    0  ...  49291  28314  28959   \n",
       "4   50000   1   2   1  57  -1   0  -1   0    0  ...  35835  20940  19146   \n",
       "\n",
       "     X17   X18    X19    X20   X21   X22   X23  \n",
       "0      0     0    689      0     0     0     0  \n",
       "1   3261     0   1000   1000  1000     0  2000  \n",
       "2  15549  1518   1500   1000  1000  1000  5000  \n",
       "3  29547  2000   2019   1200  1100  1069  1000  \n",
       "4  19131  2000  36681  10000  9000   689   679  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c6d4ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    23364\n",
       "1     6636\n",
       "Name: Y, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "848faca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    23364\n",
       "1     6636\n",
       "Name: Y, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y = y.replace({\"neg\" : 0, \"pos\" : 1})\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201d8a20",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b2ba396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7780555555555555\n",
      "Test Accuracy: 0.77975\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "#model = LogisticRegression()\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a20454",
   "metadata": {},
   "source": [
    "# Divide and Conquer\n",
    "\n",
    "Divide the training data into 11 batches, train a logistic model on each of the batch, and then combine the 11 prediction results. Consider the following two ensemble methods:\n",
    "- majority voting\n",
    "- average (or sum) of the logit output and then make decision based on its sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "806458ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_process(X, y):\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "    \n",
    "    # Define the number of batches\n",
    "    num_batches = 11\n",
    "    \n",
    "    # Randomly shuffle the data indices\n",
    "    #indices = np.random.permutation(len(X))\n",
    "    \n",
    "    #change 7/12\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    \n",
    "    # Calculate the batch size\n",
    "    batch_size = len(X_train) // num_batches\n",
    "    \n",
    "    # Make predictions on the test set using majority voting\n",
    "    preds_voting = np.zeros(len(y_test))\n",
    "    # Make predictions on the test set using average of logit\n",
    "    preds_logit = np.zeros(len(y_test))\n",
    "    #Make predictions on the test set using average of probs\n",
    "    preds_prob = np.zeros(len(y_test))\n",
    "    \n",
    "    preds_voting_weighted = np.zeros(len(y_test))\n",
    "    preds_logit_weighted = np.zeros(len(y_test))\n",
    "    preds_prob_weighted = np.zeros(len(y_test))\n",
    "    \n",
    "    total_cverr = 0\n",
    "    \n",
    "    # Split the training data into batches, fit a logistic regression model on each batch\n",
    "    for i in range(num_batches):\n",
    "        # Calculate the starting and ending indices for the current batch\n",
    "        start_index = i * batch_size\n",
    "        end_index = (i + 1) * batch_size\n",
    "        \n",
    "        # Create a logistic regression model\n",
    "        model = LogisticRegression(max_iter=500)\n",
    "        \n",
    "        # Select the current batch for training\n",
    "        #X_batch = X_train[start_index:end_index]\n",
    "        #y_batch = y_train[start_index:end_index]\n",
    "        \n",
    "        #change 7/12\n",
    "        X_batch = X_train.iloc[indices[start_index:end_index]]\n",
    "        y_batch = y_train.iloc[indices[start_index:end_index]]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_batch_scaled = scaler.fit_transform(X_batch)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Fit the model on the current batch\n",
    "        model.fit(X_batch_scaled, y_batch)\n",
    "        current_cverr = cross_val_score(model, X_batch_scaled, y_batch, cv = 5, scoring = 'accuracy').mean()\n",
    "        total_cverr += current_cverr\n",
    "               \n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        # Accumulate the predictions using majority voting\n",
    "        preds_voting += (y_pred == 1)\n",
    "        preds_voting_weighted += (y_pred == 1) * current_cverr\n",
    "    \n",
    "        # Accumulate the predictions using majority voting\n",
    "        y_pred = model.decision_function(X_test_scaled)\n",
    "        preds_logit += y_pred\n",
    "        preds_logit_weighted += y_pred * current_cverr\n",
    "        \n",
    "        #Accumulate the probs\n",
    "        y_pred = model.predict_proba(X_test_scaled)\n",
    "        preds_prob += y_pred[:,1]\n",
    "        preds_prob_weighted += y_pred[:,1] * current_cverr\n",
    "    \n",
    "    accuracy = np.zeros(7)\n",
    "    auc_accuracy = np.zeros(7)\n",
    "    \n",
    "    preds_voting_weighted = preds_voting_weighted / total_cverr * num_batches\n",
    "    preds_logit_weighted = preds_logit_weighted / total_cverr * num_batches\n",
    "    preds_prob_weighted = preds_prob_weighted / total_cverr * num_batches\n",
    "    \n",
    "    # Majority voting (selecting the most frequent prediction for each sample)\n",
    "    final_predictions = np.where(preds_voting > num_batches / 2, 1, 0)\n",
    "    accuracy[0] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[0] = roc_auc_score(y_test, preds_voting)\n",
    "    \n",
    "    final_predictions = np.where(preds_voting_weighted > num_batches / 2, 1, 0)\n",
    "    accuracy[1] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[1] = roc_auc_score(y_test, preds_voting_weighted)\n",
    "    \n",
    "    # Average of logit\n",
    "    final_predictions = np.where(preds_logit > 0, 1, 0)\n",
    "    accuracy[2] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[2] = roc_auc_score(y_test, preds_logit)\n",
    "    \n",
    "    final_predictions = np.where(preds_logit_weighted > 0, 1, 0)\n",
    "    accuracy[3] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[3] = roc_auc_score(y_test, preds_logit_weighted)\n",
    "    \n",
    "    #Average of probs\n",
    "    final_predictions = np.where(preds_prob / num_batches > 0.5, 1, 0)\n",
    "    accuracy[4] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[4] = roc_auc_score(y_test, preds_prob)\n",
    "    \n",
    "    final_predictions = np.where(preds_prob_weighted / num_batches > 0.5, 1, 0)\n",
    "    accuracy[5] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[5] = roc_auc_score(y_test, preds_prob_weighted)\n",
    "    \n",
    "    # Train a model on all 11 batches of training data\n",
    "    model = LogisticRegression(max_iter=500)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy[6] = accuracy_score(y_test, y_pred)\n",
    "    y_pred = model.decision_function(X_test_scaled)\n",
    "    auc_accuracy[6] = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, auc_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273e621",
   "metadata": {},
   "source": [
    "Try the divide and conquer approaches 10 times, reporting the following error matrix. \n",
    "- 10-by-4\n",
    "- col_1: majority voting\n",
    "- col_2: average the logit\n",
    "- col_3: average probabilities\n",
    "- col_4: using the model trained on all the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67dade4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies: [[0.80741667 0.80741667 0.80741667 0.80766667 0.8075     0.80766667\n",
      "  0.80816667]\n",
      " [0.80825    0.80825    0.80891667 0.80891667 0.80891667 0.809\n",
      "  0.80941667]\n",
      " [0.809      0.809      0.8105     0.81066667 0.81058333 0.81058333\n",
      "  0.81158333]\n",
      " [0.81191667 0.81191667 0.81175    0.81175    0.81166667 0.81166667\n",
      "  0.81266667]\n",
      " [0.81591667 0.81591667 0.81616667 0.81608333 0.81616667 0.81616667\n",
      "  0.81641667]\n",
      " [0.81058333 0.81058333 0.81166667 0.81175    0.81158333 0.81166667\n",
      "  0.81225   ]\n",
      " [0.81016667 0.81016667 0.80908333 0.809      0.80891667 0.809\n",
      "  0.81016667]\n",
      " [0.81125    0.81125    0.81133333 0.81141667 0.81141667 0.81141667\n",
      "  0.812     ]\n",
      " [0.81291667 0.81291667 0.81166667 0.81175    0.81183333 0.81175\n",
      "  0.812     ]\n",
      " [0.80941667 0.80941667 0.8085     0.80858333 0.8085     0.80858333\n",
      "  0.80833333]\n",
      " [0.81066667 0.81066667 0.81066667 0.81066667 0.81058333 0.81058333\n",
      "  0.81041667]\n",
      " [0.81025    0.81025    0.80908333 0.80908333 0.80933333 0.80933333\n",
      "  0.81083333]\n",
      " [0.805      0.805      0.80541667 0.80541667 0.8055     0.8055\n",
      "  0.80658333]\n",
      " [0.81516667 0.81516667 0.81383333 0.81383333 0.81383333 0.81383333\n",
      "  0.81441667]\n",
      " [0.80858333 0.80858333 0.80891667 0.80891667 0.809      0.809\n",
      "  0.80958333]\n",
      " [0.81       0.81       0.80975    0.80966667 0.80975    0.80983333\n",
      "  0.81025   ]\n",
      " [0.8065     0.8065     0.80758333 0.8075     0.8075     0.80766667\n",
      "  0.8075    ]\n",
      " [0.812      0.812      0.81041667 0.81033333 0.81025    0.81025\n",
      "  0.81141667]\n",
      " [0.81208333 0.81208333 0.81208333 0.81208333 0.81208333 0.81216667\n",
      "  0.81325   ]\n",
      " [0.81266667 0.81266667 0.81225    0.81225    0.81225    0.81225\n",
      "  0.81275   ]\n",
      " [0.81066667 0.81066667 0.81075    0.81075    0.81066667 0.81066667\n",
      "  0.81108333]\n",
      " [0.80491667 0.80491667 0.80533333 0.80541667 0.80541667 0.80541667\n",
      "  0.80575   ]\n",
      " [0.8075     0.8075     0.80858333 0.80841667 0.80825    0.80833333\n",
      "  0.80875   ]\n",
      " [0.81133333 0.81133333 0.81258333 0.8125     0.81258333 0.81258333\n",
      "  0.81283333]\n",
      " [0.81008333 0.81008333 0.81025    0.81025    0.81033333 0.81033333\n",
      "  0.81166667]\n",
      " [0.806      0.806      0.80625    0.80608333 0.80616667 0.80608333\n",
      "  0.80733333]\n",
      " [0.802      0.802      0.80333333 0.80341667 0.80333333 0.80333333\n",
      "  0.80391667]\n",
      " [0.81008333 0.81008333 0.81075    0.81083333 0.81075    0.81066667\n",
      "  0.81108333]\n",
      " [0.80916667 0.80916667 0.80833333 0.80841667 0.80841667 0.80825\n",
      "  0.8085    ]\n",
      " [0.80516667 0.80516667 0.80583333 0.80591667 0.806      0.80591667\n",
      "  0.80641667]\n",
      " [0.81008333 0.81008333 0.80966667 0.80958333 0.80983333 0.80975\n",
      "  0.81058333]\n",
      " [0.81133333 0.81133333 0.80983333 0.80966667 0.81       0.80991667\n",
      "  0.811     ]\n",
      " [0.80441667 0.80441667 0.80466667 0.80458333 0.80466667 0.80458333\n",
      "  0.80491667]\n",
      " [0.81075    0.81075    0.80841667 0.80841667 0.80833333 0.80833333\n",
      "  0.80983333]\n",
      " [0.81375    0.81375    0.81433333 0.8145     0.8145     0.8145\n",
      "  0.81483333]\n",
      " [0.81058333 0.81058333 0.81108333 0.81116667 0.81116667 0.811\n",
      "  0.81141667]\n",
      " [0.8125     0.8125     0.812      0.81208333 0.81216667 0.81208333\n",
      "  0.81175   ]\n",
      " [0.80516667 0.80516667 0.80425    0.80433333 0.80416667 0.80425\n",
      "  0.80458333]\n",
      " [0.80883333 0.80883333 0.808      0.808      0.808      0.808\n",
      "  0.80941667]\n",
      " [0.81066667 0.81066667 0.81275    0.81275    0.81275    0.81275\n",
      "  0.81358333]\n",
      " [0.80791667 0.80791667 0.8065     0.80641667 0.8065     0.80658333\n",
      "  0.80641667]\n",
      " [0.81708333 0.81708333 0.816      0.81608333 0.81591667 0.81591667\n",
      "  0.81608333]\n",
      " [0.80825    0.80825    0.80875    0.80908333 0.80883333 0.80891667\n",
      "  0.80941667]\n",
      " [0.80925    0.80925    0.81033333 0.81041667 0.81033333 0.81033333\n",
      "  0.81125   ]\n",
      " [0.81108333 0.81108333 0.81       0.81       0.80991667 0.81008333\n",
      "  0.811     ]\n",
      " [0.80941667 0.80941667 0.81166667 0.81166667 0.81166667 0.81158333\n",
      "  0.81166667]\n",
      " [0.80925    0.80925    0.80841667 0.8085     0.808      0.80808333\n",
      "  0.81      ]\n",
      " [0.80866667 0.80866667 0.80808333 0.80808333 0.80825    0.80825\n",
      "  0.808     ]\n",
      " [0.81416667 0.81416667 0.814      0.814      0.81408333 0.814\n",
      "  0.81458333]\n",
      " [0.80983333 0.80983333 0.81       0.81       0.81       0.81\n",
      "  0.81025   ]]\n",
      "AUC: [[0.66276249 0.66283242 0.72972905 0.72970891 0.73141235 0.73138089\n",
      "  0.72776258]\n",
      " [0.67399086 0.6740458  0.73098525 0.73096505 0.73236886 0.73235313\n",
      "  0.7284064 ]\n",
      " [0.66812073 0.66821378 0.72380389 0.72377553 0.72487546 0.72482974\n",
      "  0.72140944]\n",
      " [0.66619072 0.66614204 0.72322963 0.72322396 0.72462237 0.72461253\n",
      "  0.72122456]\n",
      " [0.67109768 0.67104706 0.72695237 0.72696627 0.72767429 0.7276951\n",
      "  0.72431441]\n",
      " [0.67261579 0.67266857 0.7292534  0.7292574  0.73020844 0.73021744\n",
      "  0.72677969]\n",
      " [0.66945529 0.66948533 0.72370655 0.7236657  0.7251178  0.72506242\n",
      "  0.72150427]\n",
      " [0.66786557 0.6678477  0.72257463 0.72257589 0.72413397 0.72413039\n",
      "  0.72103094]\n",
      " [0.67114432 0.67116998 0.72250224 0.72249527 0.72311598 0.72312244\n",
      "  0.72060044]\n",
      " [0.67458491 0.67448551 0.72683618 0.72682684 0.72820817 0.72819711\n",
      "  0.72474201]\n",
      " [0.66921681 0.66930686 0.72585923 0.72584043 0.72710119 0.72709079\n",
      "  0.723576  ]\n",
      " [0.66971762 0.66973614 0.72705314 0.72703967 0.72814972 0.72814009\n",
      "  0.72495471]\n",
      " [0.65872639 0.6587933  0.72062969 0.72058738 0.72067084 0.72063737\n",
      "  0.71710383]\n",
      " [0.66982449 0.66986533 0.72729338 0.72730352 0.72833254 0.7283546\n",
      "  0.72557185]\n",
      " [0.66383098 0.66384057 0.72234513 0.7223064  0.723049   0.72299991\n",
      "  0.71959989]\n",
      " [0.66602404 0.66602107 0.72403929 0.72402479 0.72556186 0.72553318\n",
      "  0.72262317]\n",
      " [0.66103727 0.66102101 0.72098745 0.72096977 0.72195676 0.72194501\n",
      "  0.71918787]\n",
      " [0.66758823 0.66779607 0.72999686 0.72998425 0.73109895 0.73108736\n",
      "  0.72678615]\n",
      " [0.67059935 0.67068883 0.71902548 0.71900808 0.72073862 0.72071999\n",
      "  0.7158384 ]\n",
      " [0.66739361 0.66743806 0.72837887 0.72837196 0.72947188 0.72946465\n",
      "  0.72557046]\n",
      " [0.66387273 0.66384594 0.72296826 0.72297361 0.72368022 0.72368415\n",
      "  0.72076108]\n",
      " [0.66525604 0.66521041 0.72683304 0.72682508 0.72823816 0.72823151\n",
      "  0.72477737]\n",
      " [0.66865257 0.66877304 0.7222286  0.72222095 0.72368834 0.72367437\n",
      "  0.71974405]\n",
      " [0.66821776 0.6682116  0.71723269 0.71722459 0.71866839 0.71865448\n",
      "  0.7160586 ]\n",
      " [0.66863768 0.66859638 0.72314924 0.72314537 0.72445804 0.72445384\n",
      "  0.7203396 ]\n",
      " [0.66531327 0.66536632 0.72225578 0.72224732 0.72371175 0.72371437\n",
      "  0.72017564]\n",
      " [0.66746423 0.66746866 0.72443845 0.7244301  0.72580102 0.72578647\n",
      "  0.72259679]\n",
      " [0.67052875 0.67060528 0.71911332 0.71909012 0.72009823 0.72007649\n",
      "  0.71525502]\n",
      " [0.66786006 0.66785581 0.72623522 0.72620771 0.7274701  0.72745148\n",
      "  0.72338488]\n",
      " [0.66939193 0.66945201 0.72676206 0.72674062 0.72786234 0.72783259\n",
      "  0.72459585]\n",
      " [0.66572716 0.66570674 0.72617431 0.72616099 0.72577062 0.72574248\n",
      "  0.72201121]\n",
      " [0.67176519 0.67186971 0.72379054 0.72377492 0.72527296 0.72525923\n",
      "  0.72126129]\n",
      " [0.66447568 0.66452469 0.72404134 0.7240047  0.72500789 0.72497109\n",
      "  0.72231536]\n",
      " [0.66260606 0.66261758 0.73639455 0.73636615 0.73767841 0.73764884\n",
      "  0.73258302]\n",
      " [0.66484491 0.66486057 0.72489811 0.72488847 0.7255243  0.72551685\n",
      "  0.72201598]\n",
      " [0.66778089 0.66779344 0.72461551 0.72460059 0.72571476 0.72570638\n",
      "  0.72186694]\n",
      " [0.67746926 0.67743158 0.72794305 0.72795576 0.72893162 0.72894559\n",
      "  0.72634479]\n",
      " [0.66557363 0.66550105 0.72667892 0.72667104 0.7283978  0.72838853\n",
      "  0.72525257]\n",
      " [0.66929947 0.66926988 0.71879003 0.71878161 0.71979501 0.719789\n",
      "  0.71748622]\n",
      " [0.66490875 0.66491541 0.72492104 0.72490215 0.72607438 0.72605979\n",
      "  0.7236583 ]\n",
      " [0.66630505 0.66627611 0.72136265 0.72139224 0.72334673 0.72336815\n",
      "  0.71950321]\n",
      " [0.67138231 0.67147338 0.72858749 0.72860235 0.72912751 0.72912784\n",
      "  0.72609942]\n",
      " [0.66868366 0.66869747 0.71639546 0.71638614 0.71697102 0.71696641\n",
      "  0.71299022]\n",
      " [0.66765919 0.66764593 0.72110995 0.72110091 0.72243334 0.7224299\n",
      "  0.71899091]\n",
      " [0.66879393 0.66891022 0.73039641 0.73041446 0.73173121 0.73174552\n",
      "  0.72864247]\n",
      " [0.66829627 0.66819717 0.72788461 0.72787411 0.72853524 0.72852179\n",
      "  0.72606883]\n",
      " [0.67050718 0.6705081  0.72389055 0.72388982 0.723847   0.72384825\n",
      "  0.72125745]\n",
      " [0.66239961 0.66241908 0.72630202 0.72629005 0.72671517 0.7266965\n",
      "  0.72322459]\n",
      " [0.665852   0.66581424 0.7292391  0.72922997 0.72943867 0.72943392\n",
      "  0.72436698]\n",
      " [0.66234528 0.66247838 0.71824976 0.71823789 0.71893153 0.71892538\n",
      "  0.71590637]]\n"
     ]
    }
   ],
   "source": [
    "# Number of times to repeat the process\n",
    "num_repeats = 50\n",
    "\n",
    "# Initialize an empty matrix (10-by-4) to store accuracies\n",
    "accuracies = np.zeros((num_repeats, 7))\n",
    "auc_accuracies = np.zeros((num_repeats, 7))\n",
    "\n",
    "seed = 42\n",
    "# Repeat the process and store accuracies\n",
    "for i in range(num_repeats):\n",
    "    np.random.seed(seed)\n",
    "    accuracies[i], auc_accuracies[i] = iterate_process(X, y)\n",
    "    seed += 2\n",
    "    \n",
    "\n",
    "# Print the accuracies\n",
    "print(\"Accuracies:\", accuracies)\n",
    "print(\"AUC:\", auc_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dba311e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80979333, 0.80979333, 0.809755  , 0.80977333, 0.80976333,\n",
       "       0.80976833, 0.81035833])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fcbc506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0029566 , 0.0029566 , 0.00281101, 0.00281425, 0.00281949,\n",
       "       0.00280848, 0.00280482])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7f69297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66767315, 0.66769483, 0.72476128, 0.72475054, 0.72581582,\n",
       "       0.72580451, 0.72236244])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(auc_accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c2bbc52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00354171, 0.00354087, 0.00388194, 0.00388339, 0.0039139 ,\n",
       "       0.00391484, 0.00384012])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(auc_accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f55c124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
