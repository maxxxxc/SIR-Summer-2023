{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b0ddd4b",
   "metadata": {},
   "source": [
    "# Spambase\n",
    "\n",
    "Dataset is from https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients\n",
    "\n",
    "n = 4601\n",
    "\n",
    "57 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "961a7d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b861a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/maxxxxc/SIR-Summer-2023/main/dataset/spambase.data\"\n",
    "df = pd.read_csv(url, header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9658e176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 58)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c99a4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0     1     2    3     4     5     6     7     8     9   ...    48  \\\n",
      "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.00   \n",
      "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
      "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
      "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
      "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
      "\n",
      "      49   50     51     52     53     54   55    56  57  \n",
      "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
      "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
      "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
      "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
      "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
      "\n",
      "[5 rows x 58 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4330eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c46f419",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(57, axis=1)\n",
    "y = df[57]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b12ec0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9   ...   47    48  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.0  0.00   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.0  0.00   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.0  0.01   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.0  0.00   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.0  0.00   \n",
       "\n",
       "      49   50     51     52     53     54   55    56  \n",
       "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278  \n",
       "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028  \n",
       "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259  \n",
       "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191  \n",
       "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9c6d4ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2788\n",
       "1    1813\n",
       "Name: 57, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "848faca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2788\n",
       "1    1813\n",
       "Name: 57, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y = y.replace({\"neg\" : 0, \"pos\" : 1})\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201d8a20",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b2ba396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9278985507246377\n",
      "Test Accuracy: 0.9266702878870179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\panda\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "#model = LogisticRegression()\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a20454",
   "metadata": {},
   "source": [
    "# Divide and Conquer\n",
    "\n",
    "Divide the training data into 11 batches, train a logistic model on each of the batch, and then combine the 11 prediction results. Consider the following two ensemble methods:\n",
    "- majority voting\n",
    "- average (or sum) of the logit output and then make decision based on its sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "806458ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_process(X, y):\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "    \n",
    "    # Define the number of batches\n",
    "    num_batches = 11\n",
    "    \n",
    "    # Randomly shuffle the data indices\n",
    "    #indices = np.random.permutation(len(X))\n",
    "    \n",
    "    #change 7/12\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    \n",
    "    # Calculate the batch size\n",
    "    batch_size = len(X_train) // num_batches\n",
    "    \n",
    "    # Make predictions on the test set using majority voting\n",
    "    preds_voting = np.zeros(len(y_test))\n",
    "    # Make predictions on the test set using average of logit\n",
    "    preds_logit = np.zeros(len(y_test))\n",
    "    #Make predictions on the test set using average of probs\n",
    "    preds_prob = np.zeros(len(y_test))\n",
    "    \n",
    "    preds_voting_weighted = np.zeros(len(y_test))\n",
    "    preds_logit_weighted = np.zeros(len(y_test))\n",
    "    preds_prob_weighted = np.zeros(len(y_test))\n",
    "    \n",
    "    total_cverr = 0\n",
    "    \n",
    "    # Split the training data into batches, fit a logistic regression model on each batch\n",
    "    for i in range(num_batches):\n",
    "        # Calculate the starting and ending indices for the current batch\n",
    "        start_index = i * batch_size\n",
    "        end_index = (i + 1) * batch_size\n",
    "        \n",
    "        # Create a logistic regression model\n",
    "        model = LogisticRegression(max_iter=500)\n",
    "        \n",
    "        # Select the current batch for training\n",
    "        #X_batch = X_train[start_index:end_index]\n",
    "        #y_batch = y_train[start_index:end_index]\n",
    "        \n",
    "        #change 7/12\n",
    "        X_batch = X_train.iloc[indices[start_index:end_index]]\n",
    "        y_batch = y_train.iloc[indices[start_index:end_index]]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_batch_scaled = scaler.fit_transform(X_batch)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Fit the model on the current batch\n",
    "        model.fit(X_batch_scaled, y_batch)\n",
    "        current_cverr = cross_val_score(model, X_batch_scaled, y_batch, cv = 5, scoring = 'accuracy').mean()\n",
    "        total_cverr += current_cverr\n",
    "               \n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        # Accumulate the predictions using majority voting\n",
    "        preds_voting += (y_pred == 1)\n",
    "        preds_voting_weighted += (y_pred == 1) * current_cverr\n",
    "    \n",
    "        # Accumulate the predictions using majority voting\n",
    "        y_pred = model.decision_function(X_test_scaled)\n",
    "        preds_logit += y_pred\n",
    "        preds_logit_weighted += y_pred * current_cverr\n",
    "        \n",
    "        #Accumulate the probs\n",
    "        y_pred = model.predict_proba(X_test_scaled)\n",
    "        preds_prob += y_pred[:,1]\n",
    "        preds_prob_weighted += y_pred[:,1] * current_cverr\n",
    "    \n",
    "    accuracy = np.zeros(7)\n",
    "    auc_accuracy = np.zeros(7)\n",
    "    \n",
    "    preds_voting_weighted = preds_voting_weighted / total_cverr * num_batches\n",
    "    preds_logit_weighted = preds_logit_weighted / total_cverr * num_batches\n",
    "    preds_prob_weighted = preds_prob_weighted / total_cverr * num_batches\n",
    "    \n",
    "    # Majority voting (selecting the most frequent prediction for each sample)\n",
    "    final_predictions = np.where(preds_voting > num_batches / 2, 1, 0)\n",
    "    accuracy[0] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[0] = roc_auc_score(y_test, preds_voting)\n",
    "    \n",
    "    final_predictions = np.where(preds_voting_weighted > num_batches / 2, 1, 0)\n",
    "    accuracy[1] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[1] = roc_auc_score(y_test, preds_voting_weighted)\n",
    "    \n",
    "    # Average of logit\n",
    "    final_predictions = np.where(preds_logit > 0, 1, 0)\n",
    "    accuracy[2] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[2] = roc_auc_score(y_test, preds_logit)\n",
    "    \n",
    "    final_predictions = np.where(preds_logit_weighted > 0, 1, 0)\n",
    "    accuracy[3] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[3] = roc_auc_score(y_test, preds_logit_weighted)\n",
    "    \n",
    "    #Average of probs\n",
    "    final_predictions = np.where(preds_prob / num_batches > 0.5, 1, 0)\n",
    "    accuracy[4] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[4] = roc_auc_score(y_test, preds_prob)\n",
    "    \n",
    "    final_predictions = np.where(preds_prob_weighted / num_batches > 0.5, 1, 0)\n",
    "    accuracy[5] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[5] = roc_auc_score(y_test, preds_prob_weighted)\n",
    "    \n",
    "    # Train a model on all 11 batches of training data\n",
    "    model = LogisticRegression(max_iter=500)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy[6] = accuracy_score(y_test, y_pred)\n",
    "    y_pred = model.decision_function(X_test_scaled)\n",
    "    auc_accuracy[6] = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, auc_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273e621",
   "metadata": {},
   "source": [
    "Try the divide and conquer approaches 10 times, reporting the following error matrix. \n",
    "- 10-by-4\n",
    "- col_1: majority voting\n",
    "- col_2: average the logit\n",
    "- col_3: average probabilities\n",
    "- col_4: using the model trained on all the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67dade4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies: [[0.9293862  0.9293862  0.92123846 0.92123846 0.92667029 0.92667029\n",
      "  0.92232482]\n",
      " [0.93047257 0.93047257 0.92829984 0.92829984 0.92992939 0.92992939\n",
      "  0.92829984]\n",
      " [0.92558392 0.92558392 0.91960891 0.91852254 0.92667029 0.92721347\n",
      "  0.92504074]\n",
      " [0.92449756 0.92449756 0.91254753 0.91254753 0.92341119 0.92449756\n",
      "  0.91852254]\n",
      " [0.93155894 0.93155894 0.93047257 0.92992939 0.93481803 0.93481803\n",
      "  0.92992939]\n",
      " [0.92341119 0.92341119 0.91146116 0.91091798 0.92123846 0.92069527\n",
      "  0.92123846]\n",
      " [0.91797936 0.91797936 0.91200435 0.91146116 0.92232482 0.91906573\n",
      "  0.92015209]\n",
      " [0.92395437 0.92395437 0.92286801 0.92286801 0.92884302 0.92884302\n",
      "  0.92884302]\n",
      " [0.92829984 0.92829984 0.92015209 0.92015209 0.92721347 0.92721347\n",
      "  0.91309071]\n",
      " [0.91852254 0.91852254 0.91200435 0.91254753 0.91689299 0.91689299\n",
      "  0.91960891]\n",
      " [0.92178164 0.92178164 0.91254753 0.91254753 0.92341119 0.92341119\n",
      "  0.91743618]\n",
      " [0.91091798 0.91091798 0.90222705 0.90277023 0.91309071 0.91417708\n",
      "  0.91091798]\n",
      " [0.92504074 0.92504074 0.92178164 0.92178164 0.92341119 0.92341119\n",
      "  0.92667029]\n",
      " [0.92504074 0.92504074 0.91634981 0.91580663 0.92341119 0.92395437\n",
      "  0.92015209]\n",
      " [0.9261271  0.9261271  0.92232482 0.92232482 0.92829984 0.92721347\n",
      "  0.9293862 ]\n",
      " [0.91526344 0.91526344 0.91960891 0.91852254 0.92069527 0.92069527\n",
      "  0.92232482]\n",
      " [0.92123846 0.92123846 0.91852254 0.91906573 0.92395437 0.92395437\n",
      "  0.92721347]\n",
      " [0.92015209 0.92015209 0.91146116 0.91200435 0.91797936 0.91743618\n",
      "  0.91309071]\n",
      " [0.92449756 0.92449756 0.92015209 0.91906573 0.92667029 0.92721347\n",
      "  0.9261271 ]\n",
      " [0.92775665 0.92775665 0.92178164 0.92069527 0.92721347 0.92829984\n",
      "  0.93210212]\n",
      " [0.91852254 0.91852254 0.91526344 0.91526344 0.92449756 0.92449756\n",
      "  0.92123846]\n",
      " [0.92449756 0.92449756 0.91526344 0.91526344 0.9261271  0.92558392\n",
      "  0.92341119]\n",
      " [0.91309071 0.91309071 0.90331342 0.9038566  0.91472026 0.91472026\n",
      "  0.92341119]\n",
      " [0.92829984 0.92829984 0.92775665 0.9261271  0.9293862  0.93047257\n",
      "  0.92992939]\n",
      " [0.93210212 0.93210212 0.92069527 0.92123846 0.92992939 0.9293862\n",
      "  0.9261271 ]\n",
      " [0.90765888 0.90765888 0.90765888 0.9071157  0.91363389 0.91309071\n",
      "  0.91906573]\n",
      " [0.92721347 0.92721347 0.91852254 0.91852254 0.92829984 0.92775665\n",
      "  0.93101575]\n",
      " [0.92395437 0.92395437 0.90983161 0.90983161 0.92721347 0.92721347\n",
      "  0.92667029]\n",
      " [0.93210212 0.93210212 0.92123846 0.92123846 0.93318848 0.93318848\n",
      "  0.92884302]\n",
      " [0.92775665 0.92775665 0.92449756 0.92504074 0.92992939 0.92992939\n",
      "  0.91634981]\n",
      " [0.92395437 0.92395437 0.91472026 0.91472026 0.91960891 0.91960891\n",
      "  0.92504074]\n",
      " [0.91960891 0.91960891 0.91852254 0.91906573 0.92015209 0.91960891\n",
      "  0.91526344]\n",
      " [0.91797936 0.91797936 0.91743618 0.91852254 0.92069527 0.92069527\n",
      "  0.92232482]\n",
      " [0.92721347 0.92721347 0.91797936 0.91797936 0.92667029 0.92721347\n",
      "  0.92286801]\n",
      " [0.91852254 0.91852254 0.90820206 0.90657251 0.91743618 0.91634981\n",
      "  0.91852254]\n",
      " [0.92178164 0.92178164 0.91580663 0.91580663 0.92232482 0.92286801\n",
      "  0.9261271 ]\n",
      " [0.9261271  0.9261271  0.92232482 0.92123846 0.92721347 0.92667029\n",
      "  0.92721347]\n",
      " [0.92178164 0.92178164 0.91689299 0.91689299 0.92341119 0.92341119\n",
      "  0.92721347]\n",
      " [0.91743618 0.91743618 0.90820206 0.90928843 0.92069527 0.92069527\n",
      "  0.91906573]\n",
      " [0.92449756 0.92449756 0.90548615 0.90548615 0.92558392 0.92558392\n",
      "  0.91743618]\n",
      " [0.91634981 0.91634981 0.91091798 0.9103748  0.91960891 0.91960891\n",
      "  0.92341119]\n",
      " [0.91743618 0.91743618 0.91743618 0.91743618 0.92015209 0.92015209\n",
      "  0.92775665]\n",
      " [0.92667029 0.92667029 0.92123846 0.92123846 0.92829984 0.92829984\n",
      "  0.91852254]\n",
      " [0.93101575 0.93101575 0.92558392 0.92558392 0.93373167 0.93373167\n",
      "  0.9359044 ]\n",
      " [0.91580663 0.91580663 0.91634981 0.91634981 0.91797936 0.91743618\n",
      "  0.92667029]\n",
      " [0.92069527 0.92069527 0.90439978 0.90548615 0.91689299 0.91634981\n",
      "  0.92504074]\n",
      " [0.92504074 0.92504074 0.92123846 0.92069527 0.92395437 0.92286801\n",
      "  0.92504074]\n",
      " [0.92504074 0.92504074 0.91580663 0.91689299 0.92178164 0.92178164\n",
      "  0.91852254]\n",
      " [0.93047257 0.93047257 0.92504074 0.92558392 0.93210212 0.93155894\n",
      "  0.92992939]\n",
      " [0.92395437 0.92395437 0.92286801 0.92232482 0.92992939 0.9293862\n",
      "  0.92721347]]\n",
      "AUC: [[0.96788118 0.96829697 0.96242405 0.96232392 0.97452645 0.97448372\n",
      "  0.9710524 ]\n",
      " [0.96911263 0.96922252 0.96801061 0.96802543 0.97584636 0.97586118\n",
      "  0.97259649]\n",
      " [0.96371321 0.96426451 0.96710397 0.96713265 0.97379817 0.97385056\n",
      "  0.97158673]\n",
      " [0.95561184 0.95561861 0.96306377 0.96308344 0.96872602 0.9687789\n",
      "  0.97022981]\n",
      " [0.96932963 0.96938546 0.97222388 0.97213081 0.97918621 0.97914526\n",
      "  0.97559027]\n",
      " [0.96256706 0.9620385  0.95912985 0.95921173 0.97036114 0.97030003\n",
      "  0.97114574]\n",
      " [0.96368598 0.96333872 0.95912076 0.95934896 0.97252621 0.97245676\n",
      "  0.96962908]\n",
      " [0.96317201 0.96325817 0.95968042 0.95956798 0.97355515 0.97357592\n",
      "  0.97328262]\n",
      " [0.96647883 0.96638009 0.96433138 0.96435359 0.97381964 0.9737999\n",
      "  0.97186349]\n",
      " [0.96019568 0.96108287 0.96395772 0.96400176 0.97156232 0.97162021\n",
      "  0.96760083]\n",
      " [0.9531616  0.95287856 0.96211921 0.96199206 0.96877888 0.96869697\n",
      "  0.96945378]\n",
      " [0.95167072 0.95161831 0.94564319 0.9453781  0.96415916 0.96411107\n",
      "  0.96431945]\n",
      " [0.96313994 0.96283919 0.96397056 0.96399671 0.97172513 0.97171144\n",
      "  0.97071269]\n",
      " [0.96147268 0.96158965 0.96371546 0.96370561 0.97142572 0.97141957\n",
      "  0.97078299]\n",
      " [0.9608509  0.96035287 0.95694521 0.95685047 0.97072419 0.9706656\n",
      "  0.97072294]\n",
      " [0.96071384 0.96070953 0.96390077 0.96378016 0.96877684 0.96878054\n",
      "  0.96883223]\n",
      " [0.95944488 0.95894025 0.96508588 0.96506258 0.96923937 0.96923079\n",
      "  0.97017014]\n",
      " [0.95463973 0.95464161 0.96392434 0.96387657 0.96772164 0.96769147\n",
      "  0.96890948]\n",
      " [0.96026908 0.95962087 0.96171423 0.9614902  0.96859537 0.9685917\n",
      "  0.96825505]\n",
      " [0.96609329 0.96632199 0.96545009 0.96541085 0.97395574 0.97397781\n",
      "  0.97492329]\n",
      " [0.96637022 0.96561316 0.96264952 0.96267436 0.97367561 0.97362966\n",
      "  0.97002509]\n",
      " [0.96274154 0.96251764 0.96542457 0.96535919 0.9733565  0.97330962\n",
      "  0.9708548 ]\n",
      " [0.9554429  0.95557849 0.94828547 0.94845894 0.96811224 0.96809636\n",
      "  0.97169279]\n",
      " [0.96536904 0.96511897 0.96846714 0.96839076 0.97464486 0.97467812\n",
      "  0.97708269]\n",
      " [0.96312632 0.96311347 0.96175034 0.96181277 0.97184497 0.97183273\n",
      "  0.97275209]\n",
      " [0.95499239 0.9552249  0.9550384  0.95519257 0.96787125 0.96792844\n",
      "  0.96984324]\n",
      " [0.96398634 0.96389522 0.96412547 0.96405775 0.97561576 0.97559976\n",
      "  0.97448914]\n",
      " [0.96373703 0.96360088 0.96107062 0.96109176 0.97158334 0.97160572\n",
      "  0.9711954 ]\n",
      " [0.96682125 0.96703761 0.96420112 0.96425114 0.97268184 0.97272936\n",
      "  0.97169507]\n",
      " [0.96059181 0.96052667 0.96786072 0.96785219 0.9730541  0.972992\n",
      "  0.97167325]\n",
      " [0.95140429 0.95124465 0.9577193  0.95761246 0.96651558 0.9665005\n",
      "  0.96823763]\n",
      " [0.95383842 0.9539268  0.96442835 0.964393   0.96769561 0.96769805\n",
      "  0.96895984]\n",
      " [0.95391339 0.95373967 0.96244483 0.96246678 0.96682637 0.96686417\n",
      "  0.9674018 ]\n",
      " [0.96809301 0.96832093 0.96546225 0.96541132 0.97497298 0.97493945\n",
      "  0.97166652]\n",
      " [0.95667212 0.95626488 0.9417467  0.94145187 0.9667647  0.96666151\n",
      "  0.96569101]\n",
      " [0.96481206 0.96481267 0.95807597 0.95803807 0.97176944 0.97168141\n",
      "  0.9665647 ]\n",
      " [0.96143992 0.96198244 0.96093734 0.96097355 0.97178291 0.97182786\n",
      "  0.96917703]\n",
      " [0.96060004 0.96019049 0.95942629 0.95920918 0.97001165 0.96998327\n",
      "  0.97036692]\n",
      " [0.95519563 0.95471117 0.94991885 0.94977621 0.96423249 0.96418085\n",
      "  0.96437143]\n",
      " [0.95721819 0.95717529 0.95467594 0.95466859 0.96851611 0.96852469\n",
      "  0.96740801]\n",
      " [0.95509624 0.95472899 0.96174997 0.96159177 0.96874207 0.96871194\n",
      "  0.96885884]\n",
      " [0.95257178 0.95310224 0.95819816 0.95822517 0.96502791 0.96502546\n",
      "  0.96675929]\n",
      " [0.96062969 0.96053968 0.96591865 0.96589152 0.97026374 0.97023661\n",
      "  0.96982726]\n",
      " [0.96797682 0.96775115 0.96912291 0.96898958 0.97690456 0.97687643\n",
      "  0.97610829]\n",
      " [0.96176152 0.96197699 0.96392543 0.9638427  0.97145373 0.97139446\n",
      "  0.97216865]\n",
      " [0.95455477 0.9545939  0.96268554 0.96211542 0.96911712 0.96906868\n",
      "  0.97249811]\n",
      " [0.96461941 0.96438675 0.96370678 0.96363605 0.97310735 0.97309991\n",
      "  0.97102027]\n",
      " [0.9608707  0.96078141 0.96082668 0.96092837 0.96958133 0.96958753\n",
      "  0.96828407]\n",
      " [0.96899477 0.96861655 0.96790545 0.96795762 0.97704604 0.9769976\n",
      "  0.97646102]\n",
      " [0.95494574 0.95460239 0.96194731 0.96192593 0.96818797 0.96812131\n",
      "  0.97002168]]\n"
     ]
    }
   ],
   "source": [
    "# Number of times to repeat the process\n",
    "num_repeats = 50\n",
    "\n",
    "# Initialize an empty matrix (10-by-4) to store accuracies\n",
    "accuracies = np.zeros((num_repeats, 7))\n",
    "auc_accuracies = np.zeros((num_repeats, 7))\n",
    "\n",
    "seed = 42\n",
    "# Repeat the process and store accuracies\n",
    "for i in range(num_repeats):\n",
    "    np.random.seed(seed)\n",
    "    accuracies[i], auc_accuracies[i] = iterate_process(X, y)\n",
    "    seed += 2\n",
    "    \n",
    "\n",
    "# Print the accuracies\n",
    "print(\"Accuracies:\", accuracies)\n",
    "print(\"AUC:\", auc_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dba311e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92316133, 0.92316133, 0.91695817, 0.91688213, 0.92422596,\n",
       "       0.92410646, 0.92355242])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a1141e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00547857, 0.00547857, 0.00649792, 0.00635918, 0.0051671 ,\n",
       "       0.00526079, 0.00531966])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8dbec013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96083184, 0.96076151, 0.96162423, 0.9615794 , 0.9709994 ,\n",
       "       0.97098266, 0.97049631])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(auc_accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0f031bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00508772, 0.00510967, 0.00574344, 0.00576238, 0.00334659,\n",
       "       0.00334865, 0.0028022 ])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(auc_accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e709c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
