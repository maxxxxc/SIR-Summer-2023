{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the MNIST DATA (5 vs 8)\n",
    "\n",
    "## Load necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "### Load data \n",
    "- 70K images from 10 classes\n",
    "- Each image is 28-by-28, stored in a 784 vector\n",
    "- lable (y) takes 10 different values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\panda\\anaconda3\\lib\\site-packages\\sklearn\\datasets\\_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_openml('mnist_784')\n",
    "\n",
    "# Extract features (pixel values) and target labels\n",
    "X = mnist.data.astype('float32')\n",
    "y = mnist.target.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
      "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "\n",
      "   pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
      "0      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
      "1      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
      "2      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
      "3      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
      "4      0.0  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
      "\n",
      "   pixel781  pixel782  pixel783  pixel784  \n",
      "0       0.0       0.0       0.0       0.0  \n",
      "1       0.0       0.0       0.0       0.0  \n",
      "2       0.0       0.0       0.0       0.0  \n",
      "3       0.0       0.0       0.0       0.0  \n",
      "4       0.0       0.0       0.0       0.0  \n",
      "\n",
      "[5 rows x 784 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5\n",
      "1    0\n",
      "2    4\n",
      "3    1\n",
      "4    9\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6903, 7877, 6990, 7141, 6824, 6313, 6876, 7293, 6825, 6958],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = np.bincount(y)\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display 12 randomly selected images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter data \n",
    "Filter the dataset to include only the digits 5 and 8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_filter = (y == 5) | (y == 8)\n",
    "X = X[digit_filter]\n",
    "y = y[digit_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13138, 784)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 8], dtype=int64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "n5 = np.count_nonzero(y == 5)\n",
    "n8 = np.count_nonzero(y == 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6313, 6825)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n5, n8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6825\n",
       "1    6313\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.replace({5 : 1, 8 : 0})\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "60\\% training and 40\\% test\n",
    "\n",
    "Try two different Logistic regression models:\n",
    "- Logistic regression with all 784 features;\n",
    "- Losgitic regression with 100 top PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9747526008627252\n",
      "Test Accuracy: 0.9490106544901066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\panda\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "#model = LogisticRegression()\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4.62493822  -4.52236797   4.89404422   5.94546226  12.81172949\n",
      "   0.48844461 -12.62286427   4.16760299  -1.66570542   7.58588442]\n",
      "[[9.70907044e-03 9.90290930e-01]\n",
      " [9.89253473e-01 1.07465269e-02]\n",
      " [7.43536680e-03 9.92564633e-01]\n",
      " [2.61085759e-03 9.97389142e-01]\n",
      " [2.72857185e-06 9.99997271e-01]\n",
      " [3.80260047e-01 6.19739953e-01]\n",
      " [9.99996704e-01 3.29578043e-06]\n",
      " [1.52530835e-02 9.84746916e-01]\n",
      " [8.41002401e-01 1.58997599e-01]\n",
      " [5.07308197e-04 9.99492692e-01]]\n",
      "[1 0 1 1 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "y_test_logits = model.decision_function(X_test)\n",
    "y_test_prob = model.predict_proba(X_test)\n",
    "print(y_test_logits[:10])\n",
    "print(y_test_prob[:10])\n",
    "print(y_test_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9902909295571708, 0.009709070442829145)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/(1+ np.exp(-y_test_logits[0])), 1/(1+ np.exp(y_test_logits[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA + Logistic Regression: use top 100 PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9640954072570413\n",
      "Test Accuracy: 0.9623287671232876\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=100)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Create and train the logistic regression model on the PCA-transformed data\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train_pca, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test_pca)\n",
    "y_train_pred = model.predict(X_train_pca)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide and Conquer\n",
    "\n",
    "Divide the training data into 11 batches, train a logistic model on each of the batch, and then combine the 11 prediction results. Consider the following two ensemble methods:\n",
    "- majority voting\n",
    "- average (or sum) of the logit output and then make decision based on its sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def iterate_process(X, y):\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "    \n",
    "    # Define the number of batches\n",
    "    num_batches = 11\n",
    "    \n",
    "    # Randomly shuffle the data indices\n",
    "    #indices = np.random.permutation(len(X))\n",
    "    \n",
    "    #change 7/12\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    \n",
    "    # Calculate the batch size\n",
    "    batch_size = len(X_train) // num_batches\n",
    "    \n",
    "    # Make predictions on the test set using majority voting\n",
    "    preds_voting = np.zeros(len(y_test))\n",
    "    # Make predictions on the test set using average of logit\n",
    "    preds_logit = np.zeros(len(y_test))\n",
    "    #Make predictions on the test set using average of probs\n",
    "    preds_prob = np.zeros(len(y_test))\n",
    "    \n",
    "    preds_voting_weighted = np.zeros(len(y_test))\n",
    "    preds_logit_weighted = np.zeros(len(y_test))\n",
    "    preds_prob_weighted = np.zeros(len(y_test))\n",
    "    \n",
    "    total_cverr = 0\n",
    "    \n",
    "    # Split the training data into batches, fit a logistic regression model on each batch\n",
    "    for i in range(num_batches):\n",
    "        # Calculate the starting and ending indices for the current batch\n",
    "        start_index = i * batch_size\n",
    "        end_index = (i + 1) * batch_size\n",
    "        \n",
    "        # Create a logistic regression model\n",
    "        model = LogisticRegression(max_iter=500)\n",
    "        \n",
    "        # Select the current batch for training\n",
    "        #X_batch = X_train[start_index:end_index]\n",
    "        #y_batch = y_train[start_index:end_index]\n",
    "        \n",
    "        #change 7/12\n",
    "        X_batch = X_train.iloc[indices[start_index:end_index]]\n",
    "        y_batch = y_train.iloc[indices[start_index:end_index]]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_batch_scaled = scaler.fit_transform(X_batch)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Fit the model on the current batch\n",
    "        model.fit(X_batch_scaled, y_batch)\n",
    "        current_cverr = cross_val_score(model, X_batch_scaled, y_batch, cv = 5, scoring = 'accuracy').mean()\n",
    "        total_cverr += current_cverr\n",
    "               \n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        # Accumulate the predictions using majority voting\n",
    "        preds_voting += (y_pred == 1)\n",
    "        preds_voting_weighted += (y_pred == 1) * current_cverr\n",
    "    \n",
    "        # Accumulate the predictions using majority voting\n",
    "        y_pred = model.decision_function(X_test_scaled)\n",
    "        preds_logit += y_pred\n",
    "        preds_logit_weighted += y_pred * current_cverr\n",
    "        \n",
    "        #Accumulate the probs\n",
    "        y_pred = model.predict_proba(X_test_scaled)\n",
    "        preds_prob += y_pred[:,1]\n",
    "        preds_prob_weighted += y_pred[:,1] * current_cverr\n",
    "    \n",
    "    accuracy = np.zeros(7)\n",
    "    acu_accuracy = np.zeros(7)\n",
    "    \n",
    "    preds_voting_weighted = preds_voting_weighted / total_cverr * num_batches\n",
    "    preds_logit_weighted = preds_logit_weighted / total_cverr * num_batches\n",
    "    preds_prob_weighted = preds_prob_weighted / total_cverr * num_batches\n",
    "    \n",
    "    # Majority voting (selecting the most frequent prediction for each sample)\n",
    "    final_predictions = np.where(preds_voting > num_batches / 2, 1, 0)\n",
    "    accuracy[0] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[0] = roc_auc_score(y_test, preds_voting)\n",
    "    \n",
    "    final_predictions = np.where(preds_voting_weighted > num_batches / 2, 1, 0)\n",
    "    accuracy[1] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[1] = roc_auc_score(y_test, preds_voting_weighted)\n",
    "    \n",
    "    # Average of logit\n",
    "    final_predictions = np.where(preds_logit > 0, 1, 0)\n",
    "    accuracy[2] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[2] = roc_auc_score(y_test, preds_logit)\n",
    "    \n",
    "    final_predictions = np.where(preds_logit_weighted > 0, 1, 0)\n",
    "    accuracy[3] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[3] = roc_auc_score(y_test, preds_logit_weighted)\n",
    "    \n",
    "    #Average of probs\n",
    "    final_predictions = np.where(preds_prob / num_batches > 0.5, 1, 0)\n",
    "    accuracy[4] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[4] = roc_auc_score(y_test, preds_prob)\n",
    "    \n",
    "    final_predictions = np.where(preds_prob_weighted / num_batches > 0.5, 1, 0)\n",
    "    accuracy[5] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[5] = roc_auc_score(y_test, preds_prob_weighted)\n",
    "    \n",
    "    # Train a model on all 11 batches of training data\n",
    "    model = LogisticRegression(max_iter=500)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy[6] = accuracy_score(y_test, y_pred)\n",
    "    y_pred = model.decision_function(X_test_scaled)\n",
    "    auc_accuray[6] = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration Code\n",
    "\n",
    "We need to repeat the process above 100 times and record the corresponding accuracies. Let's write a function of the process above and then call this function 100 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_process(X, y):\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "    \n",
    "    # Define the number of batches\n",
    "    num_batches = 11\n",
    "    \n",
    "    # Randomly shuffle the data indices\n",
    "    #indices = np.random.permutation(len(X))\n",
    "    \n",
    "    #change 7/12\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    \n",
    "    # Calculate the batch size\n",
    "    batch_size = len(X_train) // num_batches\n",
    "    \n",
    "    # Make predictions on the test set using majority voting\n",
    "    preds_voting = np.zeros(len(y_test))\n",
    "    # Make predictions on the test set using average of logit\n",
    "    preds_logit = np.zeros(len(y_test))\n",
    "    #Make predictions on the test set using average of probs\n",
    "    preds_prob = np.zeros(len(y_test))\n",
    "    \n",
    "    preds_voting_weighted = np.zeros(len(y_test))\n",
    "    preds_logit_weighted = np.zeros(len(y_test))\n",
    "    preds_prob_weighted = np.zeros(len(y_test))\n",
    "    \n",
    "    total_cverr = 0\n",
    "    \n",
    "    # Split the training data into batches, fit a logistic regression model on each batch\n",
    "    for i in range(num_batches):\n",
    "        # Calculate the starting and ending indices for the current batch\n",
    "        start_index = i * batch_size\n",
    "        end_index = (i + 1) * batch_size\n",
    "        \n",
    "        # Create a logistic regression model\n",
    "        model = LogisticRegression(max_iter=500)\n",
    "        \n",
    "        # Select the current batch for training\n",
    "        #X_batch = X_train[start_index:end_index]\n",
    "        #y_batch = y_train[start_index:end_index]\n",
    "        \n",
    "        #change 7/12\n",
    "        X_batch = X_train.iloc[indices[start_index:end_index]]\n",
    "        y_batch = y_train.iloc[indices[start_index:end_index]]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_batch_scaled = scaler.fit_transform(X_batch)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Fit the model on the current batch\n",
    "        model.fit(X_batch_scaled, y_batch)\n",
    "        current_cverr = cross_val_score(model, X_batch_scaled, y_batch, cv = 5, scoring = 'accuracy').mean()\n",
    "        total_cverr += current_cverr\n",
    "               \n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        # Accumulate the predictions using majority voting\n",
    "        preds_voting += (y_pred == 1)\n",
    "        preds_voting_weighted += (y_pred == 1) * current_cverr\n",
    "    \n",
    "        # Accumulate the predictions using majority voting\n",
    "        y_pred = model.decision_function(X_test_scaled)\n",
    "        preds_logit += y_pred\n",
    "        preds_logit_weighted += y_pred * current_cverr\n",
    "        \n",
    "        #Accumulate the probs\n",
    "        y_pred = model.predict_proba(X_test_scaled)\n",
    "        preds_prob += y_pred[:,1]\n",
    "        preds_prob_weighted += y_pred[:,1] * current_cverr\n",
    "    \n",
    "    accuracy = np.zeros(7)\n",
    "    auc_accuracy = np.zeros(7)\n",
    "    \n",
    "    preds_voting_weighted = preds_voting_weighted / total_cverr * num_batches\n",
    "    preds_logit_weighted = preds_logit_weighted / total_cverr * num_batches\n",
    "    preds_prob_weighted = preds_prob_weighted / total_cverr * num_batches\n",
    "    \n",
    "    # Majority voting (selecting the most frequent prediction for each sample)\n",
    "    final_predictions = np.where(preds_voting > num_batches / 2, 1, 0)\n",
    "    accuracy[0] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[0] = roc_auc_score(y_test, preds_voting)\n",
    "    \n",
    "    final_predictions = np.where(preds_voting_weighted > num_batches / 2, 1, 0)\n",
    "    accuracy[1] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[1] = roc_auc_score(y_test, preds_voting_weighted)\n",
    "    \n",
    "    # Average of logit\n",
    "    final_predictions = np.where(preds_logit > 0, 1, 0)\n",
    "    accuracy[2] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[2] = roc_auc_score(y_test, preds_logit)\n",
    "    \n",
    "    final_predictions = np.where(preds_logit_weighted > 0, 1, 0)\n",
    "    accuracy[3] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[3] = roc_auc_score(y_test, preds_logit_weighted)\n",
    "    \n",
    "    #Average of probs\n",
    "    final_predictions = np.where(preds_prob / num_batches > 0.5, 1, 0)\n",
    "    accuracy[4] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[4] = roc_auc_score(y_test, preds_prob)\n",
    "    \n",
    "    final_predictions = np.where(preds_prob_weighted / num_batches > 0.5, 1, 0)\n",
    "    accuracy[5] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[5] = roc_auc_score(y_test, preds_prob_weighted)\n",
    "    \n",
    "    # Train a model on all 11 batches of training data\n",
    "    model = LogisticRegression(max_iter=500)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy[6] = accuracy_score(y_test, y_pred)\n",
    "    y_pred = model.decision_function(X_test_scaled)\n",
    "    auc_accuracy[6] = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, auc_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies: [[0.96860731 0.96860731 0.96194825 0.96194825 0.96860731 0.96879756\n",
      "  0.95547945]\n",
      " [0.96575342 0.96575342 0.9598554  0.95966514 0.96594368 0.96575342\n",
      "  0.95509893]\n",
      " [0.96099696 0.96099696 0.96023592 0.96004566 0.96270928 0.96251903\n",
      "  0.95091324]\n",
      " [0.96213851 0.96213851 0.9608067  0.9608067  0.96118721 0.96137747\n",
      "  0.94805936]\n",
      " [0.9608067  0.9608067  0.96061644 0.96023592 0.96004566 0.96004566\n",
      "  0.95262557]\n",
      " [0.96213851 0.96213851 0.96232877 0.96232877 0.96366058 0.96366058\n",
      "  0.95300609]\n",
      " [0.96194825 0.96194825 0.96061644 0.96061644 0.9630898  0.9630898\n",
      "  0.95376712]\n",
      " [0.96118721 0.96118721 0.9608067  0.96061644 0.96232877 0.96232877\n",
      "  0.95281583]\n",
      " [0.96594368 0.96594368 0.96575342 0.96575342 0.96765601 0.96746575\n",
      "  0.95471842]\n",
      " [0.96251903 0.96251903 0.96537291 0.96537291 0.96518265 0.96499239\n",
      "  0.94920091]\n",
      " [0.96689498 0.96689498 0.96461187 0.96461187 0.96708524 0.96708524\n",
      "  0.95566971]\n",
      " [0.96289954 0.96289954 0.96156773 0.96175799 0.96442161 0.96442161\n",
      "  0.95566971]\n",
      " [0.96232877 0.96232877 0.96004566 0.96004566 0.96156773 0.96156773\n",
      "  0.95072298]\n",
      " [0.9608067  0.9608067  0.95681126 0.95719178 0.95928463 0.95966514\n",
      "  0.94901065]\n",
      " [0.96232877 0.96232877 0.9630898  0.9630898  0.96328006 0.9630898\n",
      "  0.95509893]\n",
      " [0.96023592 0.96023592 0.95890411 0.95890411 0.96023592 0.96023592\n",
      "  0.95547945]\n",
      " [0.96270928 0.96270928 0.96251903 0.96251903 0.96328006 0.96347032\n",
      "  0.95490868]\n",
      " [0.9608067  0.9608067  0.96004566 0.96004566 0.96137747 0.96137747\n",
      "  0.9543379 ]\n",
      " [0.96423135 0.96423135 0.96366058 0.96366058 0.96442161 0.96442161\n",
      "  0.95300609]\n",
      " [0.96289954 0.96289954 0.96194825 0.96213851 0.96270928 0.96270928\n",
      "  0.95148402]\n",
      " [0.96251903 0.96251903 0.96366058 0.96328006 0.9640411  0.96385084\n",
      "  0.94843988]\n",
      " [0.96289954 0.96289954 0.9608067  0.9608067  0.96347032 0.96347032\n",
      "  0.95585997]\n",
      " [0.95490868 0.95490868 0.95605023 0.95624049 0.956621   0.956621\n",
      "  0.94748858]\n",
      " [0.96347032 0.96347032 0.9640411  0.9640411  0.96461187 0.96461187\n",
      "  0.95509893]\n",
      " [0.96042618 0.96042618 0.95814307 0.95814307 0.96023592 0.96023592\n",
      "  0.9478691 ]\n",
      " [0.96270928 0.96270928 0.96175799 0.96137747 0.9630898  0.9630898\n",
      "  0.95319635]\n",
      " [0.9608067  0.9608067  0.96004566 0.96004566 0.96004566 0.96004566\n",
      "  0.94767884]\n",
      " [0.96137747 0.96137747 0.95947489 0.95947489 0.96175799 0.96175799\n",
      "  0.95376712]\n",
      " [0.95947489 0.95947489 0.9575723  0.9575723  0.95871385 0.95871385\n",
      "  0.94977169]\n",
      " [0.96232877 0.96232877 0.96213851 0.96194825 0.9640411  0.96385084\n",
      "  0.95643075]\n",
      " [0.96328006 0.96328006 0.96118721 0.96156773 0.96328006 0.96328006\n",
      "  0.9543379 ]\n",
      " [0.96461187 0.96461187 0.96385084 0.96366058 0.96499239 0.96499239\n",
      "  0.9543379 ]\n",
      " [0.96156773 0.96156773 0.9608067  0.9608067  0.96156773 0.96137747\n",
      "  0.95414764]\n",
      " [0.96765601 0.96765601 0.9663242  0.9663242  0.96765601 0.96784627\n",
      "  0.95833333]\n",
      " [0.96499239 0.96499239 0.96423135 0.96423135 0.96613394 0.9663242\n",
      "  0.95262557]\n",
      " [0.96385084 0.96385084 0.96137747 0.96156773 0.96537291 0.96537291\n",
      "  0.95338661]\n",
      " [0.95833333 0.95833333 0.9575723  0.95776256 0.95909437 0.95947489\n",
      "  0.94996195]\n",
      " [0.96423135 0.96423135 0.96099696 0.96099696 0.96385084 0.96385084\n",
      "  0.95414764]\n",
      " [0.96289954 0.96289954 0.96289954 0.96289954 0.96270928 0.96289954\n",
      "  0.956621  ]\n",
      " [0.96442161 0.96442161 0.96156773 0.96156773 0.96442161 0.96442161\n",
      "  0.95015221]\n",
      " [0.95814307 0.95814307 0.95776256 0.95776256 0.95852359 0.95833333\n",
      "  0.94691781]\n",
      " [0.96175799 0.96175799 0.96213851 0.96232877 0.96156773 0.96156773\n",
      "  0.94996195]\n",
      " [0.9598554  0.9598554  0.95947489 0.95947489 0.96042618 0.96042618\n",
      "  0.95072298]\n",
      " [0.96213851 0.96213851 0.96156773 0.96175799 0.96270928 0.96251903\n",
      "  0.94863014]\n",
      " [0.96537291 0.96537291 0.96480213 0.96480213 0.96689498 0.96708524\n",
      "  0.95966514]\n",
      " [0.96194825 0.96194825 0.96194825 0.96175799 0.96213851 0.96194825\n",
      "  0.95186454]\n",
      " [0.96328006 0.96328006 0.96366058 0.96347032 0.9640411  0.9640411\n",
      "  0.95034247]\n",
      " [0.96537291 0.96537291 0.96251903 0.96251903 0.96347032 0.96347032\n",
      "  0.95243531]\n",
      " [0.96594368 0.96594368 0.96803653 0.96803653 0.96765601 0.96765601\n",
      "  0.95681126]\n",
      " [0.96137747 0.96137747 0.95490868 0.95490868 0.96023592 0.96023592\n",
      "  0.95091324]]\n",
      "AUC: [[0.99144753 0.99142681 0.9822294  0.98219549 0.99422579 0.9942255\n",
      "  0.98573036]\n",
      " [0.99154241 0.99147118 0.98415922 0.98421174 0.99431842 0.99431741\n",
      "  0.98525393]\n",
      " [0.98975716 0.98978173 0.98848533 0.98848897 0.99307681 0.9930771\n",
      "  0.98197424]\n",
      " [0.98944973 0.98939526 0.98986568 0.98985873 0.99355257 0.99355214\n",
      "  0.98263981]\n",
      " [0.99015102 0.99017656 0.99080932 0.99080207 0.99350927 0.99351\n",
      "  0.98598314]\n",
      " [0.9910734  0.99102129 0.99068514 0.9906998  0.99411981 0.99411923\n",
      "  0.9847122 ]\n",
      " [0.9898206  0.98977955 0.99132999 0.99131592 0.99399447 0.9939907\n",
      "  0.9860821 ]\n",
      " [0.98867379 0.98871062 0.98985468 0.98987342 0.99354218 0.99354363\n",
      "  0.98442912]\n",
      " [0.9916523  0.99168532 0.99233498 0.99233745 0.99383634 0.99383083\n",
      "  0.98709734]\n",
      " [0.99189822 0.99194167 0.99169018 0.99170077 0.99450466 0.99450712\n",
      "  0.98579457]\n",
      " [0.99296268 0.9929801  0.9909036  0.99087687 0.99494057 0.99494304\n",
      "  0.9876827 ]\n",
      " [0.9904427  0.99029159 0.99047591 0.99043182 0.99390438 0.99389583\n",
      "  0.98567587]\n",
      " [0.99025273 0.99015803 0.9915003  0.99149741 0.9936896  0.99368511\n",
      "  0.98443745]\n",
      " [0.98699442 0.98694083 0.9893901  0.9893943  0.99216608 0.99216985\n",
      "  0.98407659]\n",
      " [0.98824875 0.98832841 0.98885681 0.98885492 0.99196934 0.99197007\n",
      "  0.98361267]\n",
      " [0.98786766 0.98790884 0.98833166 0.98832962 0.99282263 0.99282786\n",
      "  0.98361217]\n",
      " [0.99075043 0.99080066 0.99176043 0.99175607 0.99438165 0.99438093\n",
      "  0.98581788]\n",
      " [0.98917524 0.9892318  0.99068731 0.99068079 0.99291028 0.99291188\n",
      "  0.98532014]\n",
      " [0.99112588 0.99109878 0.99092291 0.99092696 0.99398366 0.99398395\n",
      "  0.98505869]\n",
      " [0.99070264 0.99079949 0.99164985 0.991659   0.9940182  0.99401966\n",
      "  0.98448221]\n",
      " [0.9915374  0.99161578 0.99210305 0.99210131 0.99422598 0.99422816\n",
      "  0.98548035]\n",
      " [0.99015048 0.99016505 0.98771072 0.98771057 0.99379061 0.9937893\n",
      "  0.98637504]\n",
      " [0.98896296 0.9890501  0.99048005 0.99049164 0.99269311 0.99269659\n",
      "  0.98433445]\n",
      " [0.9916434  0.99167558 0.99155578 0.99154419 0.99486387 0.99486879\n",
      "  0.98475003]\n",
      " [0.99021664 0.99032349 0.98855697 0.98858323 0.99325396 0.99325715\n",
      "  0.98619048]\n",
      " [0.99114289 0.99124355 0.99227132 0.99228118 0.99466526 0.99466715\n",
      "  0.98623932]\n",
      " [0.9895691  0.98959328 0.99118027 0.9911749  0.99372146 0.99371943\n",
      "  0.98455026]\n",
      " [0.99046574 0.99051884 0.99078979 0.99078152 0.99384081 0.99384197\n",
      "  0.98564562]\n",
      " [0.98907505 0.9890111  0.99009639 0.99008537 0.99281984 0.99281752\n",
      "  0.9850871 ]\n",
      " [0.98895399 0.9889595  0.98885316 0.98885664 0.99310502 0.99310604\n",
      "  0.98515857]\n",
      " [0.99159829 0.99167619 0.99035063 0.99036005 0.99464765 0.9946504\n",
      "  0.98820082]\n",
      " [0.99194764 0.99192516 0.99075046 0.99077047 0.99441483 0.99441338\n",
      "  0.98451665]\n",
      " [0.99139464 0.99143522 0.99047887 0.99048105 0.99438641 0.99438757\n",
      "  0.98675943]\n",
      " [0.9912568  0.99120805 0.98946215 0.98949402 0.99396699 0.99396337\n",
      "  0.98718706]\n",
      " [0.99276275 0.99278582 0.99268716 0.99267221 0.99503525 0.99503481\n",
      "  0.98580873]\n",
      " [0.99076451 0.99084289 0.98803396 0.98807358 0.99310929 0.99310856\n",
      "  0.98442554]\n",
      " [0.98914315 0.98913395 0.99001387 0.99001025 0.99351676 0.99351574\n",
      "  0.98354571]\n",
      " [0.98983379 0.98990602 0.98554723 0.98553069 0.99350645 0.99350732\n",
      "  0.98523454]\n",
      " [0.99018911 0.99021292 0.99008397 0.9900709  0.99390777 0.99390719\n",
      "  0.98288757]\n",
      " [0.9913406  0.9912815  0.99072408 0.99074827 0.99425155 0.99425386\n",
      "  0.98311824]\n",
      " [0.98944362 0.98941468 0.99085808 0.99085532 0.99357764 0.99357401\n",
      "  0.98487491]\n",
      " [0.99031776 0.99032066 0.990888   0.99089496 0.99392287 0.993922\n",
      "  0.987372  ]\n",
      " [0.98947395 0.98953694 0.99014528 0.99013222 0.99283324 0.99283643\n",
      "  0.98192163]\n",
      " [0.99095492 0.99095245 0.99092143 0.99094434 0.99414175 0.99414378\n",
      "  0.98314894]\n",
      " [0.99219068 0.99222641 0.99111002 0.99112147 0.99487772 0.99487903\n",
      "  0.98590134]\n",
      " [0.99134091 0.99135563 0.99171917 0.99171888 0.99397858 0.99397582\n",
      "  0.9859709 ]\n",
      " [0.98948743 0.9895135  0.98982097 0.98981967 0.99332361 0.9933268\n",
      "  0.98294719]\n",
      " [0.98846318 0.98836538 0.99131437 0.99131336 0.99321164 0.99321149\n",
      "  0.98580396]\n",
      " [0.99220062 0.99227004 0.99211668 0.99211872 0.99480956 0.99481073\n",
      "  0.98764569]\n",
      " [0.98906312 0.98913821 0.98202928 0.98201031 0.99320672 0.99320629\n",
      "  0.98480872]]\n"
     ]
    }
   ],
   "source": [
    "# Number of times to repeat the process\n",
    "num_repeats = 50\n",
    "\n",
    "# Initialize an empty matrix (10-by-4) to store accuracies\n",
    "accuracies = np.zeros((num_repeats, 7))\n",
    "auc_accuracies = np.zeros((num_repeats, 7))\n",
    "\n",
    "seed = 42\n",
    "# Repeat the process and store accuracies\n",
    "for i in range(num_repeats):\n",
    "    np.random.seed(seed)\n",
    "    accuracies[i], auc_accuracies[i] = iterate_process(X, y)\n",
    "    seed += 2\n",
    "    \n",
    "\n",
    "# Print the accuracies\n",
    "print(\"Accuracies:\", accuracies)\n",
    "print(\"AUC:\", auc_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96260274, 0.96260274, 0.96145738, 0.96144977, 0.96302892,\n",
       "       0.96302892, 0.95265982])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00244367, 0.00244367, 0.00261703, 0.00258749, 0.00262762,\n",
       "       0.00262459, 0.00301467])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99037749, 0.99039173, 0.98997152, 0.98997287, 0.99378146,\n",
       "       0.99378165, 0.98510728])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(auc_accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00128681, 0.00129263, 0.00225741, 0.00225779, 0.00069945,\n",
       "       0.00069927, 0.00144211])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(auc_accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13138"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13138"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13138, 784)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
