{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b0ddd4b",
   "metadata": {},
   "source": [
    "# Wireless Indoor Localization\n",
    "\n",
    "Dataset is from https://archive.ics.uci.edu/dataset/422/wireless+indoor+localization\n",
    "\n",
    "n = 2000\n",
    "\n",
    "7 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "961a7d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b861a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/maxxxxc/SIR-Summer-2023/main/dataset/wifi_localization.txt\"\n",
    "df = pd.read_csv(url, sep = '\\t', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9658e176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 8)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c99a4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0   1   2   3   4   5   6  7\n",
      "0 -64 -56 -61 -66 -71 -82 -81  1\n",
      "1 -68 -57 -61 -65 -71 -85 -85  1\n",
      "2 -63 -60 -60 -67 -76 -85 -84  1\n",
      "3 -61 -60 -68 -62 -77 -90 -80  1\n",
      "4 -63 -65 -60 -63 -77 -81 -87  1\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4330eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c46f419",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(7, axis=1)\n",
    "y = df[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b12ec0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-64</td>\n",
       "      <td>-56</td>\n",
       "      <td>-61</td>\n",
       "      <td>-66</td>\n",
       "      <td>-71</td>\n",
       "      <td>-82</td>\n",
       "      <td>-81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-68</td>\n",
       "      <td>-57</td>\n",
       "      <td>-61</td>\n",
       "      <td>-65</td>\n",
       "      <td>-71</td>\n",
       "      <td>-85</td>\n",
       "      <td>-85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-63</td>\n",
       "      <td>-60</td>\n",
       "      <td>-60</td>\n",
       "      <td>-67</td>\n",
       "      <td>-76</td>\n",
       "      <td>-85</td>\n",
       "      <td>-84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-61</td>\n",
       "      <td>-60</td>\n",
       "      <td>-68</td>\n",
       "      <td>-62</td>\n",
       "      <td>-77</td>\n",
       "      <td>-90</td>\n",
       "      <td>-80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-63</td>\n",
       "      <td>-65</td>\n",
       "      <td>-60</td>\n",
       "      <td>-63</td>\n",
       "      <td>-77</td>\n",
       "      <td>-81</td>\n",
       "      <td>-87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3   4   5   6\n",
       "0 -64 -56 -61 -66 -71 -82 -81\n",
       "1 -68 -57 -61 -65 -71 -85 -85\n",
       "2 -63 -60 -60 -67 -76 -85 -84\n",
       "3 -61 -60 -68 -62 -77 -90 -80\n",
       "4 -63 -65 -60 -63 -77 -81 -87"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "848faca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1000\n",
       "1    1000\n",
       "Name: 7, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.replace({2 : 0, 1 : 0, 3: 1, 4 : 1})\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201d8a20",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b2ba396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9216666666666666\n",
      "Test Accuracy: 0.93125\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "#model = LogisticRegression()\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a20454",
   "metadata": {},
   "source": [
    "# Divide and Conquer\n",
    "\n",
    "Divide the training data into 11 batches, train a logistic model on each of the batch, and then combine the 11 prediction results. Consider the following two ensemble methods:\n",
    "- majority voting\n",
    "- average (or sum) of the logit output and then make decision based on its sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "806458ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_process(X, y):\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "    \n",
    "    # Define the number of batches\n",
    "    num_batches = 11\n",
    "    \n",
    "    # Randomly shuffle the data indices\n",
    "    #indices = np.random.permutation(len(X))\n",
    "    \n",
    "    #change 7/12\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    \n",
    "    # Calculate the batch size\n",
    "    batch_size = len(X_train) // num_batches\n",
    "    \n",
    "    # Make predictions on the test set using majority voting\n",
    "    preds_voting = np.zeros(len(y_test))\n",
    "    # Make predictions on the test set using average of logit\n",
    "    preds_logit = np.zeros(len(y_test))\n",
    "    #Make predictions on the test set using average of probs\n",
    "    preds_prob = np.zeros(len(y_test))\n",
    "    \n",
    "    preds_voting_weighted = np.zeros(len(y_test))\n",
    "    preds_logit_weighted = np.zeros(len(y_test))\n",
    "    preds_prob_weighted = np.zeros(len(y_test))\n",
    "    \n",
    "    total_cverr = 0\n",
    "    \n",
    "    # Split the training data into batches, fit a logistic regression model on each batch\n",
    "    for i in range(num_batches):\n",
    "        # Calculate the starting and ending indices for the current batch\n",
    "        start_index = i * batch_size\n",
    "        end_index = (i + 1) * batch_size\n",
    "        \n",
    "        # Create a logistic regression model\n",
    "        model = LogisticRegression(max_iter=500)\n",
    "        \n",
    "        # Select the current batch for training\n",
    "        #X_batch = X_train[start_index:end_index]\n",
    "        #y_batch = y_train[start_index:end_index]\n",
    "        \n",
    "        #change 7/12\n",
    "        X_batch = X_train.iloc[indices[start_index:end_index]]\n",
    "        y_batch = y_train.iloc[indices[start_index:end_index]]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_batch_scaled = scaler.fit_transform(X_batch)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Fit the model on the current batch\n",
    "        model.fit(X_batch_scaled, y_batch)\n",
    "        current_cverr = cross_val_score(model, X_batch_scaled, y_batch, cv = 5, scoring = 'accuracy').mean()\n",
    "        total_cverr += current_cverr\n",
    "               \n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        # Accumulate the predictions using majority voting\n",
    "        preds_voting += (y_pred == 1)\n",
    "        preds_voting_weighted += (y_pred == 1) * current_cverr\n",
    "    \n",
    "        # Accumulate the predictions using majority voting\n",
    "        y_pred = model.decision_function(X_test_scaled)\n",
    "        preds_logit += y_pred\n",
    "        preds_logit_weighted += y_pred * current_cverr\n",
    "        \n",
    "        #Accumulate the probs\n",
    "        y_pred = model.predict_proba(X_test_scaled)\n",
    "        preds_prob += y_pred[:,1]\n",
    "        preds_prob_weighted += y_pred[:,1] * current_cverr\n",
    "    \n",
    "    accuracy = np.zeros(7)\n",
    "    auc_accuracy = np.zeros(7)\n",
    "    \n",
    "    preds_voting_weighted = preds_voting_weighted / total_cverr * num_batches\n",
    "    preds_logit_weighted = preds_logit_weighted / total_cverr * num_batches\n",
    "    preds_prob_weighted = preds_prob_weighted / total_cverr * num_batches\n",
    "    \n",
    "    # Majority voting (selecting the most frequent prediction for each sample)\n",
    "    final_predictions = np.where(preds_voting > num_batches / 2, 1, 0)\n",
    "    accuracy[0] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[0] = roc_auc_score(y_test, preds_voting)\n",
    "    \n",
    "    final_predictions = np.where(preds_voting_weighted > num_batches / 2, 1, 0)\n",
    "    accuracy[1] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[1] = roc_auc_score(y_test, preds_voting_weighted)\n",
    "    \n",
    "    # Average of logit\n",
    "    final_predictions = np.where(preds_logit > 0, 1, 0)\n",
    "    accuracy[2] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[2] = roc_auc_score(y_test, preds_logit)\n",
    "    \n",
    "    final_predictions = np.where(preds_logit_weighted > 0, 1, 0)\n",
    "    accuracy[3] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[3] = roc_auc_score(y_test, preds_logit_weighted)\n",
    "    \n",
    "    #Average of probs\n",
    "    final_predictions = np.where(preds_prob / num_batches > 0.5, 1, 0)\n",
    "    accuracy[4] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[4] = roc_auc_score(y_test, preds_prob)\n",
    "    \n",
    "    final_predictions = np.where(preds_prob_weighted / num_batches > 0.5, 1, 0)\n",
    "    accuracy[5] = accuracy_score(y_test, final_predictions)\n",
    "    auc_accuracy[5] = roc_auc_score(y_test, preds_prob_weighted)\n",
    "    \n",
    "    # Train a model on all 11 batches of training data\n",
    "    model = LogisticRegression(max_iter=500)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy[6] = accuracy_score(y_test, y_pred)\n",
    "    y_pred = model.decision_function(X_test_scaled)\n",
    "    auc_accuracy[6] = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, auc_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273e621",
   "metadata": {},
   "source": [
    "Try the divide and conquer approaches 10 times, reporting the following error matrix. \n",
    "- 10-by-4\n",
    "- col_1: majority voting\n",
    "- col_2: average the logit\n",
    "- col_3: average probabilities\n",
    "- col_4: using the model trained on all the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67dade4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies: [[0.9325  0.9325  0.9325  0.9325  0.9325  0.9325  0.93375]\n",
      " [0.91    0.91    0.9125  0.9125  0.9125  0.9125  0.90625]\n",
      " [0.9325  0.9325  0.93125 0.93    0.93    0.92875 0.9325 ]\n",
      " [0.9225  0.9225  0.92125 0.92125 0.92125 0.92125 0.9175 ]\n",
      " [0.92375 0.92375 0.92375 0.92375 0.92375 0.92375 0.92   ]\n",
      " [0.92125 0.92125 0.92125 0.92    0.92125 0.92    0.915  ]\n",
      " [0.92    0.92    0.92375 0.9225  0.9225  0.9225  0.9225 ]\n",
      " [0.925   0.925   0.92625 0.92625 0.92625 0.92625 0.9225 ]\n",
      " [0.94125 0.94125 0.94    0.94    0.94    0.94    0.94   ]\n",
      " [0.9275  0.9275  0.92625 0.92625 0.92625 0.92625 0.92125]\n",
      " [0.92625 0.92625 0.9225  0.9225  0.92125 0.9225  0.9225 ]\n",
      " [0.9225  0.9225  0.92    0.92    0.92    0.92    0.9175 ]\n",
      " [0.9225  0.9225  0.9275  0.9275  0.9275  0.9275  0.9275 ]\n",
      " [0.935   0.935   0.935   0.935   0.935   0.935   0.93   ]\n",
      " [0.92875 0.92875 0.9275  0.9275  0.9275  0.9275  0.9275 ]\n",
      " [0.9075  0.9075  0.91    0.91    0.91    0.91    0.91375]\n",
      " [0.92375 0.92375 0.9275  0.92625 0.9275  0.92625 0.92125]\n",
      " [0.9175  0.9175  0.9175  0.9175  0.9175  0.9175  0.91625]\n",
      " [0.93375 0.93375 0.93    0.93125 0.93    0.93125 0.92625]\n",
      " [0.92625 0.92625 0.93125 0.93125 0.93125 0.93125 0.92875]\n",
      " [0.92875 0.92875 0.92625 0.92625 0.92625 0.92625 0.93125]\n",
      " [0.9275  0.9275  0.92875 0.92875 0.93    0.92875 0.93   ]\n",
      " [0.91625 0.91625 0.92    0.92    0.92    0.92    0.92   ]\n",
      " [0.93    0.93    0.92875 0.9275  0.93    0.92875 0.93   ]\n",
      " [0.9225  0.9225  0.9175  0.9175  0.9175  0.91875 0.92125]\n",
      " [0.93    0.93    0.92875 0.93    0.92875 0.92875 0.92875]\n",
      " [0.9275  0.9275  0.92625 0.92625 0.92625 0.92625 0.93   ]\n",
      " [0.93125 0.93125 0.93125 0.93125 0.93125 0.93125 0.92875]\n",
      " [0.92375 0.92375 0.925   0.925   0.925   0.925   0.9225 ]\n",
      " [0.93    0.93    0.93125 0.93125 0.93125 0.93125 0.92375]\n",
      " [0.915   0.915   0.92    0.92    0.92    0.91875 0.9175 ]\n",
      " [0.91625 0.91625 0.91875 0.91875 0.91875 0.91875 0.9175 ]\n",
      " [0.91875 0.91875 0.92    0.91875 0.92    0.91875 0.92125]\n",
      " [0.91875 0.91875 0.91875 0.91875 0.91875 0.91875 0.91625]\n",
      " [0.91125 0.91125 0.9175  0.91625 0.91625 0.91625 0.915  ]\n",
      " [0.925   0.925   0.92    0.92    0.92    0.92    0.92   ]\n",
      " [0.90875 0.90875 0.9075  0.9075  0.9075  0.9075  0.91125]\n",
      " [0.93    0.93    0.92625 0.92625 0.92625 0.92625 0.9225 ]\n",
      " [0.925   0.925   0.92875 0.9275  0.92875 0.92875 0.93125]\n",
      " [0.935   0.935   0.93125 0.93125 0.93125 0.9325  0.92375]\n",
      " [0.93    0.93    0.92875 0.92875 0.92875 0.92875 0.92625]\n",
      " [0.9375  0.9375  0.9325  0.9325  0.93375 0.93375 0.9325 ]\n",
      " [0.925   0.925   0.93125 0.93125 0.93125 0.93125 0.93   ]\n",
      " [0.92625 0.92625 0.92875 0.92875 0.9275  0.92875 0.92625]\n",
      " [0.92875 0.92875 0.92875 0.92875 0.9275  0.9275  0.9275 ]\n",
      " [0.9225  0.9225  0.92125 0.92125 0.92125 0.92125 0.92125]\n",
      " [0.91    0.91    0.91625 0.91625 0.91625 0.91625 0.91375]\n",
      " [0.92    0.92    0.91125 0.9125  0.91375 0.91375 0.9125 ]\n",
      " [0.91125 0.91125 0.92    0.91875 0.91875 0.9175  0.9125 ]\n",
      " [0.93375 0.93375 0.9275  0.9275  0.9275  0.9275  0.9275 ]]\n",
      "AUC: [[0.968817   0.96911389 0.98189898 0.98193023 0.98186148 0.98187398\n",
      "  0.98293029]\n",
      " [0.95887795 0.9588967  0.9760025  0.97598375 0.97659634 0.97656509\n",
      "  0.97783404]\n",
      " [0.96438742 0.96409639 0.98262557 0.98264434 0.98270693 0.98270067\n",
      "  0.98329526]\n",
      " [0.95985276 0.95973684 0.97841479 0.97837719 0.9787406  0.97865288\n",
      "  0.97909774]\n",
      " [0.96383234 0.96420424 0.97861755 0.9786238  0.97890506 0.97886131\n",
      "  0.97980511]\n",
      " [0.97158732 0.97170607 0.98238739 0.98238114 0.98248114 0.98251239\n",
      "  0.98209989]\n",
      " [0.95705872 0.95702746 0.97866847 0.97861845 0.97866847 0.9785997\n",
      "  0.97875599]\n",
      " [0.96905941 0.96892189 0.98224822 0.98224197 0.9824795  0.982467\n",
      "  0.98232323]\n",
      " [0.97044895 0.97085612 0.9842457  0.98428955 0.98441484 0.98447748\n",
      "  0.98446495]\n",
      " [0.9675985  0.96743902 0.98300188 0.98302064 0.98310194 0.98309568\n",
      "  0.98327079]\n",
      " [0.97075312 0.97061875 0.982      0.98205625 0.98195625 0.98195625\n",
      "  0.98183125]\n",
      " [0.96578487 0.96555347 0.98075672 0.98071295 0.98117573 0.98115697\n",
      "  0.98010632]\n",
      " [0.96574152 0.96587592 0.9807845  0.98077825 0.98089702 0.98089076\n",
      "  0.98057822]\n",
      " [0.96853407 0.96874042 0.98487359 0.98484233 0.98503617 0.98513623\n",
      "  0.98436083]\n",
      " [0.9698692  0.96971601 0.98151135 0.98148634 0.98188025 0.98184899\n",
      "  0.98218037]\n",
      " [0.95342187 0.95354688 0.9748125  0.9748     0.97501875 0.9750625\n",
      "  0.9757625 ]\n",
      " [0.96637681 0.96632055 0.9802018  0.98016429 0.98023305 0.9802393\n",
      "  0.98121452]\n",
      " [0.95774198 0.95750732 0.97600811 0.97599559 0.97607694 0.97607069\n",
      "  0.97545118]\n",
      " [0.96097247 0.96087245 0.97898277 0.97895777 0.97925783 0.97917656\n",
      "  0.97968293]\n",
      " [0.97053473 0.97062538 0.98271971 0.98271346 0.98355746 0.98358872\n",
      "  0.98276972]\n",
      " [0.96370241 0.96378063 0.97943731 0.97943105 0.97962504 0.97965633\n",
      "  0.97966884]\n",
      " [0.96150529 0.96164904 0.97878697 0.97874947 0.97898697 0.97894322\n",
      "  0.97995575]\n",
      " [0.96196731 0.96201429 0.97943485 0.97940353 0.97960398 0.97960398\n",
      "  0.9801427 ]\n",
      " [0.96632453 0.96613376 0.98105442 0.9810294  0.98111697 0.98112948\n",
      "  0.98223657]\n",
      " [0.9652543  0.9653793  0.97934259 0.97932384 0.9794676  0.9794676\n",
      "  0.97922383]\n",
      " [0.96745105 0.96748235 0.98173327 0.98167067 0.98199619 0.98197741\n",
      "  0.98258464]\n",
      " [0.96675542 0.96684917 0.98291832 0.98288082 0.98299332 0.98299332\n",
      "  0.98291832]\n",
      " [0.96618534 0.9662416  0.98161588 0.98159712 0.98177215 0.98170339\n",
      "  0.98197218]\n",
      " [0.960967   0.96078229 0.97729023 0.97723388 0.97732154 0.97725266\n",
      "  0.97767217]\n",
      " [0.97340339 0.97313458 0.98263984 0.9826461  0.98293991 0.9828899\n",
      "  0.98338376]\n",
      " [0.95996211 0.96011217 0.97674695 0.9767532  0.97699705 0.97701581\n",
      "  0.97689701]\n",
      " [0.96004101 0.96004726 0.97741367 0.97739491 0.97720737 0.97720112\n",
      "  0.97787627]\n",
      " [0.96089896 0.96074888 0.97746359 0.9774886  0.9774886  0.97749486\n",
      "  0.97750736]\n",
      " [0.96269992 0.96284065 0.98072292 0.98069165 0.98044146 0.98044146\n",
      "  0.97946572]\n",
      " [0.95541612 0.95540361 0.97825543 0.97822415 0.97823041 0.97823041\n",
      "  0.97859323]\n",
      " [0.95833021 0.95857396 0.97976824 0.97978074 0.97978699 0.97973074\n",
      "  0.979862  ]\n",
      " [0.9624152  0.96230558 0.97951628 0.97958519 0.97972926 0.97972926\n",
      "  0.97945991]\n",
      " [0.95803501 0.95795371 0.98061519 0.98061519 0.98072774 0.98075276\n",
      "  0.98016496]\n",
      " [0.95684951 0.9569464  0.98022191 0.9801844  0.98025316 0.98024691\n",
      "  0.98071574]\n",
      " [0.9655386  0.96534465 0.98149921 0.98147418 0.98146793 0.98148044\n",
      "  0.98131777]\n",
      " [0.97297542 0.97312231 0.98269202 0.98271077 0.98289829 0.98292329\n",
      "  0.98292954]\n",
      " [0.97047487 0.97056888 0.98400592 0.98401845 0.98420647 0.98417513\n",
      "  0.98446343]\n",
      " [0.96963134 0.96950945 0.98062931 0.98063556 0.98081683 0.98080433\n",
      "  0.98157941]\n",
      " [0.96904204 0.96915768 0.98347867 0.98347867 0.98372246 0.9837037\n",
      "  0.98315987]\n",
      " [0.96546217 0.96531216 0.98260451 0.98263576 0.98274827 0.98270452\n",
      "  0.98261701]\n",
      " [0.96966487 0.96931173 0.97953074 0.97951199 0.98025576 0.98024951\n",
      "  0.98086202]\n",
      " [0.94960302 0.94972815 0.97707579 0.97705076 0.97758257 0.97757006\n",
      "  0.97769519]\n",
      " [0.96089296 0.9607367  0.97711021 0.97705396 0.97747275 0.97739774\n",
      "  0.97791654]\n",
      " [0.95294368 0.95290929 0.97742222 0.97740971 0.97719713 0.97719088\n",
      "  0.97671569]\n",
      " [0.95971891 0.95971891 0.98337616 0.98340117 0.98320111 0.98321361\n",
      "  0.98363874]]\n"
     ]
    }
   ],
   "source": [
    "# Number of times to repeat the process\n",
    "num_repeats = 50\n",
    "\n",
    "# Initialize an empty matrix (10-by-4) to store accuracies\n",
    "accuracies = np.zeros((num_repeats, 7))\n",
    "auc_accuracies = np.zeros((num_repeats, 7))\n",
    "\n",
    "seed = 42\n",
    "# Repeat the process and store accuracies\n",
    "for i in range(num_repeats):\n",
    "    np.random.seed(seed)\n",
    "    accuracies[i], auc_accuracies[i] = iterate_process(X, y)\n",
    "    seed += 2\n",
    "    \n",
    "\n",
    "# Print the accuracies\n",
    "print(\"Accuracies:\", accuracies)\n",
    "print(\"AUC:\", auc_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06026678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.924325, 0.924325, 0.924525, 0.924375, 0.924475, 0.9244  ,\n",
       "       0.9231  ])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "99c91732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00771852, 0.00771852, 0.00666094, 0.00668136, 0.00665249,\n",
       "       0.00669533, 0.00688858])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f7a69b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96390726, 0.9639035 , 0.98030329, 0.98029366, 0.98046602,\n",
       "       0.98045614, 0.98062023])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(auc_accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a1f7f5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00551189, 0.00550877, 0.002402  , 0.00241128, 0.00240214,\n",
       "       0.00241261, 0.00233349])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(auc_accuracies, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba134b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
